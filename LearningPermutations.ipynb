{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvhL5n76zBqx",
        "outputId": "5d57ec89-3369-4fbf-a46d-c6e649dfddd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov 27 22:28:24 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P0              29W /  70W |    421MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "eW9XY9jbzIAU"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_columns(tensor, indices):\n",
        "    return tensor[:, indices]"
      ],
      "metadata": {
        "id": "hpZs3i98YfGN"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 10 # Sequence length\n",
        "n_train = 2000  # Batch size\n",
        "vocab_size = 10 #vocab_size\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Generate a batch of random input sequences\n",
        "input_sequences = torch.randint(0, vocab_size, (n_train, N)) #n x N\n",
        "\n",
        "A = torch.eye(N).long()\n",
        "\n",
        "perm = torch.randperm(N)\n",
        "A = shuffle_columns(A, perm) #generates a random permutation\n",
        "print(A)\n",
        "\n",
        "output_sequences  = torch.matmul(input_sequences, A)\n",
        "print(torch.max(output_sequences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPWt8UNczJOX",
        "outputId": "7d0a4634-c5d9-4723-9178-30a955892d1b"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
            "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]])\n",
            "tensor(9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_sequences = input_sequences.to(device)\n",
        "train_output_sequences = output_sequences.to(device)"
      ],
      "metadata": {
        "id": "QVQFyY2Zz3Ui"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_test = 400\n",
        "val_input_sequences = torch.randint(0, vocab_size, (n_test, N))\n",
        "val_output_sequences = torch.matmul(val_input_sequences, A)\n",
        "\n",
        "val_input_sequences = val_input_sequences.to(device)\n",
        "val_output_sequences = val_output_sequences.to(device)"
      ],
      "metadata": {
        "id": "6kZhLBI65jJa"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PointwiseFeedforwardNetwork(nn.Module):\n",
        "  def __init__(self, d_embed, d_ff):\n",
        "    super(PointwiseFeedforwardNetwork, self).__init__()\n",
        "    self.d_embed = d_embed\n",
        "    self.d_ff = d_ff\n",
        "\n",
        "    self.ffn1 = nn.Linear(d_embed, d_ff)\n",
        "    self.ffn2 = nn.Linear(d_ff, d_embed)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.ffn2(self.relu(self.ffn1(x)))"
      ],
      "metadata": {
        "id": "897inNwlz5i-"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, d_embed):\n",
        "    super(Attention, self).__init__()\n",
        "\n",
        "    self.d_embed = d_embed\n",
        "    self.query_proj = nn.Linear(d_embed, d_embed).to(device)\n",
        "    self.key_proj = nn.Linear(d_embed, d_embed).to(device)\n",
        "    self.value_proj = nn.Linear(d_embed, d_embed).to(device)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    Q = self.query_proj(x)\n",
        "    K = self.key_proj(x)\n",
        "    V = self.value_proj(x)\n",
        "\n",
        "    scores = torch.matmul(Q, K.transpose(2, 1)) / (self.d_embed ** 0.5)\n",
        "\n",
        "    attention_weights = torch.softmax(scores, dim=-1)\n",
        "\n",
        "    output = torch.matmul(attention_weights, V)\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "IIShZV1Fz7nS"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "  def __init__(self, d_embed, d_ff):\n",
        "    super(TransformerLayer, self).__init__()\n",
        "    self.d_embed = d_embed\n",
        "    self.d_ff = d_ff\n",
        "\n",
        "    self.attn = Attention(self.d_embed)\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(d_embed).to(device)\n",
        "    self.norm2 = nn.LayerNorm(d_embed).to(device)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    self.ffn = PointwiseFeedforwardNetwork(d_embed, d_ff).to(device)\n",
        "    self.ffout = nn.Linear(d_embed, d_embed).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.norm1(x + self.dropout(self.attn(x)))\n",
        "    ffn = self.ffn(x)\n",
        "    x = self.norm2(x + self.dropout(ffn))\n",
        "\n",
        "    return self.ffout(x)\n"
      ],
      "metadata": {
        "id": "K03z-F4Ez-xo"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLayerTransformer(nn.Module):\n",
        "  def __init__(self, d_embed, vocab_size, d_ff, num_layers):\n",
        "    super(MultiLayerTransformer, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, d_embed).to(device)\n",
        "    self.layers = nn.ModuleList([TransformerLayer(d_embed, d_ff)] * num_layers)\n",
        "    self.fcout = nn.Linear(d_embed, vocab_size).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_embed = self.embed(x)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x_embed = layer(x_embed)\n",
        "\n",
        "    return self.fcout(x_embed)"
      ],
      "metadata": {
        "id": "OtGOdJ-s0GTd"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, d_embed, vocab_size, hidden_dim, num_layers):\n",
        "    super().__init__()\n",
        "    self.size_vocab = vocab_size\n",
        "    self.d_embed = d_embed\n",
        "\n",
        "    self.embed = nn.Embedding(vocab_size, d_embed).to(device)\n",
        "    self.rnn = nn.RNN(d_embed, hidden_dim, num_layers= num_layers, dropout=0.1, batch_first=True).to(device)\n",
        "    self.fc = nn.Linear(hidden_dim, vocab_size).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_embed = self.embed(x) # (n, N, embed_dim)\n",
        "    x_rnn, _ = self.rnn(x_embed) # (n, N, hidden_dim)\n",
        "    out = self.fc(x_rnn) # (n, N, size_vocab)\n",
        "    return out"
      ],
      "metadata": {
        "id": "11O0wUx2_pLs"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_evaluate(X_val, y_val, vocab_size, model, criterion):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      predictions = model(X_val).to(device)\n",
        "      loss = criterion(predictions.contiguous().view(-1, vocab_size), y_val.contiguous().view(-1))\n",
        "      return loss.item()"
      ],
      "metadata": {
        "id": "U_BIeonA5uFo"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(vocab_size, model, criterion, optimizer, epochs, X_train, y_train, X_val, y_val):\n",
        "  losses = []\n",
        "  val_losses = []\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(X_train)\n",
        "\n",
        "    loss = criterion(outputs.contiguous().view(-1, vocab_size), y_train.contiguous().view(-1))\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    val_loss = epoch_evaluate(X_val, y_val, vocab_size, model, criterion)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "      print(f'Epoch [{epoch+1}/{epochs}], Training loss: {loss:.4f}, Validation loss: {val_loss:.4f}')\n",
        "\n",
        "  return losses, val_losses\n"
      ],
      "metadata": {
        "id": "9Q2BKU0PE_Nv"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_ff = 32\n",
        "d_embed = 32\n",
        "num_layers = 3\n",
        "\n",
        "trans = MultiLayerTransformer(d_embed, vocab_size, d_ff, num_layers).to(device)\n",
        "optimizer_trans = torch.optim.Adam(trans.parameters())\n",
        "num_trainable_params = sum([p.numel() for p in trans.parameters()])\n",
        "print(\"# param Transformer:\", num_trainable_params)\n",
        "\n",
        "rnn = RNN(d_embed, vocab_size, d_ff, num_layers).to(device)\n",
        "optimizer_rnn = torch.optim.Adam(rnn.parameters())\n",
        "num_trainable_params = sum([p.numel() for p in rnn.parameters()])\n",
        "print(\"# param RNN:\", num_trainable_params)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "epochs = 5000\n"
      ],
      "metadata": {
        "id": "TIDpe3NCFgR-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa972e47-466f-4551-c3fc-a6ee2d0399cf"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# param Transformer: 7114\n",
            "# param RNN: 6986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trans_train_loss, trans_losses = train(vocab_size, trans, criterion, optimizer_trans, epochs, train_input_sequences, train_output_sequences, val_input_sequences, val_output_sequences)"
      ],
      "metadata": {
        "id": "BJ2n-G4RGlaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2b8a84-2c64-4ac3-ddb6-2123e31bb2c7"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/5000], Training loss: 2.2465, Validation loss: 2.2484\n",
            "Epoch [20/5000], Training loss: 2.1767, Validation loss: 2.1853\n",
            "Epoch [30/5000], Training loss: 2.1141, Validation loss: 2.1242\n",
            "Epoch [40/5000], Training loss: 2.0596, Validation loss: 2.0693\n",
            "Epoch [50/5000], Training loss: 2.0119, Validation loss: 2.0218\n",
            "Epoch [60/5000], Training loss: 1.9704, Validation loss: 1.9848\n",
            "Epoch [70/5000], Training loss: 1.9341, Validation loss: 1.9536\n",
            "Epoch [80/5000], Training loss: 1.9037, Validation loss: 1.9259\n",
            "Epoch [90/5000], Training loss: 1.8883, Validation loss: 1.9090\n",
            "Epoch [100/5000], Training loss: 1.8764, Validation loss: 1.9000\n",
            "Epoch [110/5000], Training loss: 1.8721, Validation loss: 1.8949\n",
            "Epoch [120/5000], Training loss: 1.8657, Validation loss: 1.8902\n",
            "Epoch [130/5000], Training loss: 1.8585, Validation loss: 1.8862\n",
            "Epoch [140/5000], Training loss: 1.8549, Validation loss: 1.8819\n",
            "Epoch [150/5000], Training loss: 1.8502, Validation loss: 1.8780\n",
            "Epoch [160/5000], Training loss: 1.8471, Validation loss: 1.8743\n",
            "Epoch [170/5000], Training loss: 1.8419, Validation loss: 1.8709\n",
            "Epoch [180/5000], Training loss: 1.8365, Validation loss: 1.8675\n",
            "Epoch [190/5000], Training loss: 1.8344, Validation loss: 1.8643\n",
            "Epoch [200/5000], Training loss: 1.8282, Validation loss: 1.8608\n",
            "Epoch [210/5000], Training loss: 1.8249, Validation loss: 1.8578\n",
            "Epoch [220/5000], Training loss: 1.8245, Validation loss: 1.8550\n",
            "Epoch [230/5000], Training loss: 1.8198, Validation loss: 1.8527\n",
            "Epoch [240/5000], Training loss: 1.8170, Validation loss: 1.8503\n",
            "Epoch [250/5000], Training loss: 1.8162, Validation loss: 1.8477\n",
            "Epoch [260/5000], Training loss: 1.8115, Validation loss: 1.8460\n",
            "Epoch [270/5000], Training loss: 1.8077, Validation loss: 1.8441\n",
            "Epoch [280/5000], Training loss: 1.8086, Validation loss: 1.8422\n",
            "Epoch [290/5000], Training loss: 1.8059, Validation loss: 1.8415\n",
            "Epoch [300/5000], Training loss: 1.8060, Validation loss: 1.8398\n",
            "Epoch [310/5000], Training loss: 1.8043, Validation loss: 1.8388\n",
            "Epoch [320/5000], Training loss: 1.7995, Validation loss: 1.8377\n",
            "Epoch [330/5000], Training loss: 1.8002, Validation loss: 1.8367\n",
            "Epoch [340/5000], Training loss: 1.7968, Validation loss: 1.8364\n",
            "Epoch [350/5000], Training loss: 1.7972, Validation loss: 1.8360\n",
            "Epoch [360/5000], Training loss: 1.7963, Validation loss: 1.8352\n",
            "Epoch [370/5000], Training loss: 1.7946, Validation loss: 1.8348\n",
            "Epoch [380/5000], Training loss: 1.7930, Validation loss: 1.8340\n",
            "Epoch [390/5000], Training loss: 1.7900, Validation loss: 1.8348\n",
            "Epoch [400/5000], Training loss: 1.7914, Validation loss: 1.8340\n",
            "Epoch [410/5000], Training loss: 1.7877, Validation loss: 1.8337\n",
            "Epoch [420/5000], Training loss: 1.7848, Validation loss: 1.8339\n",
            "Epoch [430/5000], Training loss: 1.7877, Validation loss: 1.8338\n",
            "Epoch [440/5000], Training loss: 1.7875, Validation loss: 1.8339\n",
            "Epoch [450/5000], Training loss: 1.7841, Validation loss: 1.8337\n",
            "Epoch [460/5000], Training loss: 1.7851, Validation loss: 1.8336\n",
            "Epoch [470/5000], Training loss: 1.7843, Validation loss: 1.8330\n",
            "Epoch [480/5000], Training loss: 1.7816, Validation loss: 1.8333\n",
            "Epoch [490/5000], Training loss: 1.7811, Validation loss: 1.8336\n",
            "Epoch [500/5000], Training loss: 1.7821, Validation loss: 1.8339\n",
            "Epoch [510/5000], Training loss: 1.7796, Validation loss: 1.8336\n",
            "Epoch [520/5000], Training loss: 1.7781, Validation loss: 1.8332\n",
            "Epoch [530/5000], Training loss: 1.7787, Validation loss: 1.8346\n",
            "Epoch [540/5000], Training loss: 1.7784, Validation loss: 1.8352\n",
            "Epoch [550/5000], Training loss: 1.7752, Validation loss: 1.8356\n",
            "Epoch [560/5000], Training loss: 1.7765, Validation loss: 1.8357\n",
            "Epoch [570/5000], Training loss: 1.7759, Validation loss: 1.8357\n",
            "Epoch [580/5000], Training loss: 1.7745, Validation loss: 1.8355\n",
            "Epoch [590/5000], Training loss: 1.7744, Validation loss: 1.8358\n",
            "Epoch [600/5000], Training loss: 1.7727, Validation loss: 1.8368\n",
            "Epoch [610/5000], Training loss: 1.7729, Validation loss: 1.8367\n",
            "Epoch [620/5000], Training loss: 1.7726, Validation loss: 1.8375\n",
            "Epoch [630/5000], Training loss: 1.7714, Validation loss: 1.8376\n",
            "Epoch [640/5000], Training loss: 1.7718, Validation loss: 1.8381\n",
            "Epoch [650/5000], Training loss: 1.7713, Validation loss: 1.8382\n",
            "Epoch [660/5000], Training loss: 1.7671, Validation loss: 1.8383\n",
            "Epoch [670/5000], Training loss: 1.7697, Validation loss: 1.8385\n",
            "Epoch [680/5000], Training loss: 1.7685, Validation loss: 1.8387\n",
            "Epoch [690/5000], Training loss: 1.7713, Validation loss: 1.8392\n",
            "Epoch [700/5000], Training loss: 1.7721, Validation loss: 1.8399\n",
            "Epoch [710/5000], Training loss: 1.7692, Validation loss: 1.8395\n",
            "Epoch [720/5000], Training loss: 1.7673, Validation loss: 1.8399\n",
            "Epoch [730/5000], Training loss: 1.7670, Validation loss: 1.8406\n",
            "Epoch [740/5000], Training loss: 1.7681, Validation loss: 1.8406\n",
            "Epoch [750/5000], Training loss: 1.7693, Validation loss: 1.8409\n",
            "Epoch [760/5000], Training loss: 1.7661, Validation loss: 1.8412\n",
            "Epoch [770/5000], Training loss: 1.7671, Validation loss: 1.8428\n",
            "Epoch [780/5000], Training loss: 1.7677, Validation loss: 1.8425\n",
            "Epoch [790/5000], Training loss: 1.7624, Validation loss: 1.8423\n",
            "Epoch [800/5000], Training loss: 1.7665, Validation loss: 1.8437\n",
            "Epoch [810/5000], Training loss: 1.7639, Validation loss: 1.8437\n",
            "Epoch [820/5000], Training loss: 1.7631, Validation loss: 1.8440\n",
            "Epoch [830/5000], Training loss: 1.7630, Validation loss: 1.8442\n",
            "Epoch [840/5000], Training loss: 1.7638, Validation loss: 1.8449\n",
            "Epoch [850/5000], Training loss: 1.7663, Validation loss: 1.8453\n",
            "Epoch [860/5000], Training loss: 1.7623, Validation loss: 1.8456\n",
            "Epoch [870/5000], Training loss: 1.7607, Validation loss: 1.8452\n",
            "Epoch [880/5000], Training loss: 1.7610, Validation loss: 1.8453\n",
            "Epoch [890/5000], Training loss: 1.7609, Validation loss: 1.8449\n",
            "Epoch [900/5000], Training loss: 1.7612, Validation loss: 1.8474\n",
            "Epoch [910/5000], Training loss: 1.7587, Validation loss: 1.8475\n",
            "Epoch [920/5000], Training loss: 1.7592, Validation loss: 1.8483\n",
            "Epoch [930/5000], Training loss: 1.7594, Validation loss: 1.8485\n",
            "Epoch [940/5000], Training loss: 1.7587, Validation loss: 1.8484\n",
            "Epoch [950/5000], Training loss: 1.7579, Validation loss: 1.8483\n",
            "Epoch [960/5000], Training loss: 1.7589, Validation loss: 1.8491\n",
            "Epoch [970/5000], Training loss: 1.7588, Validation loss: 1.8491\n",
            "Epoch [980/5000], Training loss: 1.7590, Validation loss: 1.8484\n",
            "Epoch [990/5000], Training loss: 1.7562, Validation loss: 1.8489\n",
            "Epoch [1000/5000], Training loss: 1.7566, Validation loss: 1.8501\n",
            "Epoch [1010/5000], Training loss: 1.7545, Validation loss: 1.8499\n",
            "Epoch [1020/5000], Training loss: 1.7583, Validation loss: 1.8502\n",
            "Epoch [1030/5000], Training loss: 1.7574, Validation loss: 1.8496\n",
            "Epoch [1040/5000], Training loss: 1.7570, Validation loss: 1.8507\n",
            "Epoch [1050/5000], Training loss: 1.7570, Validation loss: 1.8509\n",
            "Epoch [1060/5000], Training loss: 1.7542, Validation loss: 1.8508\n",
            "Epoch [1070/5000], Training loss: 1.7531, Validation loss: 1.8517\n",
            "Epoch [1080/5000], Training loss: 1.7545, Validation loss: 1.8508\n",
            "Epoch [1090/5000], Training loss: 1.7552, Validation loss: 1.8516\n",
            "Epoch [1100/5000], Training loss: 1.7557, Validation loss: 1.8527\n",
            "Epoch [1110/5000], Training loss: 1.7535, Validation loss: 1.8518\n",
            "Epoch [1120/5000], Training loss: 1.7556, Validation loss: 1.8525\n",
            "Epoch [1130/5000], Training loss: 1.7545, Validation loss: 1.8532\n",
            "Epoch [1140/5000], Training loss: 1.7523, Validation loss: 1.8531\n",
            "Epoch [1150/5000], Training loss: 1.7535, Validation loss: 1.8531\n",
            "Epoch [1160/5000], Training loss: 1.7546, Validation loss: 1.8534\n",
            "Epoch [1170/5000], Training loss: 1.7494, Validation loss: 1.8538\n",
            "Epoch [1180/5000], Training loss: 1.7550, Validation loss: 1.8538\n",
            "Epoch [1190/5000], Training loss: 1.7506, Validation loss: 1.8538\n",
            "Epoch [1200/5000], Training loss: 1.7524, Validation loss: 1.8547\n",
            "Epoch [1210/5000], Training loss: 1.7534, Validation loss: 1.8537\n",
            "Epoch [1220/5000], Training loss: 1.7521, Validation loss: 1.8539\n",
            "Epoch [1230/5000], Training loss: 1.7519, Validation loss: 1.8550\n",
            "Epoch [1240/5000], Training loss: 1.7521, Validation loss: 1.8547\n",
            "Epoch [1250/5000], Training loss: 1.7501, Validation loss: 1.8552\n",
            "Epoch [1260/5000], Training loss: 1.7508, Validation loss: 1.8546\n",
            "Epoch [1270/5000], Training loss: 1.7503, Validation loss: 1.8551\n",
            "Epoch [1280/5000], Training loss: 1.7497, Validation loss: 1.8568\n",
            "Epoch [1290/5000], Training loss: 1.7472, Validation loss: 1.8556\n",
            "Epoch [1300/5000], Training loss: 1.7487, Validation loss: 1.8557\n",
            "Epoch [1310/5000], Training loss: 1.7500, Validation loss: 1.8572\n",
            "Epoch [1320/5000], Training loss: 1.7511, Validation loss: 1.8569\n",
            "Epoch [1330/5000], Training loss: 1.7471, Validation loss: 1.8570\n",
            "Epoch [1340/5000], Training loss: 1.7486, Validation loss: 1.8575\n",
            "Epoch [1350/5000], Training loss: 1.7477, Validation loss: 1.8571\n",
            "Epoch [1360/5000], Training loss: 1.7501, Validation loss: 1.8579\n",
            "Epoch [1370/5000], Training loss: 1.7446, Validation loss: 1.8572\n",
            "Epoch [1380/5000], Training loss: 1.7497, Validation loss: 1.8584\n",
            "Epoch [1390/5000], Training loss: 1.7462, Validation loss: 1.8585\n",
            "Epoch [1400/5000], Training loss: 1.7435, Validation loss: 1.8586\n",
            "Epoch [1410/5000], Training loss: 1.7477, Validation loss: 1.8590\n",
            "Epoch [1420/5000], Training loss: 1.7461, Validation loss: 1.8597\n",
            "Epoch [1430/5000], Training loss: 1.7480, Validation loss: 1.8605\n",
            "Epoch [1440/5000], Training loss: 1.7473, Validation loss: 1.8597\n",
            "Epoch [1450/5000], Training loss: 1.7470, Validation loss: 1.8614\n",
            "Epoch [1460/5000], Training loss: 1.7446, Validation loss: 1.8614\n",
            "Epoch [1470/5000], Training loss: 1.7444, Validation loss: 1.8605\n",
            "Epoch [1480/5000], Training loss: 1.7457, Validation loss: 1.8594\n",
            "Epoch [1490/5000], Training loss: 1.7451, Validation loss: 1.8618\n",
            "Epoch [1500/5000], Training loss: 1.7427, Validation loss: 1.8617\n",
            "Epoch [1510/5000], Training loss: 1.7460, Validation loss: 1.8632\n",
            "Epoch [1520/5000], Training loss: 1.7412, Validation loss: 1.8619\n",
            "Epoch [1530/5000], Training loss: 1.7415, Validation loss: 1.8627\n",
            "Epoch [1540/5000], Training loss: 1.7425, Validation loss: 1.8632\n",
            "Epoch [1550/5000], Training loss: 1.7429, Validation loss: 1.8625\n",
            "Epoch [1560/5000], Training loss: 1.7452, Validation loss: 1.8634\n",
            "Epoch [1570/5000], Training loss: 1.7428, Validation loss: 1.8628\n",
            "Epoch [1580/5000], Training loss: 1.7420, Validation loss: 1.8650\n",
            "Epoch [1590/5000], Training loss: 1.7434, Validation loss: 1.8654\n",
            "Epoch [1600/5000], Training loss: 1.7408, Validation loss: 1.8638\n",
            "Epoch [1610/5000], Training loss: 1.7414, Validation loss: 1.8637\n",
            "Epoch [1620/5000], Training loss: 1.7416, Validation loss: 1.8634\n",
            "Epoch [1630/5000], Training loss: 1.7422, Validation loss: 1.8645\n",
            "Epoch [1640/5000], Training loss: 1.7432, Validation loss: 1.8646\n",
            "Epoch [1650/5000], Training loss: 1.7419, Validation loss: 1.8651\n",
            "Epoch [1660/5000], Training loss: 1.7399, Validation loss: 1.8654\n",
            "Epoch [1670/5000], Training loss: 1.7388, Validation loss: 1.8667\n",
            "Epoch [1680/5000], Training loss: 1.7400, Validation loss: 1.8660\n",
            "Epoch [1690/5000], Training loss: 1.7404, Validation loss: 1.8680\n",
            "Epoch [1700/5000], Training loss: 1.7417, Validation loss: 1.8671\n",
            "Epoch [1710/5000], Training loss: 1.7409, Validation loss: 1.8668\n",
            "Epoch [1720/5000], Training loss: 1.7382, Validation loss: 1.8665\n",
            "Epoch [1730/5000], Training loss: 1.7429, Validation loss: 1.8693\n",
            "Epoch [1740/5000], Training loss: 1.7376, Validation loss: 1.8682\n",
            "Epoch [1750/5000], Training loss: 1.7385, Validation loss: 1.8694\n",
            "Epoch [1760/5000], Training loss: 1.7416, Validation loss: 1.8683\n",
            "Epoch [1770/5000], Training loss: 1.7382, Validation loss: 1.8685\n",
            "Epoch [1780/5000], Training loss: 1.7404, Validation loss: 1.8686\n",
            "Epoch [1790/5000], Training loss: 1.7402, Validation loss: 1.8690\n",
            "Epoch [1800/5000], Training loss: 1.7412, Validation loss: 1.8693\n",
            "Epoch [1810/5000], Training loss: 1.7374, Validation loss: 1.8701\n",
            "Epoch [1820/5000], Training loss: 1.7404, Validation loss: 1.8692\n",
            "Epoch [1830/5000], Training loss: 1.7371, Validation loss: 1.8685\n",
            "Epoch [1840/5000], Training loss: 1.7387, Validation loss: 1.8688\n",
            "Epoch [1850/5000], Training loss: 1.7391, Validation loss: 1.8702\n",
            "Epoch [1860/5000], Training loss: 1.7390, Validation loss: 1.8700\n",
            "Epoch [1870/5000], Training loss: 1.7364, Validation loss: 1.8695\n",
            "Epoch [1880/5000], Training loss: 1.7403, Validation loss: 1.8723\n",
            "Epoch [1890/5000], Training loss: 1.7372, Validation loss: 1.8705\n",
            "Epoch [1900/5000], Training loss: 1.7353, Validation loss: 1.8708\n",
            "Epoch [1910/5000], Training loss: 1.7364, Validation loss: 1.8701\n",
            "Epoch [1920/5000], Training loss: 1.7399, Validation loss: 1.8700\n",
            "Epoch [1930/5000], Training loss: 1.7396, Validation loss: 1.8712\n",
            "Epoch [1940/5000], Training loss: 1.7384, Validation loss: 1.8702\n",
            "Epoch [1950/5000], Training loss: 1.7374, Validation loss: 1.8726\n",
            "Epoch [1960/5000], Training loss: 1.7360, Validation loss: 1.8712\n",
            "Epoch [1970/5000], Training loss: 1.7386, Validation loss: 1.8724\n",
            "Epoch [1980/5000], Training loss: 1.7372, Validation loss: 1.8745\n",
            "Epoch [1990/5000], Training loss: 1.7380, Validation loss: 1.8730\n",
            "Epoch [2000/5000], Training loss: 1.7368, Validation loss: 1.8735\n",
            "Epoch [2010/5000], Training loss: 1.7357, Validation loss: 1.8722\n",
            "Epoch [2020/5000], Training loss: 1.7369, Validation loss: 1.8726\n",
            "Epoch [2030/5000], Training loss: 1.7366, Validation loss: 1.8730\n",
            "Epoch [2040/5000], Training loss: 1.7339, Validation loss: 1.8717\n",
            "Epoch [2050/5000], Training loss: 1.7373, Validation loss: 1.8724\n",
            "Epoch [2060/5000], Training loss: 1.7330, Validation loss: 1.8727\n",
            "Epoch [2070/5000], Training loss: 1.7337, Validation loss: 1.8732\n",
            "Epoch [2080/5000], Training loss: 1.7363, Validation loss: 1.8745\n",
            "Epoch [2090/5000], Training loss: 1.7349, Validation loss: 1.8735\n",
            "Epoch [2100/5000], Training loss: 1.7346, Validation loss: 1.8734\n",
            "Epoch [2110/5000], Training loss: 1.7358, Validation loss: 1.8741\n",
            "Epoch [2120/5000], Training loss: 1.7320, Validation loss: 1.8744\n",
            "Epoch [2130/5000], Training loss: 1.7328, Validation loss: 1.8744\n",
            "Epoch [2140/5000], Training loss: 1.7383, Validation loss: 1.8758\n",
            "Epoch [2150/5000], Training loss: 1.7346, Validation loss: 1.8731\n",
            "Epoch [2160/5000], Training loss: 1.7339, Validation loss: 1.8747\n",
            "Epoch [2170/5000], Training loss: 1.7359, Validation loss: 1.8748\n",
            "Epoch [2180/5000], Training loss: 1.7371, Validation loss: 1.8754\n",
            "Epoch [2190/5000], Training loss: 1.7343, Validation loss: 1.8756\n",
            "Epoch [2200/5000], Training loss: 1.7318, Validation loss: 1.8756\n",
            "Epoch [2210/5000], Training loss: 1.7324, Validation loss: 1.8755\n",
            "Epoch [2220/5000], Training loss: 1.7359, Validation loss: 1.8766\n",
            "Epoch [2230/5000], Training loss: 1.7355, Validation loss: 1.8770\n",
            "Epoch [2240/5000], Training loss: 1.7331, Validation loss: 1.8769\n",
            "Epoch [2250/5000], Training loss: 1.7378, Validation loss: 1.8765\n",
            "Epoch [2260/5000], Training loss: 1.7327, Validation loss: 1.8781\n",
            "Epoch [2270/5000], Training loss: 1.7328, Validation loss: 1.8776\n",
            "Epoch [2280/5000], Training loss: 1.7348, Validation loss: 1.8779\n",
            "Epoch [2290/5000], Training loss: 1.7325, Validation loss: 1.8787\n",
            "Epoch [2300/5000], Training loss: 1.7349, Validation loss: 1.8783\n",
            "Epoch [2310/5000], Training loss: 1.7329, Validation loss: 1.8780\n",
            "Epoch [2320/5000], Training loss: 1.7295, Validation loss: 1.8770\n",
            "Epoch [2330/5000], Training loss: 1.7342, Validation loss: 1.8792\n",
            "Epoch [2340/5000], Training loss: 1.7342, Validation loss: 1.8796\n",
            "Epoch [2350/5000], Training loss: 1.7331, Validation loss: 1.8780\n",
            "Epoch [2360/5000], Training loss: 1.7335, Validation loss: 1.8778\n",
            "Epoch [2370/5000], Training loss: 1.7343, Validation loss: 1.8795\n",
            "Epoch [2380/5000], Training loss: 1.7337, Validation loss: 1.8802\n",
            "Epoch [2390/5000], Training loss: 1.7304, Validation loss: 1.8801\n",
            "Epoch [2400/5000], Training loss: 1.7313, Validation loss: 1.8791\n",
            "Epoch [2410/5000], Training loss: 1.7321, Validation loss: 1.8801\n",
            "Epoch [2420/5000], Training loss: 1.7308, Validation loss: 1.8784\n",
            "Epoch [2430/5000], Training loss: 1.7314, Validation loss: 1.8808\n",
            "Epoch [2440/5000], Training loss: 1.7339, Validation loss: 1.8802\n",
            "Epoch [2450/5000], Training loss: 1.7332, Validation loss: 1.8783\n",
            "Epoch [2460/5000], Training loss: 1.7335, Validation loss: 1.8793\n",
            "Epoch [2470/5000], Training loss: 1.7300, Validation loss: 1.8789\n",
            "Epoch [2480/5000], Training loss: 1.7324, Validation loss: 1.8793\n",
            "Epoch [2490/5000], Training loss: 1.7320, Validation loss: 1.8806\n",
            "Epoch [2500/5000], Training loss: 1.7322, Validation loss: 1.8805\n",
            "Epoch [2510/5000], Training loss: 1.7339, Validation loss: 1.8815\n",
            "Epoch [2520/5000], Training loss: 1.7296, Validation loss: 1.8803\n",
            "Epoch [2530/5000], Training loss: 1.7333, Validation loss: 1.8810\n",
            "Epoch [2540/5000], Training loss: 1.7300, Validation loss: 1.8815\n",
            "Epoch [2550/5000], Training loss: 1.7318, Validation loss: 1.8806\n",
            "Epoch [2560/5000], Training loss: 1.7280, Validation loss: 1.8787\n",
            "Epoch [2570/5000], Training loss: 1.7304, Validation loss: 1.8822\n",
            "Epoch [2580/5000], Training loss: 1.7288, Validation loss: 1.8810\n",
            "Epoch [2590/5000], Training loss: 1.7314, Validation loss: 1.8804\n",
            "Epoch [2600/5000], Training loss: 1.7308, Validation loss: 1.8796\n",
            "Epoch [2610/5000], Training loss: 1.7307, Validation loss: 1.8820\n",
            "Epoch [2620/5000], Training loss: 1.7291, Validation loss: 1.8818\n",
            "Epoch [2630/5000], Training loss: 1.7277, Validation loss: 1.8834\n",
            "Epoch [2640/5000], Training loss: 1.7315, Validation loss: 1.8825\n",
            "Epoch [2650/5000], Training loss: 1.7282, Validation loss: 1.8823\n",
            "Epoch [2660/5000], Training loss: 1.7297, Validation loss: 1.8808\n",
            "Epoch [2670/5000], Training loss: 1.7301, Validation loss: 1.8829\n",
            "Epoch [2680/5000], Training loss: 1.7341, Validation loss: 1.8814\n",
            "Epoch [2690/5000], Training loss: 1.7281, Validation loss: 1.8815\n",
            "Epoch [2700/5000], Training loss: 1.7306, Validation loss: 1.8810\n",
            "Epoch [2710/5000], Training loss: 1.7280, Validation loss: 1.8832\n",
            "Epoch [2720/5000], Training loss: 1.7296, Validation loss: 1.8837\n",
            "Epoch [2730/5000], Training loss: 1.7302, Validation loss: 1.8838\n",
            "Epoch [2740/5000], Training loss: 1.7291, Validation loss: 1.8828\n",
            "Epoch [2750/5000], Training loss: 1.7289, Validation loss: 1.8834\n",
            "Epoch [2760/5000], Training loss: 1.7276, Validation loss: 1.8835\n",
            "Epoch [2770/5000], Training loss: 1.7323, Validation loss: 1.8830\n",
            "Epoch [2780/5000], Training loss: 1.7310, Validation loss: 1.8849\n",
            "Epoch [2790/5000], Training loss: 1.7277, Validation loss: 1.8831\n",
            "Epoch [2800/5000], Training loss: 1.7306, Validation loss: 1.8823\n",
            "Epoch [2810/5000], Training loss: 1.7300, Validation loss: 1.8835\n",
            "Epoch [2820/5000], Training loss: 1.7304, Validation loss: 1.8846\n",
            "Epoch [2830/5000], Training loss: 1.7280, Validation loss: 1.8834\n",
            "Epoch [2840/5000], Training loss: 1.7279, Validation loss: 1.8857\n",
            "Epoch [2850/5000], Training loss: 1.7271, Validation loss: 1.8840\n",
            "Epoch [2860/5000], Training loss: 1.7282, Validation loss: 1.8841\n",
            "Epoch [2870/5000], Training loss: 1.7292, Validation loss: 1.8848\n",
            "Epoch [2880/5000], Training loss: 1.7298, Validation loss: 1.8847\n",
            "Epoch [2890/5000], Training loss: 1.7276, Validation loss: 1.8846\n",
            "Epoch [2900/5000], Training loss: 1.7291, Validation loss: 1.8836\n",
            "Epoch [2910/5000], Training loss: 1.7288, Validation loss: 1.8855\n",
            "Epoch [2920/5000], Training loss: 1.7280, Validation loss: 1.8847\n",
            "Epoch [2930/5000], Training loss: 1.7268, Validation loss: 1.8871\n",
            "Epoch [2940/5000], Training loss: 1.7270, Validation loss: 1.8863\n",
            "Epoch [2950/5000], Training loss: 1.7267, Validation loss: 1.8862\n",
            "Epoch [2960/5000], Training loss: 1.7277, Validation loss: 1.8855\n",
            "Epoch [2970/5000], Training loss: 1.7307, Validation loss: 1.8861\n",
            "Epoch [2980/5000], Training loss: 1.7270, Validation loss: 1.8853\n",
            "Epoch [2990/5000], Training loss: 1.7270, Validation loss: 1.8852\n",
            "Epoch [3000/5000], Training loss: 1.7294, Validation loss: 1.8870\n",
            "Epoch [3010/5000], Training loss: 1.7270, Validation loss: 1.8869\n",
            "Epoch [3020/5000], Training loss: 1.7280, Validation loss: 1.8883\n",
            "Epoch [3030/5000], Training loss: 1.7288, Validation loss: 1.8882\n",
            "Epoch [3040/5000], Training loss: 1.7315, Validation loss: 1.8873\n",
            "Epoch [3050/5000], Training loss: 1.7251, Validation loss: 1.8867\n",
            "Epoch [3060/5000], Training loss: 1.7288, Validation loss: 1.8864\n",
            "Epoch [3070/5000], Training loss: 1.7288, Validation loss: 1.8864\n",
            "Epoch [3080/5000], Training loss: 1.7262, Validation loss: 1.8858\n",
            "Epoch [3090/5000], Training loss: 1.7271, Validation loss: 1.8880\n",
            "Epoch [3100/5000], Training loss: 1.7274, Validation loss: 1.8869\n",
            "Epoch [3110/5000], Training loss: 1.7282, Validation loss: 1.8865\n",
            "Epoch [3120/5000], Training loss: 1.7276, Validation loss: 1.8875\n",
            "Epoch [3130/5000], Training loss: 1.7284, Validation loss: 1.8867\n",
            "Epoch [3140/5000], Training loss: 1.7294, Validation loss: 1.8852\n",
            "Epoch [3150/5000], Training loss: 1.7279, Validation loss: 1.8876\n",
            "Epoch [3160/5000], Training loss: 1.7260, Validation loss: 1.8878\n",
            "Epoch [3170/5000], Training loss: 1.7248, Validation loss: 1.8871\n",
            "Epoch [3180/5000], Training loss: 1.7243, Validation loss: 1.8879\n",
            "Epoch [3190/5000], Training loss: 1.7252, Validation loss: 1.8885\n",
            "Epoch [3200/5000], Training loss: 1.7276, Validation loss: 1.8863\n",
            "Epoch [3210/5000], Training loss: 1.7256, Validation loss: 1.8882\n",
            "Epoch [3220/5000], Training loss: 1.7279, Validation loss: 1.8887\n",
            "Epoch [3230/5000], Training loss: 1.7253, Validation loss: 1.8887\n",
            "Epoch [3240/5000], Training loss: 1.7286, Validation loss: 1.8898\n",
            "Epoch [3250/5000], Training loss: 1.7253, Validation loss: 1.8883\n",
            "Epoch [3260/5000], Training loss: 1.7286, Validation loss: 1.8888\n",
            "Epoch [3270/5000], Training loss: 1.7281, Validation loss: 1.8888\n",
            "Epoch [3280/5000], Training loss: 1.7226, Validation loss: 1.8903\n",
            "Epoch [3290/5000], Training loss: 1.7243, Validation loss: 1.8901\n",
            "Epoch [3300/5000], Training loss: 1.7260, Validation loss: 1.8887\n",
            "Epoch [3310/5000], Training loss: 1.7258, Validation loss: 1.8891\n",
            "Epoch [3320/5000], Training loss: 1.7251, Validation loss: 1.8897\n",
            "Epoch [3330/5000], Training loss: 1.7268, Validation loss: 1.8904\n",
            "Epoch [3340/5000], Training loss: 1.7234, Validation loss: 1.8911\n",
            "Epoch [3350/5000], Training loss: 1.7254, Validation loss: 1.8885\n",
            "Epoch [3360/5000], Training loss: 1.7236, Validation loss: 1.8912\n",
            "Epoch [3370/5000], Training loss: 1.7257, Validation loss: 1.8893\n",
            "Epoch [3380/5000], Training loss: 1.7255, Validation loss: 1.8883\n",
            "Epoch [3390/5000], Training loss: 1.7249, Validation loss: 1.8890\n",
            "Epoch [3400/5000], Training loss: 1.7258, Validation loss: 1.8891\n",
            "Epoch [3410/5000], Training loss: 1.7252, Validation loss: 1.8898\n",
            "Epoch [3420/5000], Training loss: 1.7217, Validation loss: 1.8910\n",
            "Epoch [3430/5000], Training loss: 1.7261, Validation loss: 1.8907\n",
            "Epoch [3440/5000], Training loss: 1.7226, Validation loss: 1.8920\n",
            "Epoch [3450/5000], Training loss: 1.7270, Validation loss: 1.8920\n",
            "Epoch [3460/5000], Training loss: 1.7245, Validation loss: 1.8903\n",
            "Epoch [3470/5000], Training loss: 1.7235, Validation loss: 1.8920\n",
            "Epoch [3480/5000], Training loss: 1.7263, Validation loss: 1.8910\n",
            "Epoch [3490/5000], Training loss: 1.7275, Validation loss: 1.8894\n",
            "Epoch [3500/5000], Training loss: 1.7223, Validation loss: 1.8896\n",
            "Epoch [3510/5000], Training loss: 1.7236, Validation loss: 1.8898\n",
            "Epoch [3520/5000], Training loss: 1.7230, Validation loss: 1.8903\n",
            "Epoch [3530/5000], Training loss: 1.7218, Validation loss: 1.8902\n",
            "Epoch [3540/5000], Training loss: 1.7263, Validation loss: 1.8919\n",
            "Epoch [3550/5000], Training loss: 1.7244, Validation loss: 1.8903\n",
            "Epoch [3560/5000], Training loss: 1.7245, Validation loss: 1.8908\n",
            "Epoch [3570/5000], Training loss: 1.7250, Validation loss: 1.8888\n",
            "Epoch [3580/5000], Training loss: 1.7270, Validation loss: 1.8906\n",
            "Epoch [3590/5000], Training loss: 1.7260, Validation loss: 1.8888\n",
            "Epoch [3600/5000], Training loss: 1.7241, Validation loss: 1.8915\n",
            "Epoch [3610/5000], Training loss: 1.7261, Validation loss: 1.8902\n",
            "Epoch [3620/5000], Training loss: 1.7223, Validation loss: 1.8908\n",
            "Epoch [3630/5000], Training loss: 1.7238, Validation loss: 1.8907\n",
            "Epoch [3640/5000], Training loss: 1.7272, Validation loss: 1.8898\n",
            "Epoch [3650/5000], Training loss: 1.7218, Validation loss: 1.8901\n",
            "Epoch [3660/5000], Training loss: 1.7258, Validation loss: 1.8922\n",
            "Epoch [3670/5000], Training loss: 1.7247, Validation loss: 1.8916\n",
            "Epoch [3680/5000], Training loss: 1.7243, Validation loss: 1.8926\n",
            "Epoch [3690/5000], Training loss: 1.7245, Validation loss: 1.8894\n",
            "Epoch [3700/5000], Training loss: 1.7201, Validation loss: 1.8909\n",
            "Epoch [3710/5000], Training loss: 1.7276, Validation loss: 1.8922\n",
            "Epoch [3720/5000], Training loss: 1.7273, Validation loss: 1.8938\n",
            "Epoch [3730/5000], Training loss: 1.7274, Validation loss: 1.8925\n",
            "Epoch [3740/5000], Training loss: 1.7226, Validation loss: 1.8931\n",
            "Epoch [3750/5000], Training loss: 1.7250, Validation loss: 1.8920\n",
            "Epoch [3760/5000], Training loss: 1.7239, Validation loss: 1.8900\n",
            "Epoch [3770/5000], Training loss: 1.7240, Validation loss: 1.8935\n",
            "Epoch [3780/5000], Training loss: 1.7235, Validation loss: 1.8933\n",
            "Epoch [3790/5000], Training loss: 1.7220, Validation loss: 1.8945\n",
            "Epoch [3800/5000], Training loss: 1.7227, Validation loss: 1.8932\n",
            "Epoch [3810/5000], Training loss: 1.7209, Validation loss: 1.8915\n",
            "Epoch [3820/5000], Training loss: 1.7249, Validation loss: 1.8920\n",
            "Epoch [3830/5000], Training loss: 1.7250, Validation loss: 1.8939\n",
            "Epoch [3840/5000], Training loss: 1.7233, Validation loss: 1.8928\n",
            "Epoch [3850/5000], Training loss: 1.7233, Validation loss: 1.8934\n",
            "Epoch [3860/5000], Training loss: 1.7236, Validation loss: 1.8933\n",
            "Epoch [3870/5000], Training loss: 1.7248, Validation loss: 1.8933\n",
            "Epoch [3880/5000], Training loss: 1.7207, Validation loss: 1.8930\n",
            "Epoch [3890/5000], Training loss: 1.7215, Validation loss: 1.8946\n",
            "Epoch [3900/5000], Training loss: 1.7228, Validation loss: 1.8956\n",
            "Epoch [3910/5000], Training loss: 1.7244, Validation loss: 1.8927\n",
            "Epoch [3920/5000], Training loss: 1.7231, Validation loss: 1.8920\n",
            "Epoch [3930/5000], Training loss: 1.7221, Validation loss: 1.8937\n",
            "Epoch [3940/5000], Training loss: 1.7253, Validation loss: 1.8928\n",
            "Epoch [3950/5000], Training loss: 1.7224, Validation loss: 1.8944\n",
            "Epoch [3960/5000], Training loss: 1.7210, Validation loss: 1.8942\n",
            "Epoch [3970/5000], Training loss: 1.7217, Validation loss: 1.8936\n",
            "Epoch [3980/5000], Training loss: 1.7201, Validation loss: 1.8946\n",
            "Epoch [3990/5000], Training loss: 1.7254, Validation loss: 1.8927\n",
            "Epoch [4000/5000], Training loss: 1.7212, Validation loss: 1.8935\n",
            "Epoch [4010/5000], Training loss: 1.7232, Validation loss: 1.8935\n",
            "Epoch [4020/5000], Training loss: 1.7203, Validation loss: 1.8956\n",
            "Epoch [4030/5000], Training loss: 1.7195, Validation loss: 1.8929\n",
            "Epoch [4040/5000], Training loss: 1.7199, Validation loss: 1.8950\n",
            "Epoch [4050/5000], Training loss: 1.7225, Validation loss: 1.8935\n",
            "Epoch [4060/5000], Training loss: 1.7189, Validation loss: 1.8940\n",
            "Epoch [4070/5000], Training loss: 1.7245, Validation loss: 1.8945\n",
            "Epoch [4080/5000], Training loss: 1.7237, Validation loss: 1.8939\n",
            "Epoch [4090/5000], Training loss: 1.7215, Validation loss: 1.8940\n",
            "Epoch [4100/5000], Training loss: 1.7222, Validation loss: 1.8935\n",
            "Epoch [4110/5000], Training loss: 1.7191, Validation loss: 1.8946\n",
            "Epoch [4120/5000], Training loss: 1.7212, Validation loss: 1.8945\n",
            "Epoch [4130/5000], Training loss: 1.7214, Validation loss: 1.8959\n",
            "Epoch [4140/5000], Training loss: 1.7243, Validation loss: 1.8959\n",
            "Epoch [4150/5000], Training loss: 1.7217, Validation loss: 1.8953\n",
            "Epoch [4160/5000], Training loss: 1.7212, Validation loss: 1.8921\n",
            "Epoch [4170/5000], Training loss: 1.7215, Validation loss: 1.8974\n",
            "Epoch [4180/5000], Training loss: 1.7240, Validation loss: 1.8956\n",
            "Epoch [4190/5000], Training loss: 1.7230, Validation loss: 1.8954\n",
            "Epoch [4200/5000], Training loss: 1.7211, Validation loss: 1.8946\n",
            "Epoch [4210/5000], Training loss: 1.7234, Validation loss: 1.8951\n",
            "Epoch [4220/5000], Training loss: 1.7208, Validation loss: 1.8958\n",
            "Epoch [4230/5000], Training loss: 1.7220, Validation loss: 1.8969\n",
            "Epoch [4240/5000], Training loss: 1.7189, Validation loss: 1.8975\n",
            "Epoch [4250/5000], Training loss: 1.7232, Validation loss: 1.8956\n",
            "Epoch [4260/5000], Training loss: 1.7204, Validation loss: 1.8979\n",
            "Epoch [4270/5000], Training loss: 1.7210, Validation loss: 1.8965\n",
            "Epoch [4280/5000], Training loss: 1.7233, Validation loss: 1.8947\n",
            "Epoch [4290/5000], Training loss: 1.7167, Validation loss: 1.8963\n",
            "Epoch [4300/5000], Training loss: 1.7219, Validation loss: 1.8964\n",
            "Epoch [4310/5000], Training loss: 1.7214, Validation loss: 1.8949\n",
            "Epoch [4320/5000], Training loss: 1.7245, Validation loss: 1.8984\n",
            "Epoch [4330/5000], Training loss: 1.7225, Validation loss: 1.8966\n",
            "Epoch [4340/5000], Training loss: 1.7223, Validation loss: 1.8961\n",
            "Epoch [4350/5000], Training loss: 1.7194, Validation loss: 1.8954\n",
            "Epoch [4360/5000], Training loss: 1.7224, Validation loss: 1.8955\n",
            "Epoch [4370/5000], Training loss: 1.7220, Validation loss: 1.8948\n",
            "Epoch [4380/5000], Training loss: 1.7190, Validation loss: 1.8955\n",
            "Epoch [4390/5000], Training loss: 1.7217, Validation loss: 1.8980\n",
            "Epoch [4400/5000], Training loss: 1.7228, Validation loss: 1.8958\n",
            "Epoch [4410/5000], Training loss: 1.7205, Validation loss: 1.8976\n",
            "Epoch [4420/5000], Training loss: 1.7222, Validation loss: 1.8961\n",
            "Epoch [4430/5000], Training loss: 1.7197, Validation loss: 1.8976\n",
            "Epoch [4440/5000], Training loss: 1.7220, Validation loss: 1.8967\n",
            "Epoch [4450/5000], Training loss: 1.7225, Validation loss: 1.8970\n",
            "Epoch [4460/5000], Training loss: 1.7197, Validation loss: 1.8960\n",
            "Epoch [4470/5000], Training loss: 1.7202, Validation loss: 1.8986\n",
            "Epoch [4480/5000], Training loss: 1.7210, Validation loss: 1.8973\n",
            "Epoch [4490/5000], Training loss: 1.7220, Validation loss: 1.8994\n",
            "Epoch [4500/5000], Training loss: 1.7192, Validation loss: 1.8981\n",
            "Epoch [4510/5000], Training loss: 1.7208, Validation loss: 1.8977\n",
            "Epoch [4520/5000], Training loss: 1.7166, Validation loss: 1.8975\n",
            "Epoch [4530/5000], Training loss: 1.7182, Validation loss: 1.8979\n",
            "Epoch [4540/5000], Training loss: 1.7191, Validation loss: 1.8974\n",
            "Epoch [4550/5000], Training loss: 1.7170, Validation loss: 1.8976\n",
            "Epoch [4560/5000], Training loss: 1.7225, Validation loss: 1.8994\n",
            "Epoch [4570/5000], Training loss: 1.7186, Validation loss: 1.8977\n",
            "Epoch [4580/5000], Training loss: 1.7190, Validation loss: 1.8958\n",
            "Epoch [4590/5000], Training loss: 1.7234, Validation loss: 1.8981\n",
            "Epoch [4600/5000], Training loss: 1.7234, Validation loss: 1.8992\n",
            "Epoch [4610/5000], Training loss: 1.7194, Validation loss: 1.8973\n",
            "Epoch [4620/5000], Training loss: 1.7192, Validation loss: 1.8985\n",
            "Epoch [4630/5000], Training loss: 1.7221, Validation loss: 1.8987\n",
            "Epoch [4640/5000], Training loss: 1.7206, Validation loss: 1.8995\n",
            "Epoch [4650/5000], Training loss: 1.7225, Validation loss: 1.9002\n",
            "Epoch [4660/5000], Training loss: 1.7204, Validation loss: 1.8985\n",
            "Epoch [4670/5000], Training loss: 1.7204, Validation loss: 1.9002\n",
            "Epoch [4680/5000], Training loss: 1.7199, Validation loss: 1.8993\n",
            "Epoch [4690/5000], Training loss: 1.7182, Validation loss: 1.9002\n",
            "Epoch [4700/5000], Training loss: 1.7210, Validation loss: 1.8999\n",
            "Epoch [4710/5000], Training loss: 1.7217, Validation loss: 1.8990\n",
            "Epoch [4720/5000], Training loss: 1.7205, Validation loss: 1.9013\n",
            "Epoch [4730/5000], Training loss: 1.7202, Validation loss: 1.8988\n",
            "Epoch [4740/5000], Training loss: 1.7186, Validation loss: 1.8978\n",
            "Epoch [4750/5000], Training loss: 1.7192, Validation loss: 1.9018\n",
            "Epoch [4760/5000], Training loss: 1.7240, Validation loss: 1.8984\n",
            "Epoch [4770/5000], Training loss: 1.7207, Validation loss: 1.9004\n",
            "Epoch [4780/5000], Training loss: 1.7211, Validation loss: 1.8997\n",
            "Epoch [4790/5000], Training loss: 1.7204, Validation loss: 1.8995\n",
            "Epoch [4800/5000], Training loss: 1.7194, Validation loss: 1.8986\n",
            "Epoch [4810/5000], Training loss: 1.7201, Validation loss: 1.8999\n",
            "Epoch [4820/5000], Training loss: 1.7216, Validation loss: 1.8980\n",
            "Epoch [4830/5000], Training loss: 1.7181, Validation loss: 1.8971\n",
            "Epoch [4840/5000], Training loss: 1.7191, Validation loss: 1.8987\n",
            "Epoch [4850/5000], Training loss: 1.7204, Validation loss: 1.8994\n",
            "Epoch [4860/5000], Training loss: 1.7173, Validation loss: 1.8974\n",
            "Epoch [4870/5000], Training loss: 1.7184, Validation loss: 1.9000\n",
            "Epoch [4880/5000], Training loss: 1.7216, Validation loss: 1.8996\n",
            "Epoch [4890/5000], Training loss: 1.7192, Validation loss: 1.8985\n",
            "Epoch [4900/5000], Training loss: 1.7183, Validation loss: 1.9020\n",
            "Epoch [4910/5000], Training loss: 1.7195, Validation loss: 1.9023\n",
            "Epoch [4920/5000], Training loss: 1.7197, Validation loss: 1.8995\n",
            "Epoch [4930/5000], Training loss: 1.7171, Validation loss: 1.8999\n",
            "Epoch [4940/5000], Training loss: 1.7179, Validation loss: 1.9000\n",
            "Epoch [4950/5000], Training loss: 1.7192, Validation loss: 1.8977\n",
            "Epoch [4960/5000], Training loss: 1.7174, Validation loss: 1.9003\n",
            "Epoch [4970/5000], Training loss: 1.7185, Validation loss: 1.9009\n",
            "Epoch [4980/5000], Training loss: 1.7169, Validation loss: 1.9001\n",
            "Epoch [4990/5000], Training loss: 1.7160, Validation loss: 1.8984\n",
            "Epoch [5000/5000], Training loss: 1.7189, Validation loss: 1.8997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "rnn_train_loss, rnn_losses = train(vocab_size, rnn, criterion, optimizer_rnn, epochs, train_input_sequences, train_output_sequences, val_input_sequences, val_output_sequences)"
      ],
      "metadata": {
        "id": "AzXovpPVHIpG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81cad202-84f5-4cf9-86be-10a8ac8661b0"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/5000], Training loss: 2.2808, Validation loss: 2.2784\n",
            "Epoch [20/5000], Training loss: 2.2455, Validation loss: 2.2440\n",
            "Epoch [30/5000], Training loss: 2.2156, Validation loss: 2.2159\n",
            "Epoch [40/5000], Training loss: 2.1910, Validation loss: 2.1950\n",
            "Epoch [50/5000], Training loss: 2.1681, Validation loss: 2.1778\n",
            "Epoch [60/5000], Training loss: 2.1499, Validation loss: 2.1633\n",
            "Epoch [70/5000], Training loss: 2.1345, Validation loss: 2.1504\n",
            "Epoch [80/5000], Training loss: 2.1209, Validation loss: 2.1366\n",
            "Epoch [90/5000], Training loss: 2.1082, Validation loss: 2.1242\n",
            "Epoch [100/5000], Training loss: 2.0964, Validation loss: 2.1137\n",
            "Epoch [110/5000], Training loss: 2.0827, Validation loss: 2.1041\n",
            "Epoch [120/5000], Training loss: 2.0712, Validation loss: 2.0941\n",
            "Epoch [130/5000], Training loss: 2.0573, Validation loss: 2.0840\n",
            "Epoch [140/5000], Training loss: 2.0442, Validation loss: 2.0735\n",
            "Epoch [150/5000], Training loss: 2.0344, Validation loss: 2.0654\n",
            "Epoch [160/5000], Training loss: 2.0217, Validation loss: 2.0532\n",
            "Epoch [170/5000], Training loss: 2.0064, Validation loss: 2.0386\n",
            "Epoch [180/5000], Training loss: 1.9887, Validation loss: 2.0145\n",
            "Epoch [190/5000], Training loss: 1.9603, Validation loss: 1.9798\n",
            "Epoch [200/5000], Training loss: 1.9285, Validation loss: 1.9326\n",
            "Epoch [210/5000], Training loss: 1.8908, Validation loss: 1.8841\n",
            "Epoch [220/5000], Training loss: 1.8522, Validation loss: 1.8420\n",
            "Epoch [230/5000], Training loss: 1.8209, Validation loss: 1.8027\n",
            "Epoch [240/5000], Training loss: 1.7884, Validation loss: 1.7664\n",
            "Epoch [250/5000], Training loss: 1.7578, Validation loss: 1.7332\n",
            "Epoch [260/5000], Training loss: 1.7319, Validation loss: 1.7012\n",
            "Epoch [270/5000], Training loss: 1.6988, Validation loss: 1.6673\n",
            "Epoch [280/5000], Training loss: 1.6750, Validation loss: 1.6373\n",
            "Epoch [290/5000], Training loss: 1.6504, Validation loss: 1.6080\n",
            "Epoch [300/5000], Training loss: 1.6245, Validation loss: 1.5814\n",
            "Epoch [310/5000], Training loss: 1.6041, Validation loss: 1.5564\n",
            "Epoch [320/5000], Training loss: 1.5829, Validation loss: 1.5331\n",
            "Epoch [330/5000], Training loss: 1.5661, Validation loss: 1.5110\n",
            "Epoch [340/5000], Training loss: 1.5471, Validation loss: 1.4885\n",
            "Epoch [350/5000], Training loss: 1.5297, Validation loss: 1.4678\n",
            "Epoch [360/5000], Training loss: 1.5112, Validation loss: 1.4494\n",
            "Epoch [370/5000], Training loss: 1.4946, Validation loss: 1.4311\n",
            "Epoch [380/5000], Training loss: 1.4806, Validation loss: 1.4162\n",
            "Epoch [390/5000], Training loss: 1.4607, Validation loss: 1.3984\n",
            "Epoch [400/5000], Training loss: 1.4499, Validation loss: 1.3840\n",
            "Epoch [410/5000], Training loss: 1.4369, Validation loss: 1.3709\n",
            "Epoch [420/5000], Training loss: 1.4204, Validation loss: 1.3544\n",
            "Epoch [430/5000], Training loss: 1.4114, Validation loss: 1.3423\n",
            "Epoch [440/5000], Training loss: 1.3941, Validation loss: 1.3269\n",
            "Epoch [450/5000], Training loss: 1.3843, Validation loss: 1.3157\n",
            "Epoch [460/5000], Training loss: 1.3739, Validation loss: 1.3022\n",
            "Epoch [470/5000], Training loss: 1.3632, Validation loss: 1.2912\n",
            "Epoch [480/5000], Training loss: 1.3470, Validation loss: 1.2795\n",
            "Epoch [490/5000], Training loss: 1.3433, Validation loss: 1.2700\n",
            "Epoch [500/5000], Training loss: 1.3277, Validation loss: 1.2589\n",
            "Epoch [510/5000], Training loss: 1.3177, Validation loss: 1.2509\n",
            "Epoch [520/5000], Training loss: 1.3124, Validation loss: 1.2432\n",
            "Epoch [530/5000], Training loss: 1.3019, Validation loss: 1.2353\n",
            "Epoch [540/5000], Training loss: 1.2988, Validation loss: 1.2298\n",
            "Epoch [550/5000], Training loss: 1.2863, Validation loss: 1.2226\n",
            "Epoch [560/5000], Training loss: 1.2794, Validation loss: 1.2179\n",
            "Epoch [570/5000], Training loss: 1.2699, Validation loss: 1.2127\n",
            "Epoch [580/5000], Training loss: 1.2662, Validation loss: 1.2078\n",
            "Epoch [590/5000], Training loss: 1.2613, Validation loss: 1.2041\n",
            "Epoch [600/5000], Training loss: 1.2573, Validation loss: 1.2002\n",
            "Epoch [610/5000], Training loss: 1.2510, Validation loss: 1.1969\n",
            "Epoch [620/5000], Training loss: 1.2456, Validation loss: 1.1945\n",
            "Epoch [630/5000], Training loss: 1.2396, Validation loss: 1.1915\n",
            "Epoch [640/5000], Training loss: 1.2394, Validation loss: 1.1889\n",
            "Epoch [650/5000], Training loss: 1.2328, Validation loss: 1.1866\n",
            "Epoch [660/5000], Training loss: 1.2297, Validation loss: 1.1844\n",
            "Epoch [670/5000], Training loss: 1.2244, Validation loss: 1.1828\n",
            "Epoch [680/5000], Training loss: 1.2239, Validation loss: 1.1808\n",
            "Epoch [690/5000], Training loss: 1.2216, Validation loss: 1.1795\n",
            "Epoch [700/5000], Training loss: 1.2162, Validation loss: 1.1780\n",
            "Epoch [710/5000], Training loss: 1.2170, Validation loss: 1.1761\n",
            "Epoch [720/5000], Training loss: 1.2148, Validation loss: 1.1751\n",
            "Epoch [730/5000], Training loss: 1.2098, Validation loss: 1.1740\n",
            "Epoch [740/5000], Training loss: 1.2076, Validation loss: 1.1731\n",
            "Epoch [750/5000], Training loss: 1.2067, Validation loss: 1.1720\n",
            "Epoch [760/5000], Training loss: 1.2045, Validation loss: 1.1712\n",
            "Epoch [770/5000], Training loss: 1.2028, Validation loss: 1.1704\n",
            "Epoch [780/5000], Training loss: 1.2028, Validation loss: 1.1694\n",
            "Epoch [790/5000], Training loss: 1.1989, Validation loss: 1.1694\n",
            "Epoch [800/5000], Training loss: 1.1968, Validation loss: 1.1683\n",
            "Epoch [810/5000], Training loss: 1.1942, Validation loss: 1.1678\n",
            "Epoch [820/5000], Training loss: 1.1919, Validation loss: 1.1672\n",
            "Epoch [830/5000], Training loss: 1.1939, Validation loss: 1.1665\n",
            "Epoch [840/5000], Training loss: 1.1920, Validation loss: 1.1662\n",
            "Epoch [850/5000], Training loss: 1.1891, Validation loss: 1.1659\n",
            "Epoch [860/5000], Training loss: 1.1884, Validation loss: 1.1653\n",
            "Epoch [870/5000], Training loss: 1.1854, Validation loss: 1.1649\n",
            "Epoch [880/5000], Training loss: 1.1858, Validation loss: 1.1646\n",
            "Epoch [890/5000], Training loss: 1.1854, Validation loss: 1.1645\n",
            "Epoch [900/5000], Training loss: 1.1842, Validation loss: 1.1637\n",
            "Epoch [910/5000], Training loss: 1.1833, Validation loss: 1.1638\n",
            "Epoch [920/5000], Training loss: 1.1805, Validation loss: 1.1632\n",
            "Epoch [930/5000], Training loss: 1.1809, Validation loss: 1.1630\n",
            "Epoch [940/5000], Training loss: 1.1790, Validation loss: 1.1628\n",
            "Epoch [950/5000], Training loss: 1.1806, Validation loss: 1.1622\n",
            "Epoch [960/5000], Training loss: 1.1782, Validation loss: 1.1623\n",
            "Epoch [970/5000], Training loss: 1.1751, Validation loss: 1.1620\n",
            "Epoch [980/5000], Training loss: 1.1752, Validation loss: 1.1615\n",
            "Epoch [990/5000], Training loss: 1.1747, Validation loss: 1.1616\n",
            "Epoch [1000/5000], Training loss: 1.1733, Validation loss: 1.1615\n",
            "Epoch [1010/5000], Training loss: 1.1754, Validation loss: 1.1613\n",
            "Epoch [1020/5000], Training loss: 1.1701, Validation loss: 1.1610\n",
            "Epoch [1030/5000], Training loss: 1.1712, Validation loss: 1.1607\n",
            "Epoch [1040/5000], Training loss: 1.1709, Validation loss: 1.1611\n",
            "Epoch [1050/5000], Training loss: 1.1714, Validation loss: 1.1606\n",
            "Epoch [1060/5000], Training loss: 1.1715, Validation loss: 1.1605\n",
            "Epoch [1070/5000], Training loss: 1.1686, Validation loss: 1.1603\n",
            "Epoch [1080/5000], Training loss: 1.1715, Validation loss: 1.1601\n",
            "Epoch [1090/5000], Training loss: 1.1704, Validation loss: 1.1600\n",
            "Epoch [1100/5000], Training loss: 1.1706, Validation loss: 1.1600\n",
            "Epoch [1110/5000], Training loss: 1.1687, Validation loss: 1.1601\n",
            "Epoch [1120/5000], Training loss: 1.1680, Validation loss: 1.1600\n",
            "Epoch [1130/5000], Training loss: 1.1675, Validation loss: 1.1597\n",
            "Epoch [1140/5000], Training loss: 1.1659, Validation loss: 1.1595\n",
            "Epoch [1150/5000], Training loss: 1.1659, Validation loss: 1.1599\n",
            "Epoch [1160/5000], Training loss: 1.1660, Validation loss: 1.1600\n",
            "Epoch [1170/5000], Training loss: 1.1653, Validation loss: 1.1599\n",
            "Epoch [1180/5000], Training loss: 1.1643, Validation loss: 1.1599\n",
            "Epoch [1190/5000], Training loss: 1.1649, Validation loss: 1.1593\n",
            "Epoch [1200/5000], Training loss: 1.1660, Validation loss: 1.1593\n",
            "Epoch [1210/5000], Training loss: 1.1635, Validation loss: 1.1593\n",
            "Epoch [1220/5000], Training loss: 1.1627, Validation loss: 1.1591\n",
            "Epoch [1230/5000], Training loss: 1.1618, Validation loss: 1.1592\n",
            "Epoch [1240/5000], Training loss: 1.1646, Validation loss: 1.1590\n",
            "Epoch [1250/5000], Training loss: 1.1625, Validation loss: 1.1589\n",
            "Epoch [1260/5000], Training loss: 1.1624, Validation loss: 1.1592\n",
            "Epoch [1270/5000], Training loss: 1.1608, Validation loss: 1.1590\n",
            "Epoch [1280/5000], Training loss: 1.1617, Validation loss: 1.1589\n",
            "Epoch [1290/5000], Training loss: 1.1614, Validation loss: 1.1590\n",
            "Epoch [1300/5000], Training loss: 1.1604, Validation loss: 1.1592\n",
            "Epoch [1310/5000], Training loss: 1.1607, Validation loss: 1.1590\n",
            "Epoch [1320/5000], Training loss: 1.1602, Validation loss: 1.1585\n",
            "Epoch [1330/5000], Training loss: 1.1601, Validation loss: 1.1591\n",
            "Epoch [1340/5000], Training loss: 1.1588, Validation loss: 1.1589\n",
            "Epoch [1350/5000], Training loss: 1.1607, Validation loss: 1.1586\n",
            "Epoch [1360/5000], Training loss: 1.1580, Validation loss: 1.1583\n",
            "Epoch [1370/5000], Training loss: 1.1591, Validation loss: 1.1588\n",
            "Epoch [1380/5000], Training loss: 1.1595, Validation loss: 1.1591\n",
            "Epoch [1390/5000], Training loss: 1.1571, Validation loss: 1.1590\n",
            "Epoch [1400/5000], Training loss: 1.1597, Validation loss: 1.1585\n",
            "Epoch [1410/5000], Training loss: 1.1586, Validation loss: 1.1588\n",
            "Epoch [1420/5000], Training loss: 1.1568, Validation loss: 1.1591\n",
            "Epoch [1430/5000], Training loss: 1.1577, Validation loss: 1.1587\n",
            "Epoch [1440/5000], Training loss: 1.1584, Validation loss: 1.1585\n",
            "Epoch [1450/5000], Training loss: 1.1561, Validation loss: 1.1588\n",
            "Epoch [1460/5000], Training loss: 1.1586, Validation loss: 1.1587\n",
            "Epoch [1470/5000], Training loss: 1.1552, Validation loss: 1.1585\n",
            "Epoch [1480/5000], Training loss: 1.1558, Validation loss: 1.1588\n",
            "Epoch [1490/5000], Training loss: 1.1563, Validation loss: 1.1585\n",
            "Epoch [1500/5000], Training loss: 1.1556, Validation loss: 1.1589\n",
            "Epoch [1510/5000], Training loss: 1.1551, Validation loss: 1.1583\n",
            "Epoch [1520/5000], Training loss: 1.1552, Validation loss: 1.1582\n",
            "Epoch [1530/5000], Training loss: 1.1566, Validation loss: 1.1590\n",
            "Epoch [1540/5000], Training loss: 1.1552, Validation loss: 1.1587\n",
            "Epoch [1550/5000], Training loss: 1.1555, Validation loss: 1.1591\n",
            "Epoch [1560/5000], Training loss: 1.1548, Validation loss: 1.1590\n",
            "Epoch [1570/5000], Training loss: 1.1527, Validation loss: 1.1586\n",
            "Epoch [1580/5000], Training loss: 1.1543, Validation loss: 1.1586\n",
            "Epoch [1590/5000], Training loss: 1.1538, Validation loss: 1.1591\n",
            "Epoch [1600/5000], Training loss: 1.1533, Validation loss: 1.1588\n",
            "Epoch [1610/5000], Training loss: 1.1554, Validation loss: 1.1587\n",
            "Epoch [1620/5000], Training loss: 1.1532, Validation loss: 1.1587\n",
            "Epoch [1630/5000], Training loss: 1.1534, Validation loss: 1.1588\n",
            "Epoch [1640/5000], Training loss: 1.1521, Validation loss: 1.1588\n",
            "Epoch [1650/5000], Training loss: 1.1521, Validation loss: 1.1587\n",
            "Epoch [1660/5000], Training loss: 1.1528, Validation loss: 1.1590\n",
            "Epoch [1670/5000], Training loss: 1.1521, Validation loss: 1.1590\n",
            "Epoch [1680/5000], Training loss: 1.1528, Validation loss: 1.1592\n",
            "Epoch [1690/5000], Training loss: 1.1504, Validation loss: 1.1590\n",
            "Epoch [1700/5000], Training loss: 1.1526, Validation loss: 1.1587\n",
            "Epoch [1710/5000], Training loss: 1.1502, Validation loss: 1.1590\n",
            "Epoch [1720/5000], Training loss: 1.1517, Validation loss: 1.1588\n",
            "Epoch [1730/5000], Training loss: 1.1512, Validation loss: 1.1591\n",
            "Epoch [1740/5000], Training loss: 1.1501, Validation loss: 1.1594\n",
            "Epoch [1750/5000], Training loss: 1.1519, Validation loss: 1.1593\n",
            "Epoch [1760/5000], Training loss: 1.1516, Validation loss: 1.1588\n",
            "Epoch [1770/5000], Training loss: 1.1498, Validation loss: 1.1588\n",
            "Epoch [1780/5000], Training loss: 1.1498, Validation loss: 1.1593\n",
            "Epoch [1790/5000], Training loss: 1.1497, Validation loss: 1.1592\n",
            "Epoch [1800/5000], Training loss: 1.1506, Validation loss: 1.1593\n",
            "Epoch [1810/5000], Training loss: 1.1500, Validation loss: 1.1591\n",
            "Epoch [1820/5000], Training loss: 1.1481, Validation loss: 1.1590\n",
            "Epoch [1830/5000], Training loss: 1.1506, Validation loss: 1.1594\n",
            "Epoch [1840/5000], Training loss: 1.1510, Validation loss: 1.1591\n",
            "Epoch [1850/5000], Training loss: 1.1492, Validation loss: 1.1593\n",
            "Epoch [1860/5000], Training loss: 1.1490, Validation loss: 1.1593\n",
            "Epoch [1870/5000], Training loss: 1.1488, Validation loss: 1.1595\n",
            "Epoch [1880/5000], Training loss: 1.1488, Validation loss: 1.1597\n",
            "Epoch [1890/5000], Training loss: 1.1486, Validation loss: 1.1597\n",
            "Epoch [1900/5000], Training loss: 1.1491, Validation loss: 1.1596\n",
            "Epoch [1910/5000], Training loss: 1.1491, Validation loss: 1.1597\n",
            "Epoch [1920/5000], Training loss: 1.1488, Validation loss: 1.1597\n",
            "Epoch [1930/5000], Training loss: 1.1471, Validation loss: 1.1599\n",
            "Epoch [1940/5000], Training loss: 1.1484, Validation loss: 1.1596\n",
            "Epoch [1950/5000], Training loss: 1.1487, Validation loss: 1.1595\n",
            "Epoch [1960/5000], Training loss: 1.1481, Validation loss: 1.1598\n",
            "Epoch [1970/5000], Training loss: 1.1486, Validation loss: 1.1593\n",
            "Epoch [1980/5000], Training loss: 1.1469, Validation loss: 1.1598\n",
            "Epoch [1990/5000], Training loss: 1.1480, Validation loss: 1.1600\n",
            "Epoch [2000/5000], Training loss: 1.1486, Validation loss: 1.1599\n",
            "Epoch [2010/5000], Training loss: 1.1467, Validation loss: 1.1597\n",
            "Epoch [2020/5000], Training loss: 1.1469, Validation loss: 1.1597\n",
            "Epoch [2030/5000], Training loss: 1.1460, Validation loss: 1.1602\n",
            "Epoch [2040/5000], Training loss: 1.1465, Validation loss: 1.1600\n",
            "Epoch [2050/5000], Training loss: 1.1458, Validation loss: 1.1597\n",
            "Epoch [2060/5000], Training loss: 1.1467, Validation loss: 1.1601\n",
            "Epoch [2070/5000], Training loss: 1.1478, Validation loss: 1.1602\n",
            "Epoch [2080/5000], Training loss: 1.1451, Validation loss: 1.1605\n",
            "Epoch [2090/5000], Training loss: 1.1458, Validation loss: 1.1604\n",
            "Epoch [2100/5000], Training loss: 1.1466, Validation loss: 1.1604\n",
            "Epoch [2110/5000], Training loss: 1.1463, Validation loss: 1.1601\n",
            "Epoch [2120/5000], Training loss: 1.1472, Validation loss: 1.1608\n",
            "Epoch [2130/5000], Training loss: 1.1461, Validation loss: 1.1603\n",
            "Epoch [2140/5000], Training loss: 1.1424, Validation loss: 1.1603\n",
            "Epoch [2150/5000], Training loss: 1.1452, Validation loss: 1.1604\n",
            "Epoch [2160/5000], Training loss: 1.1441, Validation loss: 1.1603\n",
            "Epoch [2170/5000], Training loss: 1.1435, Validation loss: 1.1603\n",
            "Epoch [2180/5000], Training loss: 1.1467, Validation loss: 1.1601\n",
            "Epoch [2190/5000], Training loss: 1.1449, Validation loss: 1.1605\n",
            "Epoch [2200/5000], Training loss: 1.1441, Validation loss: 1.1605\n",
            "Epoch [2210/5000], Training loss: 1.1427, Validation loss: 1.1607\n",
            "Epoch [2220/5000], Training loss: 1.1448, Validation loss: 1.1604\n",
            "Epoch [2230/5000], Training loss: 1.1449, Validation loss: 1.1605\n",
            "Epoch [2240/5000], Training loss: 1.1446, Validation loss: 1.1612\n",
            "Epoch [2250/5000], Training loss: 1.1455, Validation loss: 1.1614\n",
            "Epoch [2260/5000], Training loss: 1.1444, Validation loss: 1.1606\n",
            "Epoch [2270/5000], Training loss: 1.1441, Validation loss: 1.1608\n",
            "Epoch [2280/5000], Training loss: 1.1426, Validation loss: 1.1608\n",
            "Epoch [2290/5000], Training loss: 1.1432, Validation loss: 1.1609\n",
            "Epoch [2300/5000], Training loss: 1.1427, Validation loss: 1.1607\n",
            "Epoch [2310/5000], Training loss: 1.1440, Validation loss: 1.1609\n",
            "Epoch [2320/5000], Training loss: 1.1433, Validation loss: 1.1602\n",
            "Epoch [2330/5000], Training loss: 1.1431, Validation loss: 1.1609\n",
            "Epoch [2340/5000], Training loss: 1.1430, Validation loss: 1.1611\n",
            "Epoch [2350/5000], Training loss: 1.1420, Validation loss: 1.1613\n",
            "Epoch [2360/5000], Training loss: 1.1438, Validation loss: 1.1617\n",
            "Epoch [2370/5000], Training loss: 1.1427, Validation loss: 1.1613\n",
            "Epoch [2380/5000], Training loss: 1.1417, Validation loss: 1.1611\n",
            "Epoch [2390/5000], Training loss: 1.1410, Validation loss: 1.1615\n",
            "Epoch [2400/5000], Training loss: 1.1424, Validation loss: 1.1612\n",
            "Epoch [2410/5000], Training loss: 1.1414, Validation loss: 1.1612\n",
            "Epoch [2420/5000], Training loss: 1.1422, Validation loss: 1.1616\n",
            "Epoch [2430/5000], Training loss: 1.1403, Validation loss: 1.1624\n",
            "Epoch [2440/5000], Training loss: 1.1421, Validation loss: 1.1622\n",
            "Epoch [2450/5000], Training loss: 1.1411, Validation loss: 1.1630\n",
            "Epoch [2460/5000], Training loss: 1.1416, Validation loss: 1.1625\n",
            "Epoch [2470/5000], Training loss: 1.1418, Validation loss: 1.1618\n",
            "Epoch [2480/5000], Training loss: 1.1400, Validation loss: 1.1620\n",
            "Epoch [2490/5000], Training loss: 1.1403, Validation loss: 1.1621\n",
            "Epoch [2500/5000], Training loss: 1.1407, Validation loss: 1.1637\n",
            "Epoch [2510/5000], Training loss: 1.1415, Validation loss: 1.1627\n",
            "Epoch [2520/5000], Training loss: 1.1394, Validation loss: 1.1625\n",
            "Epoch [2530/5000], Training loss: 1.1402, Validation loss: 1.1624\n",
            "Epoch [2540/5000], Training loss: 1.1406, Validation loss: 1.1628\n",
            "Epoch [2550/5000], Training loss: 1.1407, Validation loss: 1.1630\n",
            "Epoch [2560/5000], Training loss: 1.1390, Validation loss: 1.1630\n",
            "Epoch [2570/5000], Training loss: 1.1400, Validation loss: 1.1627\n",
            "Epoch [2580/5000], Training loss: 1.1399, Validation loss: 1.1628\n",
            "Epoch [2590/5000], Training loss: 1.1380, Validation loss: 1.1626\n",
            "Epoch [2600/5000], Training loss: 1.1389, Validation loss: 1.1631\n",
            "Epoch [2610/5000], Training loss: 1.1386, Validation loss: 1.1627\n",
            "Epoch [2620/5000], Training loss: 1.1388, Validation loss: 1.1635\n",
            "Epoch [2630/5000], Training loss: 1.1395, Validation loss: 1.1637\n",
            "Epoch [2640/5000], Training loss: 1.1391, Validation loss: 1.1634\n",
            "Epoch [2650/5000], Training loss: 1.1395, Validation loss: 1.1633\n",
            "Epoch [2660/5000], Training loss: 1.1404, Validation loss: 1.1635\n",
            "Epoch [2670/5000], Training loss: 1.1391, Validation loss: 1.1635\n",
            "Epoch [2680/5000], Training loss: 1.1403, Validation loss: 1.1641\n",
            "Epoch [2690/5000], Training loss: 1.1376, Validation loss: 1.1637\n",
            "Epoch [2700/5000], Training loss: 1.1380, Validation loss: 1.1637\n",
            "Epoch [2710/5000], Training loss: 1.1370, Validation loss: 1.1644\n",
            "Epoch [2720/5000], Training loss: 1.1399, Validation loss: 1.1645\n",
            "Epoch [2730/5000], Training loss: 1.1400, Validation loss: 1.1648\n",
            "Epoch [2740/5000], Training loss: 1.1385, Validation loss: 1.1643\n",
            "Epoch [2750/5000], Training loss: 1.1366, Validation loss: 1.1649\n",
            "Epoch [2760/5000], Training loss: 1.1385, Validation loss: 1.1652\n",
            "Epoch [2770/5000], Training loss: 1.1369, Validation loss: 1.1650\n",
            "Epoch [2780/5000], Training loss: 1.1379, Validation loss: 1.1650\n",
            "Epoch [2790/5000], Training loss: 1.1384, Validation loss: 1.1666\n",
            "Epoch [2800/5000], Training loss: 1.1357, Validation loss: 1.1660\n",
            "Epoch [2810/5000], Training loss: 1.1372, Validation loss: 1.1663\n",
            "Epoch [2820/5000], Training loss: 1.1368, Validation loss: 1.1659\n",
            "Epoch [2830/5000], Training loss: 1.1373, Validation loss: 1.1657\n",
            "Epoch [2840/5000], Training loss: 1.1366, Validation loss: 1.1666\n",
            "Epoch [2850/5000], Training loss: 1.1359, Validation loss: 1.1667\n",
            "Epoch [2860/5000], Training loss: 1.1366, Validation loss: 1.1666\n",
            "Epoch [2870/5000], Training loss: 1.1360, Validation loss: 1.1672\n",
            "Epoch [2880/5000], Training loss: 1.1367, Validation loss: 1.1671\n",
            "Epoch [2890/5000], Training loss: 1.1364, Validation loss: 1.1670\n",
            "Epoch [2900/5000], Training loss: 1.1360, Validation loss: 1.1667\n",
            "Epoch [2910/5000], Training loss: 1.1350, Validation loss: 1.1666\n",
            "Epoch [2920/5000], Training loss: 1.1354, Validation loss: 1.1667\n",
            "Epoch [2930/5000], Training loss: 1.1344, Validation loss: 1.1676\n",
            "Epoch [2940/5000], Training loss: 1.1341, Validation loss: 1.1673\n",
            "Epoch [2950/5000], Training loss: 1.1367, Validation loss: 1.1684\n",
            "Epoch [2960/5000], Training loss: 1.1335, Validation loss: 1.1669\n",
            "Epoch [2970/5000], Training loss: 1.1350, Validation loss: 1.1663\n",
            "Epoch [2980/5000], Training loss: 1.1352, Validation loss: 1.1671\n",
            "Epoch [2990/5000], Training loss: 1.1350, Validation loss: 1.1670\n",
            "Epoch [3000/5000], Training loss: 1.1347, Validation loss: 1.1677\n",
            "Epoch [3010/5000], Training loss: 1.1345, Validation loss: 1.1683\n",
            "Epoch [3020/5000], Training loss: 1.1345, Validation loss: 1.1686\n",
            "Epoch [3030/5000], Training loss: 1.1334, Validation loss: 1.1679\n",
            "Epoch [3040/5000], Training loss: 1.1353, Validation loss: 1.1687\n",
            "Epoch [3050/5000], Training loss: 1.1349, Validation loss: 1.1671\n",
            "Epoch [3060/5000], Training loss: 1.1352, Validation loss: 1.1677\n",
            "Epoch [3070/5000], Training loss: 1.1329, Validation loss: 1.1685\n",
            "Epoch [3080/5000], Training loss: 1.1325, Validation loss: 1.1684\n",
            "Epoch [3090/5000], Training loss: 1.1330, Validation loss: 1.1687\n",
            "Epoch [3100/5000], Training loss: 1.1328, Validation loss: 1.1697\n",
            "Epoch [3110/5000], Training loss: 1.1318, Validation loss: 1.1695\n",
            "Epoch [3120/5000], Training loss: 1.1321, Validation loss: 1.1698\n",
            "Epoch [3130/5000], Training loss: 1.1340, Validation loss: 1.1698\n",
            "Epoch [3140/5000], Training loss: 1.1337, Validation loss: 1.1687\n",
            "Epoch [3150/5000], Training loss: 1.1333, Validation loss: 1.1699\n",
            "Epoch [3160/5000], Training loss: 1.1325, Validation loss: 1.1706\n",
            "Epoch [3170/5000], Training loss: 1.1330, Validation loss: 1.1712\n",
            "Epoch [3180/5000], Training loss: 1.1343, Validation loss: 1.1707\n",
            "Epoch [3190/5000], Training loss: 1.1330, Validation loss: 1.1710\n",
            "Epoch [3200/5000], Training loss: 1.1351, Validation loss: 1.1702\n",
            "Epoch [3210/5000], Training loss: 1.1341, Validation loss: 1.1701\n",
            "Epoch [3220/5000], Training loss: 1.1334, Validation loss: 1.1705\n",
            "Epoch [3230/5000], Training loss: 1.1320, Validation loss: 1.1710\n",
            "Epoch [3240/5000], Training loss: 1.1320, Validation loss: 1.1708\n",
            "Epoch [3250/5000], Training loss: 1.1310, Validation loss: 1.1719\n",
            "Epoch [3260/5000], Training loss: 1.1308, Validation loss: 1.1710\n",
            "Epoch [3270/5000], Training loss: 1.1311, Validation loss: 1.1707\n",
            "Epoch [3280/5000], Training loss: 1.1292, Validation loss: 1.1696\n",
            "Epoch [3290/5000], Training loss: 1.1314, Validation loss: 1.1700\n",
            "Epoch [3300/5000], Training loss: 1.1312, Validation loss: 1.1718\n",
            "Epoch [3310/5000], Training loss: 1.1319, Validation loss: 1.1705\n",
            "Epoch [3320/5000], Training loss: 1.1303, Validation loss: 1.1710\n",
            "Epoch [3330/5000], Training loss: 1.1296, Validation loss: 1.1729\n",
            "Epoch [3340/5000], Training loss: 1.1310, Validation loss: 1.1710\n",
            "Epoch [3350/5000], Training loss: 1.1306, Validation loss: 1.1736\n",
            "Epoch [3360/5000], Training loss: 1.1298, Validation loss: 1.1715\n",
            "Epoch [3370/5000], Training loss: 1.1312, Validation loss: 1.1712\n",
            "Epoch [3380/5000], Training loss: 1.1308, Validation loss: 1.1729\n",
            "Epoch [3390/5000], Training loss: 1.1294, Validation loss: 1.1724\n",
            "Epoch [3400/5000], Training loss: 1.1319, Validation loss: 1.1725\n",
            "Epoch [3410/5000], Training loss: 1.1315, Validation loss: 1.1723\n",
            "Epoch [3420/5000], Training loss: 1.1291, Validation loss: 1.1727\n",
            "Epoch [3430/5000], Training loss: 1.1304, Validation loss: 1.1722\n",
            "Epoch [3440/5000], Training loss: 1.1297, Validation loss: 1.1738\n",
            "Epoch [3450/5000], Training loss: 1.1290, Validation loss: 1.1730\n",
            "Epoch [3460/5000], Training loss: 1.1294, Validation loss: 1.1722\n",
            "Epoch [3470/5000], Training loss: 1.1287, Validation loss: 1.1723\n",
            "Epoch [3480/5000], Training loss: 1.1290, Validation loss: 1.1726\n",
            "Epoch [3490/5000], Training loss: 1.1308, Validation loss: 1.1729\n",
            "Epoch [3500/5000], Training loss: 1.1300, Validation loss: 1.1727\n",
            "Epoch [3510/5000], Training loss: 1.1299, Validation loss: 1.1735\n",
            "Epoch [3520/5000], Training loss: 1.1293, Validation loss: 1.1734\n",
            "Epoch [3530/5000], Training loss: 1.1305, Validation loss: 1.1738\n",
            "Epoch [3540/5000], Training loss: 1.1303, Validation loss: 1.1734\n",
            "Epoch [3550/5000], Training loss: 1.1297, Validation loss: 1.1737\n",
            "Epoch [3560/5000], Training loss: 1.1291, Validation loss: 1.1733\n",
            "Epoch [3570/5000], Training loss: 1.1268, Validation loss: 1.1730\n",
            "Epoch [3580/5000], Training loss: 1.1275, Validation loss: 1.1744\n",
            "Epoch [3590/5000], Training loss: 1.1280, Validation loss: 1.1739\n",
            "Epoch [3600/5000], Training loss: 1.1298, Validation loss: 1.1762\n",
            "Epoch [3610/5000], Training loss: 1.1267, Validation loss: 1.1751\n",
            "Epoch [3620/5000], Training loss: 1.1289, Validation loss: 1.1751\n",
            "Epoch [3630/5000], Training loss: 1.1265, Validation loss: 1.1752\n",
            "Epoch [3640/5000], Training loss: 1.1272, Validation loss: 1.1730\n",
            "Epoch [3650/5000], Training loss: 1.1279, Validation loss: 1.1737\n",
            "Epoch [3660/5000], Training loss: 1.1275, Validation loss: 1.1752\n",
            "Epoch [3670/5000], Training loss: 1.1271, Validation loss: 1.1764\n",
            "Epoch [3680/5000], Training loss: 1.1265, Validation loss: 1.1757\n",
            "Epoch [3690/5000], Training loss: 1.1265, Validation loss: 1.1753\n",
            "Epoch [3700/5000], Training loss: 1.1270, Validation loss: 1.1746\n",
            "Epoch [3710/5000], Training loss: 1.1284, Validation loss: 1.1756\n",
            "Epoch [3720/5000], Training loss: 1.1281, Validation loss: 1.1773\n",
            "Epoch [3730/5000], Training loss: 1.1267, Validation loss: 1.1788\n",
            "Epoch [3740/5000], Training loss: 1.1249, Validation loss: 1.1764\n",
            "Epoch [3750/5000], Training loss: 1.1255, Validation loss: 1.1764\n",
            "Epoch [3760/5000], Training loss: 1.1268, Validation loss: 1.1760\n",
            "Epoch [3770/5000], Training loss: 1.1251, Validation loss: 1.1769\n",
            "Epoch [3780/5000], Training loss: 1.1278, Validation loss: 1.1763\n",
            "Epoch [3790/5000], Training loss: 1.1265, Validation loss: 1.1778\n",
            "Epoch [3800/5000], Training loss: 1.1267, Validation loss: 1.1757\n",
            "Epoch [3810/5000], Training loss: 1.1277, Validation loss: 1.1765\n",
            "Epoch [3820/5000], Training loss: 1.1269, Validation loss: 1.1770\n",
            "Epoch [3830/5000], Training loss: 1.1243, Validation loss: 1.1763\n",
            "Epoch [3840/5000], Training loss: 1.1276, Validation loss: 1.1748\n",
            "Epoch [3850/5000], Training loss: 1.1257, Validation loss: 1.1774\n",
            "Epoch [3860/5000], Training loss: 1.1262, Validation loss: 1.1785\n",
            "Epoch [3870/5000], Training loss: 1.1267, Validation loss: 1.1768\n",
            "Epoch [3880/5000], Training loss: 1.1252, Validation loss: 1.1774\n",
            "Epoch [3890/5000], Training loss: 1.1244, Validation loss: 1.1783\n",
            "Epoch [3900/5000], Training loss: 1.1248, Validation loss: 1.1779\n",
            "Epoch [3910/5000], Training loss: 1.1257, Validation loss: 1.1797\n",
            "Epoch [3920/5000], Training loss: 1.1263, Validation loss: 1.1783\n",
            "Epoch [3930/5000], Training loss: 1.1234, Validation loss: 1.1780\n",
            "Epoch [3940/5000], Training loss: 1.1262, Validation loss: 1.1775\n",
            "Epoch [3950/5000], Training loss: 1.1255, Validation loss: 1.1774\n",
            "Epoch [3960/5000], Training loss: 1.1248, Validation loss: 1.1777\n",
            "Epoch [3970/5000], Training loss: 1.1240, Validation loss: 1.1786\n",
            "Epoch [3980/5000], Training loss: 1.1254, Validation loss: 1.1802\n",
            "Epoch [3990/5000], Training loss: 1.1235, Validation loss: 1.1775\n",
            "Epoch [4000/5000], Training loss: 1.1226, Validation loss: 1.1776\n",
            "Epoch [4010/5000], Training loss: 1.1236, Validation loss: 1.1776\n",
            "Epoch [4020/5000], Training loss: 1.1235, Validation loss: 1.1773\n",
            "Epoch [4030/5000], Training loss: 1.1239, Validation loss: 1.1776\n",
            "Epoch [4040/5000], Training loss: 1.1239, Validation loss: 1.1807\n",
            "Epoch [4050/5000], Training loss: 1.1244, Validation loss: 1.1782\n",
            "Epoch [4060/5000], Training loss: 1.1228, Validation loss: 1.1798\n",
            "Epoch [4070/5000], Training loss: 1.1248, Validation loss: 1.1786\n",
            "Epoch [4080/5000], Training loss: 1.1238, Validation loss: 1.1797\n",
            "Epoch [4090/5000], Training loss: 1.1242, Validation loss: 1.1808\n",
            "Epoch [4100/5000], Training loss: 1.1249, Validation loss: 1.1799\n",
            "Epoch [4110/5000], Training loss: 1.1227, Validation loss: 1.1807\n",
            "Epoch [4120/5000], Training loss: 1.1236, Validation loss: 1.1805\n",
            "Epoch [4130/5000], Training loss: 1.1219, Validation loss: 1.1801\n",
            "Epoch [4140/5000], Training loss: 1.1236, Validation loss: 1.1804\n",
            "Epoch [4150/5000], Training loss: 1.1238, Validation loss: 1.1826\n",
            "Epoch [4160/5000], Training loss: 1.1211, Validation loss: 1.1797\n",
            "Epoch [4170/5000], Training loss: 1.1226, Validation loss: 1.1808\n",
            "Epoch [4180/5000], Training loss: 1.1229, Validation loss: 1.1825\n",
            "Epoch [4190/5000], Training loss: 1.1252, Validation loss: 1.1822\n",
            "Epoch [4200/5000], Training loss: 1.1238, Validation loss: 1.1803\n",
            "Epoch [4210/5000], Training loss: 1.1207, Validation loss: 1.1789\n",
            "Epoch [4220/5000], Training loss: 1.1229, Validation loss: 1.1793\n",
            "Epoch [4230/5000], Training loss: 1.1231, Validation loss: 1.1797\n",
            "Epoch [4240/5000], Training loss: 1.1224, Validation loss: 1.1802\n",
            "Epoch [4250/5000], Training loss: 1.1208, Validation loss: 1.1804\n",
            "Epoch [4260/5000], Training loss: 1.1224, Validation loss: 1.1827\n",
            "Epoch [4270/5000], Training loss: 1.1224, Validation loss: 1.1808\n",
            "Epoch [4280/5000], Training loss: 1.1230, Validation loss: 1.1807\n",
            "Epoch [4290/5000], Training loss: 1.1228, Validation loss: 1.1809\n",
            "Epoch [4300/5000], Training loss: 1.1249, Validation loss: 1.1799\n",
            "Epoch [4310/5000], Training loss: 1.1219, Validation loss: 1.1808\n",
            "Epoch [4320/5000], Training loss: 1.1217, Validation loss: 1.1806\n",
            "Epoch [4330/5000], Training loss: 1.1205, Validation loss: 1.1800\n",
            "Epoch [4340/5000], Training loss: 1.1195, Validation loss: 1.1789\n",
            "Epoch [4350/5000], Training loss: 1.1215, Validation loss: 1.1796\n",
            "Epoch [4360/5000], Training loss: 1.1214, Validation loss: 1.1792\n",
            "Epoch [4370/5000], Training loss: 1.1217, Validation loss: 1.1812\n",
            "Epoch [4380/5000], Training loss: 1.1230, Validation loss: 1.1820\n",
            "Epoch [4390/5000], Training loss: 1.1180, Validation loss: 1.1813\n",
            "Epoch [4400/5000], Training loss: 1.1206, Validation loss: 1.1803\n",
            "Epoch [4410/5000], Training loss: 1.1213, Validation loss: 1.1815\n",
            "Epoch [4420/5000], Training loss: 1.1210, Validation loss: 1.1811\n",
            "Epoch [4430/5000], Training loss: 1.1215, Validation loss: 1.1808\n",
            "Epoch [4440/5000], Training loss: 1.1173, Validation loss: 1.1784\n",
            "Epoch [4450/5000], Training loss: 1.1198, Validation loss: 1.1818\n",
            "Epoch [4460/5000], Training loss: 1.1216, Validation loss: 1.1829\n",
            "Epoch [4470/5000], Training loss: 1.1223, Validation loss: 1.1821\n",
            "Epoch [4480/5000], Training loss: 1.1198, Validation loss: 1.1829\n",
            "Epoch [4490/5000], Training loss: 1.1203, Validation loss: 1.1814\n",
            "Epoch [4500/5000], Training loss: 1.1192, Validation loss: 1.1833\n",
            "Epoch [4510/5000], Training loss: 1.1199, Validation loss: 1.1844\n",
            "Epoch [4520/5000], Training loss: 1.1214, Validation loss: 1.1842\n",
            "Epoch [4530/5000], Training loss: 1.1202, Validation loss: 1.1827\n",
            "Epoch [4540/5000], Training loss: 1.1219, Validation loss: 1.1849\n",
            "Epoch [4550/5000], Training loss: 1.1192, Validation loss: 1.1826\n",
            "Epoch [4560/5000], Training loss: 1.1180, Validation loss: 1.1821\n",
            "Epoch [4570/5000], Training loss: 1.1187, Validation loss: 1.1825\n",
            "Epoch [4580/5000], Training loss: 1.1184, Validation loss: 1.1835\n",
            "Epoch [4590/5000], Training loss: 1.1166, Validation loss: 1.1853\n",
            "Epoch [4600/5000], Training loss: 1.1199, Validation loss: 1.1852\n",
            "Epoch [4610/5000], Training loss: 1.1178, Validation loss: 1.1863\n",
            "Epoch [4620/5000], Training loss: 1.1205, Validation loss: 1.1830\n",
            "Epoch [4630/5000], Training loss: 1.1196, Validation loss: 1.1838\n",
            "Epoch [4640/5000], Training loss: 1.1194, Validation loss: 1.1839\n",
            "Epoch [4650/5000], Training loss: 1.1202, Validation loss: 1.1842\n",
            "Epoch [4660/5000], Training loss: 1.1198, Validation loss: 1.1850\n",
            "Epoch [4670/5000], Training loss: 1.1178, Validation loss: 1.1847\n",
            "Epoch [4680/5000], Training loss: 1.1159, Validation loss: 1.1848\n",
            "Epoch [4690/5000], Training loss: 1.1175, Validation loss: 1.1828\n",
            "Epoch [4700/5000], Training loss: 1.1205, Validation loss: 1.1838\n",
            "Epoch [4710/5000], Training loss: 1.1200, Validation loss: 1.1833\n",
            "Epoch [4720/5000], Training loss: 1.1199, Validation loss: 1.1826\n",
            "Epoch [4730/5000], Training loss: 1.1180, Validation loss: 1.1831\n",
            "Epoch [4740/5000], Training loss: 1.1218, Validation loss: 1.1833\n",
            "Epoch [4750/5000], Training loss: 1.1176, Validation loss: 1.1839\n",
            "Epoch [4760/5000], Training loss: 1.1167, Validation loss: 1.1840\n",
            "Epoch [4770/5000], Training loss: 1.1171, Validation loss: 1.1856\n",
            "Epoch [4780/5000], Training loss: 1.1185, Validation loss: 1.1846\n",
            "Epoch [4790/5000], Training loss: 1.1163, Validation loss: 1.1853\n",
            "Epoch [4800/5000], Training loss: 1.1178, Validation loss: 1.1841\n",
            "Epoch [4810/5000], Training loss: 1.1180, Validation loss: 1.1860\n",
            "Epoch [4820/5000], Training loss: 1.1163, Validation loss: 1.1849\n",
            "Epoch [4830/5000], Training loss: 1.1176, Validation loss: 1.1862\n",
            "Epoch [4840/5000], Training loss: 1.1186, Validation loss: 1.1846\n",
            "Epoch [4850/5000], Training loss: 1.1197, Validation loss: 1.1825\n",
            "Epoch [4860/5000], Training loss: 1.1191, Validation loss: 1.1864\n",
            "Epoch [4870/5000], Training loss: 1.1170, Validation loss: 1.1864\n",
            "Epoch [4880/5000], Training loss: 1.1183, Validation loss: 1.1853\n",
            "Epoch [4890/5000], Training loss: 1.1166, Validation loss: 1.1856\n",
            "Epoch [4900/5000], Training loss: 1.1192, Validation loss: 1.1850\n",
            "Epoch [4910/5000], Training loss: 1.1162, Validation loss: 1.1869\n",
            "Epoch [4920/5000], Training loss: 1.1164, Validation loss: 1.1844\n",
            "Epoch [4930/5000], Training loss: 1.1161, Validation loss: 1.1859\n",
            "Epoch [4940/5000], Training loss: 1.1180, Validation loss: 1.1846\n",
            "Epoch [4950/5000], Training loss: 1.1176, Validation loss: 1.1848\n",
            "Epoch [4960/5000], Training loss: 1.1155, Validation loss: 1.1860\n",
            "Epoch [4970/5000], Training loss: 1.1167, Validation loss: 1.1869\n",
            "Epoch [4980/5000], Training loss: 1.1159, Validation loss: 1.1864\n",
            "Epoch [4990/5000], Training loss: 1.1164, Validation loss: 1.1866\n",
            "Epoch [5000/5000], Training loss: 1.1190, Validation loss: 1.1869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(trans_train_loss, label= \"Transformer Train\")\n",
        "plt.plot(trans_losses, label= \"Transformer Val\")\n",
        "plt.plot(rnn_train_loss, label = \"RNN Train\")\n",
        "plt.plot(rnn_losses, label = \"RNN Val\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vgOVYqBQHUE1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "999907c8-9b00-4952-e49b-8d640ace04fe"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}