{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvhL5n76zBqx",
        "outputId": "5d57ec89-3369-4fbf-a46d-c6e649dfddd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov 27 22:28:24 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P0              29W /  70W |    421MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "eW9XY9jbzIAU"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_columns(tensor, indices):\n",
        "    return tensor[:, indices]"
      ],
      "metadata": {
        "id": "hpZs3i98YfGN"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 10 # Sequence length\n",
        "n_train = 2000  # Batch size\n",
        "vocab_size = 10 #vocab_size\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Generate a batch of random input sequences\n",
        "input_sequences = torch.randint(0, vocab_size, (n_train, N)) #n x N\n",
        "\n",
        "A = torch.eye(N).long()\n",
        "\n",
        "perm = torch.randperm(N)\n",
        "A = shuffle_columns(A, perm) #generates a random permutation\n",
        "print(A)\n",
        "\n",
        "output_sequences  = torch.matmul(input_sequences, A)\n",
        "print(torch.max(output_sequences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPWt8UNczJOX",
        "outputId": "7d0a4634-c5d9-4723-9178-30a955892d1b"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
            "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]])\n",
            "tensor(9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_sequences = input_sequences.to(device)\n",
        "train_output_sequences = output_sequences.to(device)"
      ],
      "metadata": {
        "id": "QVQFyY2Zz3Ui"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_test = 400\n",
        "val_input_sequences = torch.randint(0, vocab_size, (n_test, N))\n",
        "val_output_sequences = torch.matmul(val_input_sequences, A)\n",
        "\n",
        "val_input_sequences = val_input_sequences.to(device)\n",
        "val_output_sequences = val_output_sequences.to(device)"
      ],
      "metadata": {
        "id": "6kZhLBI65jJa"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PointwiseFeedforwardNetwork(nn.Module):\n",
        "  def __init__(self, d_embed, d_ff):\n",
        "    super(PointwiseFeedforwardNetwork, self).__init__()\n",
        "    self.d_embed = d_embed\n",
        "    self.d_ff = d_ff\n",
        "\n",
        "    self.ffn1 = nn.Linear(d_embed, d_ff)\n",
        "    self.ffn2 = nn.Linear(d_ff, d_embed)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.ffn2(self.relu(self.ffn1(x)))"
      ],
      "metadata": {
        "id": "897inNwlz5i-"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, d_embed):\n",
        "    super(Attention, self).__init__()\n",
        "\n",
        "    self.d_embed = d_embed\n",
        "    self.query_proj = nn.Linear(d_embed, d_embed).to(device)\n",
        "    self.key_proj = nn.Linear(d_embed, d_embed).to(device)\n",
        "    self.value_proj = nn.Linear(d_embed, d_embed).to(device)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    Q = self.query_proj(x)\n",
        "    K = self.key_proj(x)\n",
        "    V = self.value_proj(x)\n",
        "\n",
        "    scores = torch.matmul(Q, K.transpose(2, 1)) / (self.d_embed ** 0.5)\n",
        "\n",
        "    attention_weights = torch.softmax(scores, dim=-1)\n",
        "\n",
        "    output = torch.matmul(attention_weights, V)\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "IIShZV1Fz7nS"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "  def __init__(self, d_embed, d_ff):\n",
        "    super(TransformerLayer, self).__init__()\n",
        "    self.d_embed = d_embed\n",
        "    self.d_ff = d_ff\n",
        "\n",
        "    self.attn = Attention(self.d_embed)\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(d_embed).to(device)\n",
        "    self.norm2 = nn.LayerNorm(d_embed).to(device)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    self.ffn = PointwiseFeedforwardNetwork(d_embed, d_ff).to(device)\n",
        "    self.ffout = nn.Linear(d_embed, d_embed).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.norm1(x + self.dropout(self.attn(x)))\n",
        "    ffn = self.ffn(x)\n",
        "    x = self.norm2(x + self.dropout(ffn))\n",
        "\n",
        "    return self.ffout(x)\n"
      ],
      "metadata": {
        "id": "K03z-F4Ez-xo"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLayerTransformer(nn.Module):\n",
        "  def __init__(self, d_embed, vocab_size, d_ff, num_layers):\n",
        "    super(MultiLayerTransformer, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, d_embed).to(device)\n",
        "    self.layers = nn.ModuleList([TransformerLayer(d_embed, d_ff)] * num_layers)\n",
        "    self.fcout = nn.Linear(d_embed, vocab_size).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_embed = self.embed(x)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x_embed = layer(x_embed)\n",
        "\n",
        "    return self.fcout(x_embed)"
      ],
      "metadata": {
        "id": "OtGOdJ-s0GTd"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, d_embed, vocab_size, hidden_dim, num_layers):\n",
        "    super().__init__()\n",
        "    self.size_vocab = vocab_size\n",
        "    self.d_embed = d_embed\n",
        "\n",
        "    self.embed = nn.Embedding(vocab_size, d_embed).to(device)\n",
        "    self.rnn = nn.RNN(d_embed, hidden_dim, num_layers= num_layers, dropout=0.1, batch_first=True).to(device)\n",
        "    self.fc = nn.Linear(hidden_dim, vocab_size).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_embed = self.embed(x) # (n, N, embed_dim)\n",
        "    x_rnn, _ = self.rnn(x_embed) # (n, N, hidden_dim)\n",
        "    out = self.fc(x_rnn) # (n, N, size_vocab)\n",
        "    return out"
      ],
      "metadata": {
        "id": "11O0wUx2_pLs"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_evaluate(X_val, y_val, vocab_size, model, criterion):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      predictions = model(X_val).to(device)\n",
        "      loss = criterion(predictions.contiguous().view(-1, vocab_size), y_val.contiguous().view(-1))\n",
        "      return loss.item()"
      ],
      "metadata": {
        "id": "U_BIeonA5uFo"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(vocab_size, model, criterion, optimizer, epochs, X_train, y_train, X_val, y_val):\n",
        "  losses = []\n",
        "  val_losses = []\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(X_train)\n",
        "\n",
        "    loss = criterion(outputs.contiguous().view(-1, vocab_size), y_train.contiguous().view(-1))\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    val_loss = epoch_evaluate(X_val, y_val, vocab_size, model, criterion)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "      print(f'Epoch [{epoch+1}/{epochs}], Training loss: {loss:.4f}, Validation loss: {val_loss:.4f}')\n",
        "\n",
        "  return losses, val_losses\n"
      ],
      "metadata": {
        "id": "9Q2BKU0PE_Nv"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_ff = 32\n",
        "d_embed = 32\n",
        "num_layers = 3\n",
        "\n",
        "trans = MultiLayerTransformer(d_embed, vocab_size, d_ff, num_layers).to(device)\n",
        "optimizer_trans = torch.optim.Adam(trans.parameters())\n",
        "num_trainable_params = sum([p.numel() for p in trans.parameters()])\n",
        "print(\"# param Transformer:\", num_trainable_params)\n",
        "\n",
        "rnn = RNN(d_embed, vocab_size, d_ff, num_layers).to(device)\n",
        "optimizer_rnn = torch.optim.Adam(rnn.parameters())\n",
        "num_trainable_params = sum([p.numel() for p in rnn.parameters()])\n",
        "print(\"# param RNN:\", num_trainable_params)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "epochs = 5000\n"
      ],
      "metadata": {
        "id": "TIDpe3NCFgR-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa972e47-466f-4551-c3fc-a6ee2d0399cf"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# param Transformer: 7114\n",
            "# param RNN: 6986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trans_train_loss, trans_losses = train(vocab_size, trans, criterion, optimizer_trans, epochs, train_input_sequences, train_output_sequences, val_input_sequences, val_output_sequences)"
      ],
      "metadata": {
        "id": "BJ2n-G4RGlaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2b8a84-2c64-4ac3-ddb6-2123e31bb2c7"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/5000], Training loss: 2.2465, Validation loss: 2.2484\n",
            "Epoch [20/5000], Training loss: 2.1767, Validation loss: 2.1853\n",
            "Epoch [30/5000], Training loss: 2.1141, Validation loss: 2.1242\n",
            "Epoch [40/5000], Training loss: 2.0596, Validation loss: 2.0693\n",
            "Epoch [50/5000], Training loss: 2.0119, Validation loss: 2.0218\n",
            "Epoch [60/5000], Training loss: 1.9704, Validation loss: 1.9848\n",
            "Epoch [70/5000], Training loss: 1.9341, Validation loss: 1.9536\n",
            "Epoch [80/5000], Training loss: 1.9037, Validation loss: 1.9259\n",
            "Epoch [90/5000], Training loss: 1.8883, Validation loss: 1.9090\n",
            "Epoch [100/5000], Training loss: 1.8764, Validation loss: 1.9000\n",
            "Epoch [110/5000], Training loss: 1.8721, Validation loss: 1.8949\n",
            "Epoch [120/5000], Training loss: 1.8657, Validation loss: 1.8902\n",
            "Epoch [130/5000], Training loss: 1.8585, Validation loss: 1.8862\n",
            "Epoch [140/5000], Training loss: 1.8549, Validation loss: 1.8819\n",
            "Epoch [150/5000], Training loss: 1.8502, Validation loss: 1.8780\n",
            "Epoch [160/5000], Training loss: 1.8471, Validation loss: 1.8743\n",
            "Epoch [170/5000], Training loss: 1.8419, Validation loss: 1.8709\n",
            "Epoch [180/5000], Training loss: 1.8365, Validation loss: 1.8675\n",
            "Epoch [190/5000], Training loss: 1.8344, Validation loss: 1.8643\n",
            "Epoch [200/5000], Training loss: 1.8282, Validation loss: 1.8608\n",
            "Epoch [210/5000], Training loss: 1.8249, Validation loss: 1.8578\n",
            "Epoch [220/5000], Training loss: 1.8245, Validation loss: 1.8550\n",
            "Epoch [230/5000], Training loss: 1.8198, Validation loss: 1.8527\n",
            "Epoch [240/5000], Training loss: 1.8170, Validation loss: 1.8503\n",
            "Epoch [250/5000], Training loss: 1.8162, Validation loss: 1.8477\n",
            "Epoch [260/5000], Training loss: 1.8115, Validation loss: 1.8460\n",
            "Epoch [270/5000], Training loss: 1.8077, Validation loss: 1.8441\n",
            "Epoch [280/5000], Training loss: 1.8086, Validation loss: 1.8422\n",
            "Epoch [290/5000], Training loss: 1.8059, Validation loss: 1.8415\n",
            "Epoch [300/5000], Training loss: 1.8060, Validation loss: 1.8398\n",
            "Epoch [310/5000], Training loss: 1.8043, Validation loss: 1.8388\n",
            "Epoch [320/5000], Training loss: 1.7995, Validation loss: 1.8377\n",
            "Epoch [330/5000], Training loss: 1.8002, Validation loss: 1.8367\n",
            "Epoch [340/5000], Training loss: 1.7968, Validation loss: 1.8364\n",
            "Epoch [350/5000], Training loss: 1.7972, Validation loss: 1.8360\n",
            "Epoch [360/5000], Training loss: 1.7963, Validation loss: 1.8352\n",
            "Epoch [370/5000], Training loss: 1.7946, Validation loss: 1.8348\n",
            "Epoch [380/5000], Training loss: 1.7930, Validation loss: 1.8340\n",
            "Epoch [390/5000], Training loss: 1.7900, Validation loss: 1.8348\n",
            "Epoch [400/5000], Training loss: 1.7914, Validation loss: 1.8340\n",
            "Epoch [410/5000], Training loss: 1.7877, Validation loss: 1.8337\n",
            "Epoch [420/5000], Training loss: 1.7848, Validation loss: 1.8339\n",
            "Epoch [430/5000], Training loss: 1.7877, Validation loss: 1.8338\n",
            "Epoch [440/5000], Training loss: 1.7875, Validation loss: 1.8339\n",
            "Epoch [450/5000], Training loss: 1.7841, Validation loss: 1.8337\n",
            "Epoch [460/5000], Training loss: 1.7851, Validation loss: 1.8336\n",
            "Epoch [470/5000], Training loss: 1.7843, Validation loss: 1.8330\n",
            "Epoch [480/5000], Training loss: 1.7816, Validation loss: 1.8333\n",
            "Epoch [490/5000], Training loss: 1.7811, Validation loss: 1.8336\n",
            "Epoch [500/5000], Training loss: 1.7821, Validation loss: 1.8339\n",
            "Epoch [510/5000], Training loss: 1.7796, Validation loss: 1.8336\n",
            "Epoch [520/5000], Training loss: 1.7781, Validation loss: 1.8332\n",
            "Epoch [530/5000], Training loss: 1.7787, Validation loss: 1.8346\n",
            "Epoch [540/5000], Training loss: 1.7784, Validation loss: 1.8352\n",
            "Epoch [550/5000], Training loss: 1.7752, Validation loss: 1.8356\n",
            "Epoch [560/5000], Training loss: 1.7765, Validation loss: 1.8357\n",
            "Epoch [570/5000], Training loss: 1.7759, Validation loss: 1.8357\n",
            "Epoch [580/5000], Training loss: 1.7745, Validation loss: 1.8355\n",
            "Epoch [590/5000], Training loss: 1.7744, Validation loss: 1.8358\n",
            "Epoch [600/5000], Training loss: 1.7727, Validation loss: 1.8368\n",
            "Epoch [610/5000], Training loss: 1.7729, Validation loss: 1.8367\n",
            "Epoch [620/5000], Training loss: 1.7726, Validation loss: 1.8375\n",
            "Epoch [630/5000], Training loss: 1.7714, Validation loss: 1.8376\n",
            "Epoch [640/5000], Training loss: 1.7718, Validation loss: 1.8381\n",
            "Epoch [650/5000], Training loss: 1.7713, Validation loss: 1.8382\n",
            "Epoch [660/5000], Training loss: 1.7671, Validation loss: 1.8383\n",
            "Epoch [670/5000], Training loss: 1.7697, Validation loss: 1.8385\n",
            "Epoch [680/5000], Training loss: 1.7685, Validation loss: 1.8387\n",
            "Epoch [690/5000], Training loss: 1.7713, Validation loss: 1.8392\n",
            "Epoch [700/5000], Training loss: 1.7721, Validation loss: 1.8399\n",
            "Epoch [710/5000], Training loss: 1.7692, Validation loss: 1.8395\n",
            "Epoch [720/5000], Training loss: 1.7673, Validation loss: 1.8399\n",
            "Epoch [730/5000], Training loss: 1.7670, Validation loss: 1.8406\n",
            "Epoch [740/5000], Training loss: 1.7681, Validation loss: 1.8406\n",
            "Epoch [750/5000], Training loss: 1.7693, Validation loss: 1.8409\n",
            "Epoch [760/5000], Training loss: 1.7661, Validation loss: 1.8412\n",
            "Epoch [770/5000], Training loss: 1.7671, Validation loss: 1.8428\n",
            "Epoch [780/5000], Training loss: 1.7677, Validation loss: 1.8425\n",
            "Epoch [790/5000], Training loss: 1.7624, Validation loss: 1.8423\n",
            "Epoch [800/5000], Training loss: 1.7665, Validation loss: 1.8437\n",
            "Epoch [810/5000], Training loss: 1.7639, Validation loss: 1.8437\n",
            "Epoch [820/5000], Training loss: 1.7631, Validation loss: 1.8440\n",
            "Epoch [830/5000], Training loss: 1.7630, Validation loss: 1.8442\n",
            "Epoch [840/5000], Training loss: 1.7638, Validation loss: 1.8449\n",
            "Epoch [850/5000], Training loss: 1.7663, Validation loss: 1.8453\n",
            "Epoch [860/5000], Training loss: 1.7623, Validation loss: 1.8456\n",
            "Epoch [870/5000], Training loss: 1.7607, Validation loss: 1.8452\n",
            "Epoch [880/5000], Training loss: 1.7610, Validation loss: 1.8453\n",
            "Epoch [890/5000], Training loss: 1.7609, Validation loss: 1.8449\n",
            "Epoch [900/5000], Training loss: 1.7612, Validation loss: 1.8474\n",
            "Epoch [910/5000], Training loss: 1.7587, Validation loss: 1.8475\n",
            "Epoch [920/5000], Training loss: 1.7592, Validation loss: 1.8483\n",
            "Epoch [930/5000], Training loss: 1.7594, Validation loss: 1.8485\n",
            "Epoch [940/5000], Training loss: 1.7587, Validation loss: 1.8484\n",
            "Epoch [950/5000], Training loss: 1.7579, Validation loss: 1.8483\n",
            "Epoch [960/5000], Training loss: 1.7589, Validation loss: 1.8491\n",
            "Epoch [970/5000], Training loss: 1.7588, Validation loss: 1.8491\n",
            "Epoch [980/5000], Training loss: 1.7590, Validation loss: 1.8484\n",
            "Epoch [990/5000], Training loss: 1.7562, Validation loss: 1.8489\n",
            "Epoch [1000/5000], Training loss: 1.7566, Validation loss: 1.8501\n",
            "Epoch [1010/5000], Training loss: 1.7545, Validation loss: 1.8499\n",
            "Epoch [1020/5000], Training loss: 1.7583, Validation loss: 1.8502\n",
            "Epoch [1030/5000], Training loss: 1.7574, Validation loss: 1.8496\n",
            "Epoch [1040/5000], Training loss: 1.7570, Validation loss: 1.8507\n",
            "Epoch [1050/5000], Training loss: 1.7570, Validation loss: 1.8509\n",
            "Epoch [1060/5000], Training loss: 1.7542, Validation loss: 1.8508\n",
            "Epoch [1070/5000], Training loss: 1.7531, Validation loss: 1.8517\n",
            "Epoch [1080/5000], Training loss: 1.7545, Validation loss: 1.8508\n",
            "Epoch [1090/5000], Training loss: 1.7552, Validation loss: 1.8516\n",
            "Epoch [1100/5000], Training loss: 1.7557, Validation loss: 1.8527\n",
            "Epoch [1110/5000], Training loss: 1.7535, Validation loss: 1.8518\n",
            "Epoch [1120/5000], Training loss: 1.7556, Validation loss: 1.8525\n",
            "Epoch [1130/5000], Training loss: 1.7545, Validation loss: 1.8532\n",
            "Epoch [1140/5000], Training loss: 1.7523, Validation loss: 1.8531\n",
            "Epoch [1150/5000], Training loss: 1.7535, Validation loss: 1.8531\n",
            "Epoch [1160/5000], Training loss: 1.7546, Validation loss: 1.8534\n",
            "Epoch [1170/5000], Training loss: 1.7494, Validation loss: 1.8538\n",
            "Epoch [1180/5000], Training loss: 1.7550, Validation loss: 1.8538\n",
            "Epoch [1190/5000], Training loss: 1.7506, Validation loss: 1.8538\n",
            "Epoch [1200/5000], Training loss: 1.7524, Validation loss: 1.8547\n",
            "Epoch [1210/5000], Training loss: 1.7534, Validation loss: 1.8537\n",
            "Epoch [1220/5000], Training loss: 1.7521, Validation loss: 1.8539\n",
            "Epoch [1230/5000], Training loss: 1.7519, Validation loss: 1.8550\n",
            "Epoch [1240/5000], Training loss: 1.7521, Validation loss: 1.8547\n",
            "Epoch [1250/5000], Training loss: 1.7501, Validation loss: 1.8552\n",
            "Epoch [1260/5000], Training loss: 1.7508, Validation loss: 1.8546\n",
            "Epoch [1270/5000], Training loss: 1.7503, Validation loss: 1.8551\n",
            "Epoch [1280/5000], Training loss: 1.7497, Validation loss: 1.8568\n",
            "Epoch [1290/5000], Training loss: 1.7472, Validation loss: 1.8556\n",
            "Epoch [1300/5000], Training loss: 1.7487, Validation loss: 1.8557\n",
            "Epoch [1310/5000], Training loss: 1.7500, Validation loss: 1.8572\n",
            "Epoch [1320/5000], Training loss: 1.7511, Validation loss: 1.8569\n",
            "Epoch [1330/5000], Training loss: 1.7471, Validation loss: 1.8570\n",
            "Epoch [1340/5000], Training loss: 1.7486, Validation loss: 1.8575\n",
            "Epoch [1350/5000], Training loss: 1.7477, Validation loss: 1.8571\n",
            "Epoch [1360/5000], Training loss: 1.7501, Validation loss: 1.8579\n",
            "Epoch [1370/5000], Training loss: 1.7446, Validation loss: 1.8572\n",
            "Epoch [1380/5000], Training loss: 1.7497, Validation loss: 1.8584\n",
            "Epoch [1390/5000], Training loss: 1.7462, Validation loss: 1.8585\n",
            "Epoch [1400/5000], Training loss: 1.7435, Validation loss: 1.8586\n",
            "Epoch [1410/5000], Training loss: 1.7477, Validation loss: 1.8590\n",
            "Epoch [1420/5000], Training loss: 1.7461, Validation loss: 1.8597\n",
            "Epoch [1430/5000], Training loss: 1.7480, Validation loss: 1.8605\n",
            "Epoch [1440/5000], Training loss: 1.7473, Validation loss: 1.8597\n",
            "Epoch [1450/5000], Training loss: 1.7470, Validation loss: 1.8614\n",
            "Epoch [1460/5000], Training loss: 1.7446, Validation loss: 1.8614\n",
            "Epoch [1470/5000], Training loss: 1.7444, Validation loss: 1.8605\n",
            "Epoch [1480/5000], Training loss: 1.7457, Validation loss: 1.8594\n",
            "Epoch [1490/5000], Training loss: 1.7451, Validation loss: 1.8618\n",
            "Epoch [1500/5000], Training loss: 1.7427, Validation loss: 1.8617\n",
            "Epoch [1510/5000], Training loss: 1.7460, Validation loss: 1.8632\n",
            "Epoch [1520/5000], Training loss: 1.7412, Validation loss: 1.8619\n",
            "Epoch [1530/5000], Training loss: 1.7415, Validation loss: 1.8627\n",
            "Epoch [1540/5000], Training loss: 1.7425, Validation loss: 1.8632\n",
            "Epoch [1550/5000], Training loss: 1.7429, Validation loss: 1.8625\n",
            "Epoch [1560/5000], Training loss: 1.7452, Validation loss: 1.8634\n",
            "Epoch [1570/5000], Training loss: 1.7428, Validation loss: 1.8628\n",
            "Epoch [1580/5000], Training loss: 1.7420, Validation loss: 1.8650\n",
            "Epoch [1590/5000], Training loss: 1.7434, Validation loss: 1.8654\n",
            "Epoch [1600/5000], Training loss: 1.7408, Validation loss: 1.8638\n",
            "Epoch [1610/5000], Training loss: 1.7414, Validation loss: 1.8637\n",
            "Epoch [1620/5000], Training loss: 1.7416, Validation loss: 1.8634\n",
            "Epoch [1630/5000], Training loss: 1.7422, Validation loss: 1.8645\n",
            "Epoch [1640/5000], Training loss: 1.7432, Validation loss: 1.8646\n",
            "Epoch [1650/5000], Training loss: 1.7419, Validation loss: 1.8651\n",
            "Epoch [1660/5000], Training loss: 1.7399, Validation loss: 1.8654\n",
            "Epoch [1670/5000], Training loss: 1.7388, Validation loss: 1.8667\n",
            "Epoch [1680/5000], Training loss: 1.7400, Validation loss: 1.8660\n",
            "Epoch [1690/5000], Training loss: 1.7404, Validation loss: 1.8680\n",
            "Epoch [1700/5000], Training loss: 1.7417, Validation loss: 1.8671\n",
            "Epoch [1710/5000], Training loss: 1.7409, Validation loss: 1.8668\n",
            "Epoch [1720/5000], Training loss: 1.7382, Validation loss: 1.8665\n",
            "Epoch [1730/5000], Training loss: 1.7429, Validation loss: 1.8693\n",
            "Epoch [1740/5000], Training loss: 1.7376, Validation loss: 1.8682\n",
            "Epoch [1750/5000], Training loss: 1.7385, Validation loss: 1.8694\n",
            "Epoch [1760/5000], Training loss: 1.7416, Validation loss: 1.8683\n",
            "Epoch [1770/5000], Training loss: 1.7382, Validation loss: 1.8685\n",
            "Epoch [1780/5000], Training loss: 1.7404, Validation loss: 1.8686\n",
            "Epoch [1790/5000], Training loss: 1.7402, Validation loss: 1.8690\n",
            "Epoch [1800/5000], Training loss: 1.7412, Validation loss: 1.8693\n",
            "Epoch [1810/5000], Training loss: 1.7374, Validation loss: 1.8701\n",
            "Epoch [1820/5000], Training loss: 1.7404, Validation loss: 1.8692\n",
            "Epoch [1830/5000], Training loss: 1.7371, Validation loss: 1.8685\n",
            "Epoch [1840/5000], Training loss: 1.7387, Validation loss: 1.8688\n",
            "Epoch [1850/5000], Training loss: 1.7391, Validation loss: 1.8702\n",
            "Epoch [1860/5000], Training loss: 1.7390, Validation loss: 1.8700\n",
            "Epoch [1870/5000], Training loss: 1.7364, Validation loss: 1.8695\n",
            "Epoch [1880/5000], Training loss: 1.7403, Validation loss: 1.8723\n",
            "Epoch [1890/5000], Training loss: 1.7372, Validation loss: 1.8705\n",
            "Epoch [1900/5000], Training loss: 1.7353, Validation loss: 1.8708\n",
            "Epoch [1910/5000], Training loss: 1.7364, Validation loss: 1.8701\n",
            "Epoch [1920/5000], Training loss: 1.7399, Validation loss: 1.8700\n",
            "Epoch [1930/5000], Training loss: 1.7396, Validation loss: 1.8712\n",
            "Epoch [1940/5000], Training loss: 1.7384, Validation loss: 1.8702\n",
            "Epoch [1950/5000], Training loss: 1.7374, Validation loss: 1.8726\n",
            "Epoch [1960/5000], Training loss: 1.7360, Validation loss: 1.8712\n",
            "Epoch [1970/5000], Training loss: 1.7386, Validation loss: 1.8724\n",
            "Epoch [1980/5000], Training loss: 1.7372, Validation loss: 1.8745\n",
            "Epoch [1990/5000], Training loss: 1.7380, Validation loss: 1.8730\n",
            "Epoch [2000/5000], Training loss: 1.7368, Validation loss: 1.8735\n",
            "Epoch [2010/5000], Training loss: 1.7357, Validation loss: 1.8722\n",
            "Epoch [2020/5000], Training loss: 1.7369, Validation loss: 1.8726\n",
            "Epoch [2030/5000], Training loss: 1.7366, Validation loss: 1.8730\n",
            "Epoch [2040/5000], Training loss: 1.7339, Validation loss: 1.8717\n",
            "Epoch [2050/5000], Training loss: 1.7373, Validation loss: 1.8724\n",
            "Epoch [2060/5000], Training loss: 1.7330, Validation loss: 1.8727\n",
            "Epoch [2070/5000], Training loss: 1.7337, Validation loss: 1.8732\n",
            "Epoch [2080/5000], Training loss: 1.7363, Validation loss: 1.8745\n",
            "Epoch [2090/5000], Training loss: 1.7349, Validation loss: 1.8735\n",
            "Epoch [2100/5000], Training loss: 1.7346, Validation loss: 1.8734\n",
            "Epoch [2110/5000], Training loss: 1.7358, Validation loss: 1.8741\n",
            "Epoch [2120/5000], Training loss: 1.7320, Validation loss: 1.8744\n",
            "Epoch [2130/5000], Training loss: 1.7328, Validation loss: 1.8744\n",
            "Epoch [2140/5000], Training loss: 1.7383, Validation loss: 1.8758\n",
            "Epoch [2150/5000], Training loss: 1.7346, Validation loss: 1.8731\n",
            "Epoch [2160/5000], Training loss: 1.7339, Validation loss: 1.8747\n",
            "Epoch [2170/5000], Training loss: 1.7359, Validation loss: 1.8748\n",
            "Epoch [2180/5000], Training loss: 1.7371, Validation loss: 1.8754\n",
            "Epoch [2190/5000], Training loss: 1.7343, Validation loss: 1.8756\n",
            "Epoch [2200/5000], Training loss: 1.7318, Validation loss: 1.8756\n",
            "Epoch [2210/5000], Training loss: 1.7324, Validation loss: 1.8755\n",
            "Epoch [2220/5000], Training loss: 1.7359, Validation loss: 1.8766\n",
            "Epoch [2230/5000], Training loss: 1.7355, Validation loss: 1.8770\n",
            "Epoch [2240/5000], Training loss: 1.7331, Validation loss: 1.8769\n",
            "Epoch [2250/5000], Training loss: 1.7378, Validation loss: 1.8765\n",
            "Epoch [2260/5000], Training loss: 1.7327, Validation loss: 1.8781\n",
            "Epoch [2270/5000], Training loss: 1.7328, Validation loss: 1.8776\n",
            "Epoch [2280/5000], Training loss: 1.7348, Validation loss: 1.8779\n",
            "Epoch [2290/5000], Training loss: 1.7325, Validation loss: 1.8787\n",
            "Epoch [2300/5000], Training loss: 1.7349, Validation loss: 1.8783\n",
            "Epoch [2310/5000], Training loss: 1.7329, Validation loss: 1.8780\n",
            "Epoch [2320/5000], Training loss: 1.7295, Validation loss: 1.8770\n",
            "Epoch [2330/5000], Training loss: 1.7342, Validation loss: 1.8792\n",
            "Epoch [2340/5000], Training loss: 1.7342, Validation loss: 1.8796\n",
            "Epoch [2350/5000], Training loss: 1.7331, Validation loss: 1.8780\n",
            "Epoch [2360/5000], Training loss: 1.7335, Validation loss: 1.8778\n",
            "Epoch [2370/5000], Training loss: 1.7343, Validation loss: 1.8795\n",
            "Epoch [2380/5000], Training loss: 1.7337, Validation loss: 1.8802\n",
            "Epoch [2390/5000], Training loss: 1.7304, Validation loss: 1.8801\n",
            "Epoch [2400/5000], Training loss: 1.7313, Validation loss: 1.8791\n",
            "Epoch [2410/5000], Training loss: 1.7321, Validation loss: 1.8801\n",
            "Epoch [2420/5000], Training loss: 1.7308, Validation loss: 1.8784\n",
            "Epoch [2430/5000], Training loss: 1.7314, Validation loss: 1.8808\n",
            "Epoch [2440/5000], Training loss: 1.7339, Validation loss: 1.8802\n",
            "Epoch [2450/5000], Training loss: 1.7332, Validation loss: 1.8783\n",
            "Epoch [2460/5000], Training loss: 1.7335, Validation loss: 1.8793\n",
            "Epoch [2470/5000], Training loss: 1.7300, Validation loss: 1.8789\n",
            "Epoch [2480/5000], Training loss: 1.7324, Validation loss: 1.8793\n",
            "Epoch [2490/5000], Training loss: 1.7320, Validation loss: 1.8806\n",
            "Epoch [2500/5000], Training loss: 1.7322, Validation loss: 1.8805\n",
            "Epoch [2510/5000], Training loss: 1.7339, Validation loss: 1.8815\n",
            "Epoch [2520/5000], Training loss: 1.7296, Validation loss: 1.8803\n",
            "Epoch [2530/5000], Training loss: 1.7333, Validation loss: 1.8810\n",
            "Epoch [2540/5000], Training loss: 1.7300, Validation loss: 1.8815\n",
            "Epoch [2550/5000], Training loss: 1.7318, Validation loss: 1.8806\n",
            "Epoch [2560/5000], Training loss: 1.7280, Validation loss: 1.8787\n",
            "Epoch [2570/5000], Training loss: 1.7304, Validation loss: 1.8822\n",
            "Epoch [2580/5000], Training loss: 1.7288, Validation loss: 1.8810\n",
            "Epoch [2590/5000], Training loss: 1.7314, Validation loss: 1.8804\n",
            "Epoch [2600/5000], Training loss: 1.7308, Validation loss: 1.8796\n",
            "Epoch [2610/5000], Training loss: 1.7307, Validation loss: 1.8820\n",
            "Epoch [2620/5000], Training loss: 1.7291, Validation loss: 1.8818\n",
            "Epoch [2630/5000], Training loss: 1.7277, Validation loss: 1.8834\n",
            "Epoch [2640/5000], Training loss: 1.7315, Validation loss: 1.8825\n",
            "Epoch [2650/5000], Training loss: 1.7282, Validation loss: 1.8823\n",
            "Epoch [2660/5000], Training loss: 1.7297, Validation loss: 1.8808\n",
            "Epoch [2670/5000], Training loss: 1.7301, Validation loss: 1.8829\n",
            "Epoch [2680/5000], Training loss: 1.7341, Validation loss: 1.8814\n",
            "Epoch [2690/5000], Training loss: 1.7281, Validation loss: 1.8815\n",
            "Epoch [2700/5000], Training loss: 1.7306, Validation loss: 1.8810\n",
            "Epoch [2710/5000], Training loss: 1.7280, Validation loss: 1.8832\n",
            "Epoch [2720/5000], Training loss: 1.7296, Validation loss: 1.8837\n",
            "Epoch [2730/5000], Training loss: 1.7302, Validation loss: 1.8838\n",
            "Epoch [2740/5000], Training loss: 1.7291, Validation loss: 1.8828\n",
            "Epoch [2750/5000], Training loss: 1.7289, Validation loss: 1.8834\n",
            "Epoch [2760/5000], Training loss: 1.7276, Validation loss: 1.8835\n",
            "Epoch [2770/5000], Training loss: 1.7323, Validation loss: 1.8830\n",
            "Epoch [2780/5000], Training loss: 1.7310, Validation loss: 1.8849\n",
            "Epoch [2790/5000], Training loss: 1.7277, Validation loss: 1.8831\n",
            "Epoch [2800/5000], Training loss: 1.7306, Validation loss: 1.8823\n",
            "Epoch [2810/5000], Training loss: 1.7300, Validation loss: 1.8835\n",
            "Epoch [2820/5000], Training loss: 1.7304, Validation loss: 1.8846\n",
            "Epoch [2830/5000], Training loss: 1.7280, Validation loss: 1.8834\n",
            "Epoch [2840/5000], Training loss: 1.7279, Validation loss: 1.8857\n",
            "Epoch [2850/5000], Training loss: 1.7271, Validation loss: 1.8840\n",
            "Epoch [2860/5000], Training loss: 1.7282, Validation loss: 1.8841\n",
            "Epoch [2870/5000], Training loss: 1.7292, Validation loss: 1.8848\n",
            "Epoch [2880/5000], Training loss: 1.7298, Validation loss: 1.8847\n",
            "Epoch [2890/5000], Training loss: 1.7276, Validation loss: 1.8846\n",
            "Epoch [2900/5000], Training loss: 1.7291, Validation loss: 1.8836\n",
            "Epoch [2910/5000], Training loss: 1.7288, Validation loss: 1.8855\n",
            "Epoch [2920/5000], Training loss: 1.7280, Validation loss: 1.8847\n",
            "Epoch [2930/5000], Training loss: 1.7268, Validation loss: 1.8871\n",
            "Epoch [2940/5000], Training loss: 1.7270, Validation loss: 1.8863\n",
            "Epoch [2950/5000], Training loss: 1.7267, Validation loss: 1.8862\n",
            "Epoch [2960/5000], Training loss: 1.7277, Validation loss: 1.8855\n",
            "Epoch [2970/5000], Training loss: 1.7307, Validation loss: 1.8861\n",
            "Epoch [2980/5000], Training loss: 1.7270, Validation loss: 1.8853\n",
            "Epoch [2990/5000], Training loss: 1.7270, Validation loss: 1.8852\n",
            "Epoch [3000/5000], Training loss: 1.7294, Validation loss: 1.8870\n",
            "Epoch [3010/5000], Training loss: 1.7270, Validation loss: 1.8869\n",
            "Epoch [3020/5000], Training loss: 1.7280, Validation loss: 1.8883\n",
            "Epoch [3030/5000], Training loss: 1.7288, Validation loss: 1.8882\n",
            "Epoch [3040/5000], Training loss: 1.7315, Validation loss: 1.8873\n",
            "Epoch [3050/5000], Training loss: 1.7251, Validation loss: 1.8867\n",
            "Epoch [3060/5000], Training loss: 1.7288, Validation loss: 1.8864\n",
            "Epoch [3070/5000], Training loss: 1.7288, Validation loss: 1.8864\n",
            "Epoch [3080/5000], Training loss: 1.7262, Validation loss: 1.8858\n",
            "Epoch [3090/5000], Training loss: 1.7271, Validation loss: 1.8880\n",
            "Epoch [3100/5000], Training loss: 1.7274, Validation loss: 1.8869\n",
            "Epoch [3110/5000], Training loss: 1.7282, Validation loss: 1.8865\n",
            "Epoch [3120/5000], Training loss: 1.7276, Validation loss: 1.8875\n",
            "Epoch [3130/5000], Training loss: 1.7284, Validation loss: 1.8867\n",
            "Epoch [3140/5000], Training loss: 1.7294, Validation loss: 1.8852\n",
            "Epoch [3150/5000], Training loss: 1.7279, Validation loss: 1.8876\n",
            "Epoch [3160/5000], Training loss: 1.7260, Validation loss: 1.8878\n",
            "Epoch [3170/5000], Training loss: 1.7248, Validation loss: 1.8871\n",
            "Epoch [3180/5000], Training loss: 1.7243, Validation loss: 1.8879\n",
            "Epoch [3190/5000], Training loss: 1.7252, Validation loss: 1.8885\n",
            "Epoch [3200/5000], Training loss: 1.7276, Validation loss: 1.8863\n",
            "Epoch [3210/5000], Training loss: 1.7256, Validation loss: 1.8882\n",
            "Epoch [3220/5000], Training loss: 1.7279, Validation loss: 1.8887\n",
            "Epoch [3230/5000], Training loss: 1.7253, Validation loss: 1.8887\n",
            "Epoch [3240/5000], Training loss: 1.7286, Validation loss: 1.8898\n",
            "Epoch [3250/5000], Training loss: 1.7253, Validation loss: 1.8883\n",
            "Epoch [3260/5000], Training loss: 1.7286, Validation loss: 1.8888\n",
            "Epoch [3270/5000], Training loss: 1.7281, Validation loss: 1.8888\n",
            "Epoch [3280/5000], Training loss: 1.7226, Validation loss: 1.8903\n",
            "Epoch [3290/5000], Training loss: 1.7243, Validation loss: 1.8901\n",
            "Epoch [3300/5000], Training loss: 1.7260, Validation loss: 1.8887\n",
            "Epoch [3310/5000], Training loss: 1.7258, Validation loss: 1.8891\n",
            "Epoch [3320/5000], Training loss: 1.7251, Validation loss: 1.8897\n",
            "Epoch [3330/5000], Training loss: 1.7268, Validation loss: 1.8904\n",
            "Epoch [3340/5000], Training loss: 1.7234, Validation loss: 1.8911\n",
            "Epoch [3350/5000], Training loss: 1.7254, Validation loss: 1.8885\n",
            "Epoch [3360/5000], Training loss: 1.7236, Validation loss: 1.8912\n",
            "Epoch [3370/5000], Training loss: 1.7257, Validation loss: 1.8893\n",
            "Epoch [3380/5000], Training loss: 1.7255, Validation loss: 1.8883\n",
            "Epoch [3390/5000], Training loss: 1.7249, Validation loss: 1.8890\n",
            "Epoch [3400/5000], Training loss: 1.7258, Validation loss: 1.8891\n",
            "Epoch [3410/5000], Training loss: 1.7252, Validation loss: 1.8898\n",
            "Epoch [3420/5000], Training loss: 1.7217, Validation loss: 1.8910\n",
            "Epoch [3430/5000], Training loss: 1.7261, Validation loss: 1.8907\n",
            "Epoch [3440/5000], Training loss: 1.7226, Validation loss: 1.8920\n",
            "Epoch [3450/5000], Training loss: 1.7270, Validation loss: 1.8920\n",
            "Epoch [3460/5000], Training loss: 1.7245, Validation loss: 1.8903\n",
            "Epoch [3470/5000], Training loss: 1.7235, Validation loss: 1.8920\n",
            "Epoch [3480/5000], Training loss: 1.7263, Validation loss: 1.8910\n",
            "Epoch [3490/5000], Training loss: 1.7275, Validation loss: 1.8894\n",
            "Epoch [3500/5000], Training loss: 1.7223, Validation loss: 1.8896\n",
            "Epoch [3510/5000], Training loss: 1.7236, Validation loss: 1.8898\n",
            "Epoch [3520/5000], Training loss: 1.7230, Validation loss: 1.8903\n",
            "Epoch [3530/5000], Training loss: 1.7218, Validation loss: 1.8902\n",
            "Epoch [3540/5000], Training loss: 1.7263, Validation loss: 1.8919\n",
            "Epoch [3550/5000], Training loss: 1.7244, Validation loss: 1.8903\n",
            "Epoch [3560/5000], Training loss: 1.7245, Validation loss: 1.8908\n",
            "Epoch [3570/5000], Training loss: 1.7250, Validation loss: 1.8888\n",
            "Epoch [3580/5000], Training loss: 1.7270, Validation loss: 1.8906\n",
            "Epoch [3590/5000], Training loss: 1.7260, Validation loss: 1.8888\n",
            "Epoch [3600/5000], Training loss: 1.7241, Validation loss: 1.8915\n",
            "Epoch [3610/5000], Training loss: 1.7261, Validation loss: 1.8902\n",
            "Epoch [3620/5000], Training loss: 1.7223, Validation loss: 1.8908\n",
            "Epoch [3630/5000], Training loss: 1.7238, Validation loss: 1.8907\n",
            "Epoch [3640/5000], Training loss: 1.7272, Validation loss: 1.8898\n",
            "Epoch [3650/5000], Training loss: 1.7218, Validation loss: 1.8901\n",
            "Epoch [3660/5000], Training loss: 1.7258, Validation loss: 1.8922\n",
            "Epoch [3670/5000], Training loss: 1.7247, Validation loss: 1.8916\n",
            "Epoch [3680/5000], Training loss: 1.7243, Validation loss: 1.8926\n",
            "Epoch [3690/5000], Training loss: 1.7245, Validation loss: 1.8894\n",
            "Epoch [3700/5000], Training loss: 1.7201, Validation loss: 1.8909\n",
            "Epoch [3710/5000], Training loss: 1.7276, Validation loss: 1.8922\n",
            "Epoch [3720/5000], Training loss: 1.7273, Validation loss: 1.8938\n",
            "Epoch [3730/5000], Training loss: 1.7274, Validation loss: 1.8925\n",
            "Epoch [3740/5000], Training loss: 1.7226, Validation loss: 1.8931\n",
            "Epoch [3750/5000], Training loss: 1.7250, Validation loss: 1.8920\n",
            "Epoch [3760/5000], Training loss: 1.7239, Validation loss: 1.8900\n",
            "Epoch [3770/5000], Training loss: 1.7240, Validation loss: 1.8935\n",
            "Epoch [3780/5000], Training loss: 1.7235, Validation loss: 1.8933\n",
            "Epoch [3790/5000], Training loss: 1.7220, Validation loss: 1.8945\n",
            "Epoch [3800/5000], Training loss: 1.7227, Validation loss: 1.8932\n",
            "Epoch [3810/5000], Training loss: 1.7209, Validation loss: 1.8915\n",
            "Epoch [3820/5000], Training loss: 1.7249, Validation loss: 1.8920\n",
            "Epoch [3830/5000], Training loss: 1.7250, Validation loss: 1.8939\n",
            "Epoch [3840/5000], Training loss: 1.7233, Validation loss: 1.8928\n",
            "Epoch [3850/5000], Training loss: 1.7233, Validation loss: 1.8934\n",
            "Epoch [3860/5000], Training loss: 1.7236, Validation loss: 1.8933\n",
            "Epoch [3870/5000], Training loss: 1.7248, Validation loss: 1.8933\n",
            "Epoch [3880/5000], Training loss: 1.7207, Validation loss: 1.8930\n",
            "Epoch [3890/5000], Training loss: 1.7215, Validation loss: 1.8946\n",
            "Epoch [3900/5000], Training loss: 1.7228, Validation loss: 1.8956\n",
            "Epoch [3910/5000], Training loss: 1.7244, Validation loss: 1.8927\n",
            "Epoch [3920/5000], Training loss: 1.7231, Validation loss: 1.8920\n",
            "Epoch [3930/5000], Training loss: 1.7221, Validation loss: 1.8937\n",
            "Epoch [3940/5000], Training loss: 1.7253, Validation loss: 1.8928\n",
            "Epoch [3950/5000], Training loss: 1.7224, Validation loss: 1.8944\n",
            "Epoch [3960/5000], Training loss: 1.7210, Validation loss: 1.8942\n",
            "Epoch [3970/5000], Training loss: 1.7217, Validation loss: 1.8936\n",
            "Epoch [3980/5000], Training loss: 1.7201, Validation loss: 1.8946\n",
            "Epoch [3990/5000], Training loss: 1.7254, Validation loss: 1.8927\n",
            "Epoch [4000/5000], Training loss: 1.7212, Validation loss: 1.8935\n",
            "Epoch [4010/5000], Training loss: 1.7232, Validation loss: 1.8935\n",
            "Epoch [4020/5000], Training loss: 1.7203, Validation loss: 1.8956\n",
            "Epoch [4030/5000], Training loss: 1.7195, Validation loss: 1.8929\n",
            "Epoch [4040/5000], Training loss: 1.7199, Validation loss: 1.8950\n",
            "Epoch [4050/5000], Training loss: 1.7225, Validation loss: 1.8935\n",
            "Epoch [4060/5000], Training loss: 1.7189, Validation loss: 1.8940\n",
            "Epoch [4070/5000], Training loss: 1.7245, Validation loss: 1.8945\n",
            "Epoch [4080/5000], Training loss: 1.7237, Validation loss: 1.8939\n",
            "Epoch [4090/5000], Training loss: 1.7215, Validation loss: 1.8940\n",
            "Epoch [4100/5000], Training loss: 1.7222, Validation loss: 1.8935\n",
            "Epoch [4110/5000], Training loss: 1.7191, Validation loss: 1.8946\n",
            "Epoch [4120/5000], Training loss: 1.7212, Validation loss: 1.8945\n",
            "Epoch [4130/5000], Training loss: 1.7214, Validation loss: 1.8959\n",
            "Epoch [4140/5000], Training loss: 1.7243, Validation loss: 1.8959\n",
            "Epoch [4150/5000], Training loss: 1.7217, Validation loss: 1.8953\n",
            "Epoch [4160/5000], Training loss: 1.7212, Validation loss: 1.8921\n",
            "Epoch [4170/5000], Training loss: 1.7215, Validation loss: 1.8974\n",
            "Epoch [4180/5000], Training loss: 1.7240, Validation loss: 1.8956\n",
            "Epoch [4190/5000], Training loss: 1.7230, Validation loss: 1.8954\n",
            "Epoch [4200/5000], Training loss: 1.7211, Validation loss: 1.8946\n",
            "Epoch [4210/5000], Training loss: 1.7234, Validation loss: 1.8951\n",
            "Epoch [4220/5000], Training loss: 1.7208, Validation loss: 1.8958\n",
            "Epoch [4230/5000], Training loss: 1.7220, Validation loss: 1.8969\n",
            "Epoch [4240/5000], Training loss: 1.7189, Validation loss: 1.8975\n",
            "Epoch [4250/5000], Training loss: 1.7232, Validation loss: 1.8956\n",
            "Epoch [4260/5000], Training loss: 1.7204, Validation loss: 1.8979\n",
            "Epoch [4270/5000], Training loss: 1.7210, Validation loss: 1.8965\n",
            "Epoch [4280/5000], Training loss: 1.7233, Validation loss: 1.8947\n",
            "Epoch [4290/5000], Training loss: 1.7167, Validation loss: 1.8963\n",
            "Epoch [4300/5000], Training loss: 1.7219, Validation loss: 1.8964\n",
            "Epoch [4310/5000], Training loss: 1.7214, Validation loss: 1.8949\n",
            "Epoch [4320/5000], Training loss: 1.7245, Validation loss: 1.8984\n",
            "Epoch [4330/5000], Training loss: 1.7225, Validation loss: 1.8966\n",
            "Epoch [4340/5000], Training loss: 1.7223, Validation loss: 1.8961\n",
            "Epoch [4350/5000], Training loss: 1.7194, Validation loss: 1.8954\n",
            "Epoch [4360/5000], Training loss: 1.7224, Validation loss: 1.8955\n",
            "Epoch [4370/5000], Training loss: 1.7220, Validation loss: 1.8948\n",
            "Epoch [4380/5000], Training loss: 1.7190, Validation loss: 1.8955\n",
            "Epoch [4390/5000], Training loss: 1.7217, Validation loss: 1.8980\n",
            "Epoch [4400/5000], Training loss: 1.7228, Validation loss: 1.8958\n",
            "Epoch [4410/5000], Training loss: 1.7205, Validation loss: 1.8976\n",
            "Epoch [4420/5000], Training loss: 1.7222, Validation loss: 1.8961\n",
            "Epoch [4430/5000], Training loss: 1.7197, Validation loss: 1.8976\n",
            "Epoch [4440/5000], Training loss: 1.7220, Validation loss: 1.8967\n",
            "Epoch [4450/5000], Training loss: 1.7225, Validation loss: 1.8970\n",
            "Epoch [4460/5000], Training loss: 1.7197, Validation loss: 1.8960\n",
            "Epoch [4470/5000], Training loss: 1.7202, Validation loss: 1.8986\n",
            "Epoch [4480/5000], Training loss: 1.7210, Validation loss: 1.8973\n",
            "Epoch [4490/5000], Training loss: 1.7220, Validation loss: 1.8994\n",
            "Epoch [4500/5000], Training loss: 1.7192, Validation loss: 1.8981\n",
            "Epoch [4510/5000], Training loss: 1.7208, Validation loss: 1.8977\n",
            "Epoch [4520/5000], Training loss: 1.7166, Validation loss: 1.8975\n",
            "Epoch [4530/5000], Training loss: 1.7182, Validation loss: 1.8979\n",
            "Epoch [4540/5000], Training loss: 1.7191, Validation loss: 1.8974\n",
            "Epoch [4550/5000], Training loss: 1.7170, Validation loss: 1.8976\n",
            "Epoch [4560/5000], Training loss: 1.7225, Validation loss: 1.8994\n",
            "Epoch [4570/5000], Training loss: 1.7186, Validation loss: 1.8977\n",
            "Epoch [4580/5000], Training loss: 1.7190, Validation loss: 1.8958\n",
            "Epoch [4590/5000], Training loss: 1.7234, Validation loss: 1.8981\n",
            "Epoch [4600/5000], Training loss: 1.7234, Validation loss: 1.8992\n",
            "Epoch [4610/5000], Training loss: 1.7194, Validation loss: 1.8973\n",
            "Epoch [4620/5000], Training loss: 1.7192, Validation loss: 1.8985\n",
            "Epoch [4630/5000], Training loss: 1.7221, Validation loss: 1.8987\n",
            "Epoch [4640/5000], Training loss: 1.7206, Validation loss: 1.8995\n",
            "Epoch [4650/5000], Training loss: 1.7225, Validation loss: 1.9002\n",
            "Epoch [4660/5000], Training loss: 1.7204, Validation loss: 1.8985\n",
            "Epoch [4670/5000], Training loss: 1.7204, Validation loss: 1.9002\n",
            "Epoch [4680/5000], Training loss: 1.7199, Validation loss: 1.8993\n",
            "Epoch [4690/5000], Training loss: 1.7182, Validation loss: 1.9002\n",
            "Epoch [4700/5000], Training loss: 1.7210, Validation loss: 1.8999\n",
            "Epoch [4710/5000], Training loss: 1.7217, Validation loss: 1.8990\n",
            "Epoch [4720/5000], Training loss: 1.7205, Validation loss: 1.9013\n",
            "Epoch [4730/5000], Training loss: 1.7202, Validation loss: 1.8988\n",
            "Epoch [4740/5000], Training loss: 1.7186, Validation loss: 1.8978\n",
            "Epoch [4750/5000], Training loss: 1.7192, Validation loss: 1.9018\n",
            "Epoch [4760/5000], Training loss: 1.7240, Validation loss: 1.8984\n",
            "Epoch [4770/5000], Training loss: 1.7207, Validation loss: 1.9004\n",
            "Epoch [4780/5000], Training loss: 1.7211, Validation loss: 1.8997\n",
            "Epoch [4790/5000], Training loss: 1.7204, Validation loss: 1.8995\n",
            "Epoch [4800/5000], Training loss: 1.7194, Validation loss: 1.8986\n",
            "Epoch [4810/5000], Training loss: 1.7201, Validation loss: 1.8999\n",
            "Epoch [4820/5000], Training loss: 1.7216, Validation loss: 1.8980\n",
            "Epoch [4830/5000], Training loss: 1.7181, Validation loss: 1.8971\n",
            "Epoch [4840/5000], Training loss: 1.7191, Validation loss: 1.8987\n",
            "Epoch [4850/5000], Training loss: 1.7204, Validation loss: 1.8994\n",
            "Epoch [4860/5000], Training loss: 1.7173, Validation loss: 1.8974\n",
            "Epoch [4870/5000], Training loss: 1.7184, Validation loss: 1.9000\n",
            "Epoch [4880/5000], Training loss: 1.7216, Validation loss: 1.8996\n",
            "Epoch [4890/5000], Training loss: 1.7192, Validation loss: 1.8985\n",
            "Epoch [4900/5000], Training loss: 1.7183, Validation loss: 1.9020\n",
            "Epoch [4910/5000], Training loss: 1.7195, Validation loss: 1.9023\n",
            "Epoch [4920/5000], Training loss: 1.7197, Validation loss: 1.8995\n",
            "Epoch [4930/5000], Training loss: 1.7171, Validation loss: 1.8999\n",
            "Epoch [4940/5000], Training loss: 1.7179, Validation loss: 1.9000\n",
            "Epoch [4950/5000], Training loss: 1.7192, Validation loss: 1.8977\n",
            "Epoch [4960/5000], Training loss: 1.7174, Validation loss: 1.9003\n",
            "Epoch [4970/5000], Training loss: 1.7185, Validation loss: 1.9009\n",
            "Epoch [4980/5000], Training loss: 1.7169, Validation loss: 1.9001\n",
            "Epoch [4990/5000], Training loss: 1.7160, Validation loss: 1.8984\n",
            "Epoch [5000/5000], Training loss: 1.7189, Validation loss: 1.8997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "rnn_train_loss, rnn_losses = train(vocab_size, rnn, criterion, optimizer_rnn, epochs, train_input_sequences, train_output_sequences, val_input_sequences, val_output_sequences)"
      ],
      "metadata": {
        "id": "AzXovpPVHIpG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81cad202-84f5-4cf9-86be-10a8ac8661b0"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/5000], Training loss: 2.2808, Validation loss: 2.2784\n",
            "Epoch [20/5000], Training loss: 2.2455, Validation loss: 2.2440\n",
            "Epoch [30/5000], Training loss: 2.2156, Validation loss: 2.2159\n",
            "Epoch [40/5000], Training loss: 2.1910, Validation loss: 2.1950\n",
            "Epoch [50/5000], Training loss: 2.1681, Validation loss: 2.1778\n",
            "Epoch [60/5000], Training loss: 2.1499, Validation loss: 2.1633\n",
            "Epoch [70/5000], Training loss: 2.1345, Validation loss: 2.1504\n",
            "Epoch [80/5000], Training loss: 2.1209, Validation loss: 2.1366\n",
            "Epoch [90/5000], Training loss: 2.1082, Validation loss: 2.1242\n",
            "Epoch [100/5000], Training loss: 2.0964, Validation loss: 2.1137\n",
            "Epoch [110/5000], Training loss: 2.0827, Validation loss: 2.1041\n",
            "Epoch [120/5000], Training loss: 2.0712, Validation loss: 2.0941\n",
            "Epoch [130/5000], Training loss: 2.0573, Validation loss: 2.0840\n",
            "Epoch [140/5000], Training loss: 2.0442, Validation loss: 2.0735\n",
            "Epoch [150/5000], Training loss: 2.0344, Validation loss: 2.0654\n",
            "Epoch [160/5000], Training loss: 2.0217, Validation loss: 2.0532\n",
            "Epoch [170/5000], Training loss: 2.0064, Validation loss: 2.0386\n",
            "Epoch [180/5000], Training loss: 1.9887, Validation loss: 2.0145\n",
            "Epoch [190/5000], Training loss: 1.9603, Validation loss: 1.9798\n",
            "Epoch [200/5000], Training loss: 1.9285, Validation loss: 1.9326\n",
            "Epoch [210/5000], Training loss: 1.8908, Validation loss: 1.8841\n",
            "Epoch [220/5000], Training loss: 1.8522, Validation loss: 1.8420\n",
            "Epoch [230/5000], Training loss: 1.8209, Validation loss: 1.8027\n",
            "Epoch [240/5000], Training loss: 1.7884, Validation loss: 1.7664\n",
            "Epoch [250/5000], Training loss: 1.7578, Validation loss: 1.7332\n",
            "Epoch [260/5000], Training loss: 1.7319, Validation loss: 1.7012\n",
            "Epoch [270/5000], Training loss: 1.6988, Validation loss: 1.6673\n",
            "Epoch [280/5000], Training loss: 1.6750, Validation loss: 1.6373\n",
            "Epoch [290/5000], Training loss: 1.6504, Validation loss: 1.6080\n",
            "Epoch [300/5000], Training loss: 1.6245, Validation loss: 1.5814\n",
            "Epoch [310/5000], Training loss: 1.6041, Validation loss: 1.5564\n",
            "Epoch [320/5000], Training loss: 1.5829, Validation loss: 1.5331\n",
            "Epoch [330/5000], Training loss: 1.5661, Validation loss: 1.5110\n",
            "Epoch [340/5000], Training loss: 1.5471, Validation loss: 1.4885\n",
            "Epoch [350/5000], Training loss: 1.5297, Validation loss: 1.4678\n",
            "Epoch [360/5000], Training loss: 1.5112, Validation loss: 1.4494\n",
            "Epoch [370/5000], Training loss: 1.4946, Validation loss: 1.4311\n",
            "Epoch [380/5000], Training loss: 1.4806, Validation loss: 1.4162\n",
            "Epoch [390/5000], Training loss: 1.4607, Validation loss: 1.3984\n",
            "Epoch [400/5000], Training loss: 1.4499, Validation loss: 1.3840\n",
            "Epoch [410/5000], Training loss: 1.4369, Validation loss: 1.3709\n",
            "Epoch [420/5000], Training loss: 1.4204, Validation loss: 1.3544\n",
            "Epoch [430/5000], Training loss: 1.4114, Validation loss: 1.3423\n",
            "Epoch [440/5000], Training loss: 1.3941, Validation loss: 1.3269\n",
            "Epoch [450/5000], Training loss: 1.3843, Validation loss: 1.3157\n",
            "Epoch [460/5000], Training loss: 1.3739, Validation loss: 1.3022\n",
            "Epoch [470/5000], Training loss: 1.3632, Validation loss: 1.2912\n",
            "Epoch [480/5000], Training loss: 1.3470, Validation loss: 1.2795\n",
            "Epoch [490/5000], Training loss: 1.3433, Validation loss: 1.2700\n",
            "Epoch [500/5000], Training loss: 1.3277, Validation loss: 1.2589\n",
            "Epoch [510/5000], Training loss: 1.3177, Validation loss: 1.2509\n",
            "Epoch [520/5000], Training loss: 1.3124, Validation loss: 1.2432\n",
            "Epoch [530/5000], Training loss: 1.3019, Validation loss: 1.2353\n",
            "Epoch [540/5000], Training loss: 1.2988, Validation loss: 1.2298\n",
            "Epoch [550/5000], Training loss: 1.2863, Validation loss: 1.2226\n",
            "Epoch [560/5000], Training loss: 1.2794, Validation loss: 1.2179\n",
            "Epoch [570/5000], Training loss: 1.2699, Validation loss: 1.2127\n",
            "Epoch [580/5000], Training loss: 1.2662, Validation loss: 1.2078\n",
            "Epoch [590/5000], Training loss: 1.2613, Validation loss: 1.2041\n",
            "Epoch [600/5000], Training loss: 1.2573, Validation loss: 1.2002\n",
            "Epoch [610/5000], Training loss: 1.2510, Validation loss: 1.1969\n",
            "Epoch [620/5000], Training loss: 1.2456, Validation loss: 1.1945\n",
            "Epoch [630/5000], Training loss: 1.2396, Validation loss: 1.1915\n",
            "Epoch [640/5000], Training loss: 1.2394, Validation loss: 1.1889\n",
            "Epoch [650/5000], Training loss: 1.2328, Validation loss: 1.1866\n",
            "Epoch [660/5000], Training loss: 1.2297, Validation loss: 1.1844\n",
            "Epoch [670/5000], Training loss: 1.2244, Validation loss: 1.1828\n",
            "Epoch [680/5000], Training loss: 1.2239, Validation loss: 1.1808\n",
            "Epoch [690/5000], Training loss: 1.2216, Validation loss: 1.1795\n",
            "Epoch [700/5000], Training loss: 1.2162, Validation loss: 1.1780\n",
            "Epoch [710/5000], Training loss: 1.2170, Validation loss: 1.1761\n",
            "Epoch [720/5000], Training loss: 1.2148, Validation loss: 1.1751\n",
            "Epoch [730/5000], Training loss: 1.2098, Validation loss: 1.1740\n",
            "Epoch [740/5000], Training loss: 1.2076, Validation loss: 1.1731\n",
            "Epoch [750/5000], Training loss: 1.2067, Validation loss: 1.1720\n",
            "Epoch [760/5000], Training loss: 1.2045, Validation loss: 1.1712\n",
            "Epoch [770/5000], Training loss: 1.2028, Validation loss: 1.1704\n",
            "Epoch [780/5000], Training loss: 1.2028, Validation loss: 1.1694\n",
            "Epoch [790/5000], Training loss: 1.1989, Validation loss: 1.1694\n",
            "Epoch [800/5000], Training loss: 1.1968, Validation loss: 1.1683\n",
            "Epoch [810/5000], Training loss: 1.1942, Validation loss: 1.1678\n",
            "Epoch [820/5000], Training loss: 1.1919, Validation loss: 1.1672\n",
            "Epoch [830/5000], Training loss: 1.1939, Validation loss: 1.1665\n",
            "Epoch [840/5000], Training loss: 1.1920, Validation loss: 1.1662\n",
            "Epoch [850/5000], Training loss: 1.1891, Validation loss: 1.1659\n",
            "Epoch [860/5000], Training loss: 1.1884, Validation loss: 1.1653\n",
            "Epoch [870/5000], Training loss: 1.1854, Validation loss: 1.1649\n",
            "Epoch [880/5000], Training loss: 1.1858, Validation loss: 1.1646\n",
            "Epoch [890/5000], Training loss: 1.1854, Validation loss: 1.1645\n",
            "Epoch [900/5000], Training loss: 1.1842, Validation loss: 1.1637\n",
            "Epoch [910/5000], Training loss: 1.1833, Validation loss: 1.1638\n",
            "Epoch [920/5000], Training loss: 1.1805, Validation loss: 1.1632\n",
            "Epoch [930/5000], Training loss: 1.1809, Validation loss: 1.1630\n",
            "Epoch [940/5000], Training loss: 1.1790, Validation loss: 1.1628\n",
            "Epoch [950/5000], Training loss: 1.1806, Validation loss: 1.1622\n",
            "Epoch [960/5000], Training loss: 1.1782, Validation loss: 1.1623\n",
            "Epoch [970/5000], Training loss: 1.1751, Validation loss: 1.1620\n",
            "Epoch [980/5000], Training loss: 1.1752, Validation loss: 1.1615\n",
            "Epoch [990/5000], Training loss: 1.1747, Validation loss: 1.1616\n",
            "Epoch [1000/5000], Training loss: 1.1733, Validation loss: 1.1615\n",
            "Epoch [1010/5000], Training loss: 1.1754, Validation loss: 1.1613\n",
            "Epoch [1020/5000], Training loss: 1.1701, Validation loss: 1.1610\n",
            "Epoch [1030/5000], Training loss: 1.1712, Validation loss: 1.1607\n",
            "Epoch [1040/5000], Training loss: 1.1709, Validation loss: 1.1611\n",
            "Epoch [1050/5000], Training loss: 1.1714, Validation loss: 1.1606\n",
            "Epoch [1060/5000], Training loss: 1.1715, Validation loss: 1.1605\n",
            "Epoch [1070/5000], Training loss: 1.1686, Validation loss: 1.1603\n",
            "Epoch [1080/5000], Training loss: 1.1715, Validation loss: 1.1601\n",
            "Epoch [1090/5000], Training loss: 1.1704, Validation loss: 1.1600\n",
            "Epoch [1100/5000], Training loss: 1.1706, Validation loss: 1.1600\n",
            "Epoch [1110/5000], Training loss: 1.1687, Validation loss: 1.1601\n",
            "Epoch [1120/5000], Training loss: 1.1680, Validation loss: 1.1600\n",
            "Epoch [1130/5000], Training loss: 1.1675, Validation loss: 1.1597\n",
            "Epoch [1140/5000], Training loss: 1.1659, Validation loss: 1.1595\n",
            "Epoch [1150/5000], Training loss: 1.1659, Validation loss: 1.1599\n",
            "Epoch [1160/5000], Training loss: 1.1660, Validation loss: 1.1600\n",
            "Epoch [1170/5000], Training loss: 1.1653, Validation loss: 1.1599\n",
            "Epoch [1180/5000], Training loss: 1.1643, Validation loss: 1.1599\n",
            "Epoch [1190/5000], Training loss: 1.1649, Validation loss: 1.1593\n",
            "Epoch [1200/5000], Training loss: 1.1660, Validation loss: 1.1593\n",
            "Epoch [1210/5000], Training loss: 1.1635, Validation loss: 1.1593\n",
            "Epoch [1220/5000], Training loss: 1.1627, Validation loss: 1.1591\n",
            "Epoch [1230/5000], Training loss: 1.1618, Validation loss: 1.1592\n",
            "Epoch [1240/5000], Training loss: 1.1646, Validation loss: 1.1590\n",
            "Epoch [1250/5000], Training loss: 1.1625, Validation loss: 1.1589\n",
            "Epoch [1260/5000], Training loss: 1.1624, Validation loss: 1.1592\n",
            "Epoch [1270/5000], Training loss: 1.1608, Validation loss: 1.1590\n",
            "Epoch [1280/5000], Training loss: 1.1617, Validation loss: 1.1589\n",
            "Epoch [1290/5000], Training loss: 1.1614, Validation loss: 1.1590\n",
            "Epoch [1300/5000], Training loss: 1.1604, Validation loss: 1.1592\n",
            "Epoch [1310/5000], Training loss: 1.1607, Validation loss: 1.1590\n",
            "Epoch [1320/5000], Training loss: 1.1602, Validation loss: 1.1585\n",
            "Epoch [1330/5000], Training loss: 1.1601, Validation loss: 1.1591\n",
            "Epoch [1340/5000], Training loss: 1.1588, Validation loss: 1.1589\n",
            "Epoch [1350/5000], Training loss: 1.1607, Validation loss: 1.1586\n",
            "Epoch [1360/5000], Training loss: 1.1580, Validation loss: 1.1583\n",
            "Epoch [1370/5000], Training loss: 1.1591, Validation loss: 1.1588\n",
            "Epoch [1380/5000], Training loss: 1.1595, Validation loss: 1.1591\n",
            "Epoch [1390/5000], Training loss: 1.1571, Validation loss: 1.1590\n",
            "Epoch [1400/5000], Training loss: 1.1597, Validation loss: 1.1585\n",
            "Epoch [1410/5000], Training loss: 1.1586, Validation loss: 1.1588\n",
            "Epoch [1420/5000], Training loss: 1.1568, Validation loss: 1.1591\n",
            "Epoch [1430/5000], Training loss: 1.1577, Validation loss: 1.1587\n",
            "Epoch [1440/5000], Training loss: 1.1584, Validation loss: 1.1585\n",
            "Epoch [1450/5000], Training loss: 1.1561, Validation loss: 1.1588\n",
            "Epoch [1460/5000], Training loss: 1.1586, Validation loss: 1.1587\n",
            "Epoch [1470/5000], Training loss: 1.1552, Validation loss: 1.1585\n",
            "Epoch [1480/5000], Training loss: 1.1558, Validation loss: 1.1588\n",
            "Epoch [1490/5000], Training loss: 1.1563, Validation loss: 1.1585\n",
            "Epoch [1500/5000], Training loss: 1.1556, Validation loss: 1.1589\n",
            "Epoch [1510/5000], Training loss: 1.1551, Validation loss: 1.1583\n",
            "Epoch [1520/5000], Training loss: 1.1552, Validation loss: 1.1582\n",
            "Epoch [1530/5000], Training loss: 1.1566, Validation loss: 1.1590\n",
            "Epoch [1540/5000], Training loss: 1.1552, Validation loss: 1.1587\n",
            "Epoch [1550/5000], Training loss: 1.1555, Validation loss: 1.1591\n",
            "Epoch [1560/5000], Training loss: 1.1548, Validation loss: 1.1590\n",
            "Epoch [1570/5000], Training loss: 1.1527, Validation loss: 1.1586\n",
            "Epoch [1580/5000], Training loss: 1.1543, Validation loss: 1.1586\n",
            "Epoch [1590/5000], Training loss: 1.1538, Validation loss: 1.1591\n",
            "Epoch [1600/5000], Training loss: 1.1533, Validation loss: 1.1588\n",
            "Epoch [1610/5000], Training loss: 1.1554, Validation loss: 1.1587\n",
            "Epoch [1620/5000], Training loss: 1.1532, Validation loss: 1.1587\n",
            "Epoch [1630/5000], Training loss: 1.1534, Validation loss: 1.1588\n",
            "Epoch [1640/5000], Training loss: 1.1521, Validation loss: 1.1588\n",
            "Epoch [1650/5000], Training loss: 1.1521, Validation loss: 1.1587\n",
            "Epoch [1660/5000], Training loss: 1.1528, Validation loss: 1.1590\n",
            "Epoch [1670/5000], Training loss: 1.1521, Validation loss: 1.1590\n",
            "Epoch [1680/5000], Training loss: 1.1528, Validation loss: 1.1592\n",
            "Epoch [1690/5000], Training loss: 1.1504, Validation loss: 1.1590\n",
            "Epoch [1700/5000], Training loss: 1.1526, Validation loss: 1.1587\n",
            "Epoch [1710/5000], Training loss: 1.1502, Validation loss: 1.1590\n",
            "Epoch [1720/5000], Training loss: 1.1517, Validation loss: 1.1588\n",
            "Epoch [1730/5000], Training loss: 1.1512, Validation loss: 1.1591\n",
            "Epoch [1740/5000], Training loss: 1.1501, Validation loss: 1.1594\n",
            "Epoch [1750/5000], Training loss: 1.1519, Validation loss: 1.1593\n",
            "Epoch [1760/5000], Training loss: 1.1516, Validation loss: 1.1588\n",
            "Epoch [1770/5000], Training loss: 1.1498, Validation loss: 1.1588\n",
            "Epoch [1780/5000], Training loss: 1.1498, Validation loss: 1.1593\n",
            "Epoch [1790/5000], Training loss: 1.1497, Validation loss: 1.1592\n",
            "Epoch [1800/5000], Training loss: 1.1506, Validation loss: 1.1593\n",
            "Epoch [1810/5000], Training loss: 1.1500, Validation loss: 1.1591\n",
            "Epoch [1820/5000], Training loss: 1.1481, Validation loss: 1.1590\n",
            "Epoch [1830/5000], Training loss: 1.1506, Validation loss: 1.1594\n",
            "Epoch [1840/5000], Training loss: 1.1510, Validation loss: 1.1591\n",
            "Epoch [1850/5000], Training loss: 1.1492, Validation loss: 1.1593\n",
            "Epoch [1860/5000], Training loss: 1.1490, Validation loss: 1.1593\n",
            "Epoch [1870/5000], Training loss: 1.1488, Validation loss: 1.1595\n",
            "Epoch [1880/5000], Training loss: 1.1488, Validation loss: 1.1597\n",
            "Epoch [1890/5000], Training loss: 1.1486, Validation loss: 1.1597\n",
            "Epoch [1900/5000], Training loss: 1.1491, Validation loss: 1.1596\n",
            "Epoch [1910/5000], Training loss: 1.1491, Validation loss: 1.1597\n",
            "Epoch [1920/5000], Training loss: 1.1488, Validation loss: 1.1597\n",
            "Epoch [1930/5000], Training loss: 1.1471, Validation loss: 1.1599\n",
            "Epoch [1940/5000], Training loss: 1.1484, Validation loss: 1.1596\n",
            "Epoch [1950/5000], Training loss: 1.1487, Validation loss: 1.1595\n",
            "Epoch [1960/5000], Training loss: 1.1481, Validation loss: 1.1598\n",
            "Epoch [1970/5000], Training loss: 1.1486, Validation loss: 1.1593\n",
            "Epoch [1980/5000], Training loss: 1.1469, Validation loss: 1.1598\n",
            "Epoch [1990/5000], Training loss: 1.1480, Validation loss: 1.1600\n",
            "Epoch [2000/5000], Training loss: 1.1486, Validation loss: 1.1599\n",
            "Epoch [2010/5000], Training loss: 1.1467, Validation loss: 1.1597\n",
            "Epoch [2020/5000], Training loss: 1.1469, Validation loss: 1.1597\n",
            "Epoch [2030/5000], Training loss: 1.1460, Validation loss: 1.1602\n",
            "Epoch [2040/5000], Training loss: 1.1465, Validation loss: 1.1600\n",
            "Epoch [2050/5000], Training loss: 1.1458, Validation loss: 1.1597\n",
            "Epoch [2060/5000], Training loss: 1.1467, Validation loss: 1.1601\n",
            "Epoch [2070/5000], Training loss: 1.1478, Validation loss: 1.1602\n",
            "Epoch [2080/5000], Training loss: 1.1451, Validation loss: 1.1605\n",
            "Epoch [2090/5000], Training loss: 1.1458, Validation loss: 1.1604\n",
            "Epoch [2100/5000], Training loss: 1.1466, Validation loss: 1.1604\n",
            "Epoch [2110/5000], Training loss: 1.1463, Validation loss: 1.1601\n",
            "Epoch [2120/5000], Training loss: 1.1472, Validation loss: 1.1608\n",
            "Epoch [2130/5000], Training loss: 1.1461, Validation loss: 1.1603\n",
            "Epoch [2140/5000], Training loss: 1.1424, Validation loss: 1.1603\n",
            "Epoch [2150/5000], Training loss: 1.1452, Validation loss: 1.1604\n",
            "Epoch [2160/5000], Training loss: 1.1441, Validation loss: 1.1603\n",
            "Epoch [2170/5000], Training loss: 1.1435, Validation loss: 1.1603\n",
            "Epoch [2180/5000], Training loss: 1.1467, Validation loss: 1.1601\n",
            "Epoch [2190/5000], Training loss: 1.1449, Validation loss: 1.1605\n",
            "Epoch [2200/5000], Training loss: 1.1441, Validation loss: 1.1605\n",
            "Epoch [2210/5000], Training loss: 1.1427, Validation loss: 1.1607\n",
            "Epoch [2220/5000], Training loss: 1.1448, Validation loss: 1.1604\n",
            "Epoch [2230/5000], Training loss: 1.1449, Validation loss: 1.1605\n",
            "Epoch [2240/5000], Training loss: 1.1446, Validation loss: 1.1612\n",
            "Epoch [2250/5000], Training loss: 1.1455, Validation loss: 1.1614\n",
            "Epoch [2260/5000], Training loss: 1.1444, Validation loss: 1.1606\n",
            "Epoch [2270/5000], Training loss: 1.1441, Validation loss: 1.1608\n",
            "Epoch [2280/5000], Training loss: 1.1426, Validation loss: 1.1608\n",
            "Epoch [2290/5000], Training loss: 1.1432, Validation loss: 1.1609\n",
            "Epoch [2300/5000], Training loss: 1.1427, Validation loss: 1.1607\n",
            "Epoch [2310/5000], Training loss: 1.1440, Validation loss: 1.1609\n",
            "Epoch [2320/5000], Training loss: 1.1433, Validation loss: 1.1602\n",
            "Epoch [2330/5000], Training loss: 1.1431, Validation loss: 1.1609\n",
            "Epoch [2340/5000], Training loss: 1.1430, Validation loss: 1.1611\n",
            "Epoch [2350/5000], Training loss: 1.1420, Validation loss: 1.1613\n",
            "Epoch [2360/5000], Training loss: 1.1438, Validation loss: 1.1617\n",
            "Epoch [2370/5000], Training loss: 1.1427, Validation loss: 1.1613\n",
            "Epoch [2380/5000], Training loss: 1.1417, Validation loss: 1.1611\n",
            "Epoch [2390/5000], Training loss: 1.1410, Validation loss: 1.1615\n",
            "Epoch [2400/5000], Training loss: 1.1424, Validation loss: 1.1612\n",
            "Epoch [2410/5000], Training loss: 1.1414, Validation loss: 1.1612\n",
            "Epoch [2420/5000], Training loss: 1.1422, Validation loss: 1.1616\n",
            "Epoch [2430/5000], Training loss: 1.1403, Validation loss: 1.1624\n",
            "Epoch [2440/5000], Training loss: 1.1421, Validation loss: 1.1622\n",
            "Epoch [2450/5000], Training loss: 1.1411, Validation loss: 1.1630\n",
            "Epoch [2460/5000], Training loss: 1.1416, Validation loss: 1.1625\n",
            "Epoch [2470/5000], Training loss: 1.1418, Validation loss: 1.1618\n",
            "Epoch [2480/5000], Training loss: 1.1400, Validation loss: 1.1620\n",
            "Epoch [2490/5000], Training loss: 1.1403, Validation loss: 1.1621\n",
            "Epoch [2500/5000], Training loss: 1.1407, Validation loss: 1.1637\n",
            "Epoch [2510/5000], Training loss: 1.1415, Validation loss: 1.1627\n",
            "Epoch [2520/5000], Training loss: 1.1394, Validation loss: 1.1625\n",
            "Epoch [2530/5000], Training loss: 1.1402, Validation loss: 1.1624\n",
            "Epoch [2540/5000], Training loss: 1.1406, Validation loss: 1.1628\n",
            "Epoch [2550/5000], Training loss: 1.1407, Validation loss: 1.1630\n",
            "Epoch [2560/5000], Training loss: 1.1390, Validation loss: 1.1630\n",
            "Epoch [2570/5000], Training loss: 1.1400, Validation loss: 1.1627\n",
            "Epoch [2580/5000], Training loss: 1.1399, Validation loss: 1.1628\n",
            "Epoch [2590/5000], Training loss: 1.1380, Validation loss: 1.1626\n",
            "Epoch [2600/5000], Training loss: 1.1389, Validation loss: 1.1631\n",
            "Epoch [2610/5000], Training loss: 1.1386, Validation loss: 1.1627\n",
            "Epoch [2620/5000], Training loss: 1.1388, Validation loss: 1.1635\n",
            "Epoch [2630/5000], Training loss: 1.1395, Validation loss: 1.1637\n",
            "Epoch [2640/5000], Training loss: 1.1391, Validation loss: 1.1634\n",
            "Epoch [2650/5000], Training loss: 1.1395, Validation loss: 1.1633\n",
            "Epoch [2660/5000], Training loss: 1.1404, Validation loss: 1.1635\n",
            "Epoch [2670/5000], Training loss: 1.1391, Validation loss: 1.1635\n",
            "Epoch [2680/5000], Training loss: 1.1403, Validation loss: 1.1641\n",
            "Epoch [2690/5000], Training loss: 1.1376, Validation loss: 1.1637\n",
            "Epoch [2700/5000], Training loss: 1.1380, Validation loss: 1.1637\n",
            "Epoch [2710/5000], Training loss: 1.1370, Validation loss: 1.1644\n",
            "Epoch [2720/5000], Training loss: 1.1399, Validation loss: 1.1645\n",
            "Epoch [2730/5000], Training loss: 1.1400, Validation loss: 1.1648\n",
            "Epoch [2740/5000], Training loss: 1.1385, Validation loss: 1.1643\n",
            "Epoch [2750/5000], Training loss: 1.1366, Validation loss: 1.1649\n",
            "Epoch [2760/5000], Training loss: 1.1385, Validation loss: 1.1652\n",
            "Epoch [2770/5000], Training loss: 1.1369, Validation loss: 1.1650\n",
            "Epoch [2780/5000], Training loss: 1.1379, Validation loss: 1.1650\n",
            "Epoch [2790/5000], Training loss: 1.1384, Validation loss: 1.1666\n",
            "Epoch [2800/5000], Training loss: 1.1357, Validation loss: 1.1660\n",
            "Epoch [2810/5000], Training loss: 1.1372, Validation loss: 1.1663\n",
            "Epoch [2820/5000], Training loss: 1.1368, Validation loss: 1.1659\n",
            "Epoch [2830/5000], Training loss: 1.1373, Validation loss: 1.1657\n",
            "Epoch [2840/5000], Training loss: 1.1366, Validation loss: 1.1666\n",
            "Epoch [2850/5000], Training loss: 1.1359, Validation loss: 1.1667\n",
            "Epoch [2860/5000], Training loss: 1.1366, Validation loss: 1.1666\n",
            "Epoch [2870/5000], Training loss: 1.1360, Validation loss: 1.1672\n",
            "Epoch [2880/5000], Training loss: 1.1367, Validation loss: 1.1671\n",
            "Epoch [2890/5000], Training loss: 1.1364, Validation loss: 1.1670\n",
            "Epoch [2900/5000], Training loss: 1.1360, Validation loss: 1.1667\n",
            "Epoch [2910/5000], Training loss: 1.1350, Validation loss: 1.1666\n",
            "Epoch [2920/5000], Training loss: 1.1354, Validation loss: 1.1667\n",
            "Epoch [2930/5000], Training loss: 1.1344, Validation loss: 1.1676\n",
            "Epoch [2940/5000], Training loss: 1.1341, Validation loss: 1.1673\n",
            "Epoch [2950/5000], Training loss: 1.1367, Validation loss: 1.1684\n",
            "Epoch [2960/5000], Training loss: 1.1335, Validation loss: 1.1669\n",
            "Epoch [2970/5000], Training loss: 1.1350, Validation loss: 1.1663\n",
            "Epoch [2980/5000], Training loss: 1.1352, Validation loss: 1.1671\n",
            "Epoch [2990/5000], Training loss: 1.1350, Validation loss: 1.1670\n",
            "Epoch [3000/5000], Training loss: 1.1347, Validation loss: 1.1677\n",
            "Epoch [3010/5000], Training loss: 1.1345, Validation loss: 1.1683\n",
            "Epoch [3020/5000], Training loss: 1.1345, Validation loss: 1.1686\n",
            "Epoch [3030/5000], Training loss: 1.1334, Validation loss: 1.1679\n",
            "Epoch [3040/5000], Training loss: 1.1353, Validation loss: 1.1687\n",
            "Epoch [3050/5000], Training loss: 1.1349, Validation loss: 1.1671\n",
            "Epoch [3060/5000], Training loss: 1.1352, Validation loss: 1.1677\n",
            "Epoch [3070/5000], Training loss: 1.1329, Validation loss: 1.1685\n",
            "Epoch [3080/5000], Training loss: 1.1325, Validation loss: 1.1684\n",
            "Epoch [3090/5000], Training loss: 1.1330, Validation loss: 1.1687\n",
            "Epoch [3100/5000], Training loss: 1.1328, Validation loss: 1.1697\n",
            "Epoch [3110/5000], Training loss: 1.1318, Validation loss: 1.1695\n",
            "Epoch [3120/5000], Training loss: 1.1321, Validation loss: 1.1698\n",
            "Epoch [3130/5000], Training loss: 1.1340, Validation loss: 1.1698\n",
            "Epoch [3140/5000], Training loss: 1.1337, Validation loss: 1.1687\n",
            "Epoch [3150/5000], Training loss: 1.1333, Validation loss: 1.1699\n",
            "Epoch [3160/5000], Training loss: 1.1325, Validation loss: 1.1706\n",
            "Epoch [3170/5000], Training loss: 1.1330, Validation loss: 1.1712\n",
            "Epoch [3180/5000], Training loss: 1.1343, Validation loss: 1.1707\n",
            "Epoch [3190/5000], Training loss: 1.1330, Validation loss: 1.1710\n",
            "Epoch [3200/5000], Training loss: 1.1351, Validation loss: 1.1702\n",
            "Epoch [3210/5000], Training loss: 1.1341, Validation loss: 1.1701\n",
            "Epoch [3220/5000], Training loss: 1.1334, Validation loss: 1.1705\n",
            "Epoch [3230/5000], Training loss: 1.1320, Validation loss: 1.1710\n",
            "Epoch [3240/5000], Training loss: 1.1320, Validation loss: 1.1708\n",
            "Epoch [3250/5000], Training loss: 1.1310, Validation loss: 1.1719\n",
            "Epoch [3260/5000], Training loss: 1.1308, Validation loss: 1.1710\n",
            "Epoch [3270/5000], Training loss: 1.1311, Validation loss: 1.1707\n",
            "Epoch [3280/5000], Training loss: 1.1292, Validation loss: 1.1696\n",
            "Epoch [3290/5000], Training loss: 1.1314, Validation loss: 1.1700\n",
            "Epoch [3300/5000], Training loss: 1.1312, Validation loss: 1.1718\n",
            "Epoch [3310/5000], Training loss: 1.1319, Validation loss: 1.1705\n",
            "Epoch [3320/5000], Training loss: 1.1303, Validation loss: 1.1710\n",
            "Epoch [3330/5000], Training loss: 1.1296, Validation loss: 1.1729\n",
            "Epoch [3340/5000], Training loss: 1.1310, Validation loss: 1.1710\n",
            "Epoch [3350/5000], Training loss: 1.1306, Validation loss: 1.1736\n",
            "Epoch [3360/5000], Training loss: 1.1298, Validation loss: 1.1715\n",
            "Epoch [3370/5000], Training loss: 1.1312, Validation loss: 1.1712\n",
            "Epoch [3380/5000], Training loss: 1.1308, Validation loss: 1.1729\n",
            "Epoch [3390/5000], Training loss: 1.1294, Validation loss: 1.1724\n",
            "Epoch [3400/5000], Training loss: 1.1319, Validation loss: 1.1725\n",
            "Epoch [3410/5000], Training loss: 1.1315, Validation loss: 1.1723\n",
            "Epoch [3420/5000], Training loss: 1.1291, Validation loss: 1.1727\n",
            "Epoch [3430/5000], Training loss: 1.1304, Validation loss: 1.1722\n",
            "Epoch [3440/5000], Training loss: 1.1297, Validation loss: 1.1738\n",
            "Epoch [3450/5000], Training loss: 1.1290, Validation loss: 1.1730\n",
            "Epoch [3460/5000], Training loss: 1.1294, Validation loss: 1.1722\n",
            "Epoch [3470/5000], Training loss: 1.1287, Validation loss: 1.1723\n",
            "Epoch [3480/5000], Training loss: 1.1290, Validation loss: 1.1726\n",
            "Epoch [3490/5000], Training loss: 1.1308, Validation loss: 1.1729\n",
            "Epoch [3500/5000], Training loss: 1.1300, Validation loss: 1.1727\n",
            "Epoch [3510/5000], Training loss: 1.1299, Validation loss: 1.1735\n",
            "Epoch [3520/5000], Training loss: 1.1293, Validation loss: 1.1734\n",
            "Epoch [3530/5000], Training loss: 1.1305, Validation loss: 1.1738\n",
            "Epoch [3540/5000], Training loss: 1.1303, Validation loss: 1.1734\n",
            "Epoch [3550/5000], Training loss: 1.1297, Validation loss: 1.1737\n",
            "Epoch [3560/5000], Training loss: 1.1291, Validation loss: 1.1733\n",
            "Epoch [3570/5000], Training loss: 1.1268, Validation loss: 1.1730\n",
            "Epoch [3580/5000], Training loss: 1.1275, Validation loss: 1.1744\n",
            "Epoch [3590/5000], Training loss: 1.1280, Validation loss: 1.1739\n",
            "Epoch [3600/5000], Training loss: 1.1298, Validation loss: 1.1762\n",
            "Epoch [3610/5000], Training loss: 1.1267, Validation loss: 1.1751\n",
            "Epoch [3620/5000], Training loss: 1.1289, Validation loss: 1.1751\n",
            "Epoch [3630/5000], Training loss: 1.1265, Validation loss: 1.1752\n",
            "Epoch [3640/5000], Training loss: 1.1272, Validation loss: 1.1730\n",
            "Epoch [3650/5000], Training loss: 1.1279, Validation loss: 1.1737\n",
            "Epoch [3660/5000], Training loss: 1.1275, Validation loss: 1.1752\n",
            "Epoch [3670/5000], Training loss: 1.1271, Validation loss: 1.1764\n",
            "Epoch [3680/5000], Training loss: 1.1265, Validation loss: 1.1757\n",
            "Epoch [3690/5000], Training loss: 1.1265, Validation loss: 1.1753\n",
            "Epoch [3700/5000], Training loss: 1.1270, Validation loss: 1.1746\n",
            "Epoch [3710/5000], Training loss: 1.1284, Validation loss: 1.1756\n",
            "Epoch [3720/5000], Training loss: 1.1281, Validation loss: 1.1773\n",
            "Epoch [3730/5000], Training loss: 1.1267, Validation loss: 1.1788\n",
            "Epoch [3740/5000], Training loss: 1.1249, Validation loss: 1.1764\n",
            "Epoch [3750/5000], Training loss: 1.1255, Validation loss: 1.1764\n",
            "Epoch [3760/5000], Training loss: 1.1268, Validation loss: 1.1760\n",
            "Epoch [3770/5000], Training loss: 1.1251, Validation loss: 1.1769\n",
            "Epoch [3780/5000], Training loss: 1.1278, Validation loss: 1.1763\n",
            "Epoch [3790/5000], Training loss: 1.1265, Validation loss: 1.1778\n",
            "Epoch [3800/5000], Training loss: 1.1267, Validation loss: 1.1757\n",
            "Epoch [3810/5000], Training loss: 1.1277, Validation loss: 1.1765\n",
            "Epoch [3820/5000], Training loss: 1.1269, Validation loss: 1.1770\n",
            "Epoch [3830/5000], Training loss: 1.1243, Validation loss: 1.1763\n",
            "Epoch [3840/5000], Training loss: 1.1276, Validation loss: 1.1748\n",
            "Epoch [3850/5000], Training loss: 1.1257, Validation loss: 1.1774\n",
            "Epoch [3860/5000], Training loss: 1.1262, Validation loss: 1.1785\n",
            "Epoch [3870/5000], Training loss: 1.1267, Validation loss: 1.1768\n",
            "Epoch [3880/5000], Training loss: 1.1252, Validation loss: 1.1774\n",
            "Epoch [3890/5000], Training loss: 1.1244, Validation loss: 1.1783\n",
            "Epoch [3900/5000], Training loss: 1.1248, Validation loss: 1.1779\n",
            "Epoch [3910/5000], Training loss: 1.1257, Validation loss: 1.1797\n",
            "Epoch [3920/5000], Training loss: 1.1263, Validation loss: 1.1783\n",
            "Epoch [3930/5000], Training loss: 1.1234, Validation loss: 1.1780\n",
            "Epoch [3940/5000], Training loss: 1.1262, Validation loss: 1.1775\n",
            "Epoch [3950/5000], Training loss: 1.1255, Validation loss: 1.1774\n",
            "Epoch [3960/5000], Training loss: 1.1248, Validation loss: 1.1777\n",
            "Epoch [3970/5000], Training loss: 1.1240, Validation loss: 1.1786\n",
            "Epoch [3980/5000], Training loss: 1.1254, Validation loss: 1.1802\n",
            "Epoch [3990/5000], Training loss: 1.1235, Validation loss: 1.1775\n",
            "Epoch [4000/5000], Training loss: 1.1226, Validation loss: 1.1776\n",
            "Epoch [4010/5000], Training loss: 1.1236, Validation loss: 1.1776\n",
            "Epoch [4020/5000], Training loss: 1.1235, Validation loss: 1.1773\n",
            "Epoch [4030/5000], Training loss: 1.1239, Validation loss: 1.1776\n",
            "Epoch [4040/5000], Training loss: 1.1239, Validation loss: 1.1807\n",
            "Epoch [4050/5000], Training loss: 1.1244, Validation loss: 1.1782\n",
            "Epoch [4060/5000], Training loss: 1.1228, Validation loss: 1.1798\n",
            "Epoch [4070/5000], Training loss: 1.1248, Validation loss: 1.1786\n",
            "Epoch [4080/5000], Training loss: 1.1238, Validation loss: 1.1797\n",
            "Epoch [4090/5000], Training loss: 1.1242, Validation loss: 1.1808\n",
            "Epoch [4100/5000], Training loss: 1.1249, Validation loss: 1.1799\n",
            "Epoch [4110/5000], Training loss: 1.1227, Validation loss: 1.1807\n",
            "Epoch [4120/5000], Training loss: 1.1236, Validation loss: 1.1805\n",
            "Epoch [4130/5000], Training loss: 1.1219, Validation loss: 1.1801\n",
            "Epoch [4140/5000], Training loss: 1.1236, Validation loss: 1.1804\n",
            "Epoch [4150/5000], Training loss: 1.1238, Validation loss: 1.1826\n",
            "Epoch [4160/5000], Training loss: 1.1211, Validation loss: 1.1797\n",
            "Epoch [4170/5000], Training loss: 1.1226, Validation loss: 1.1808\n",
            "Epoch [4180/5000], Training loss: 1.1229, Validation loss: 1.1825\n",
            "Epoch [4190/5000], Training loss: 1.1252, Validation loss: 1.1822\n",
            "Epoch [4200/5000], Training loss: 1.1238, Validation loss: 1.1803\n",
            "Epoch [4210/5000], Training loss: 1.1207, Validation loss: 1.1789\n",
            "Epoch [4220/5000], Training loss: 1.1229, Validation loss: 1.1793\n",
            "Epoch [4230/5000], Training loss: 1.1231, Validation loss: 1.1797\n",
            "Epoch [4240/5000], Training loss: 1.1224, Validation loss: 1.1802\n",
            "Epoch [4250/5000], Training loss: 1.1208, Validation loss: 1.1804\n",
            "Epoch [4260/5000], Training loss: 1.1224, Validation loss: 1.1827\n",
            "Epoch [4270/5000], Training loss: 1.1224, Validation loss: 1.1808\n",
            "Epoch [4280/5000], Training loss: 1.1230, Validation loss: 1.1807\n",
            "Epoch [4290/5000], Training loss: 1.1228, Validation loss: 1.1809\n",
            "Epoch [4300/5000], Training loss: 1.1249, Validation loss: 1.1799\n",
            "Epoch [4310/5000], Training loss: 1.1219, Validation loss: 1.1808\n",
            "Epoch [4320/5000], Training loss: 1.1217, Validation loss: 1.1806\n",
            "Epoch [4330/5000], Training loss: 1.1205, Validation loss: 1.1800\n",
            "Epoch [4340/5000], Training loss: 1.1195, Validation loss: 1.1789\n",
            "Epoch [4350/5000], Training loss: 1.1215, Validation loss: 1.1796\n",
            "Epoch [4360/5000], Training loss: 1.1214, Validation loss: 1.1792\n",
            "Epoch [4370/5000], Training loss: 1.1217, Validation loss: 1.1812\n",
            "Epoch [4380/5000], Training loss: 1.1230, Validation loss: 1.1820\n",
            "Epoch [4390/5000], Training loss: 1.1180, Validation loss: 1.1813\n",
            "Epoch [4400/5000], Training loss: 1.1206, Validation loss: 1.1803\n",
            "Epoch [4410/5000], Training loss: 1.1213, Validation loss: 1.1815\n",
            "Epoch [4420/5000], Training loss: 1.1210, Validation loss: 1.1811\n",
            "Epoch [4430/5000], Training loss: 1.1215, Validation loss: 1.1808\n",
            "Epoch [4440/5000], Training loss: 1.1173, Validation loss: 1.1784\n",
            "Epoch [4450/5000], Training loss: 1.1198, Validation loss: 1.1818\n",
            "Epoch [4460/5000], Training loss: 1.1216, Validation loss: 1.1829\n",
            "Epoch [4470/5000], Training loss: 1.1223, Validation loss: 1.1821\n",
            "Epoch [4480/5000], Training loss: 1.1198, Validation loss: 1.1829\n",
            "Epoch [4490/5000], Training loss: 1.1203, Validation loss: 1.1814\n",
            "Epoch [4500/5000], Training loss: 1.1192, Validation loss: 1.1833\n",
            "Epoch [4510/5000], Training loss: 1.1199, Validation loss: 1.1844\n",
            "Epoch [4520/5000], Training loss: 1.1214, Validation loss: 1.1842\n",
            "Epoch [4530/5000], Training loss: 1.1202, Validation loss: 1.1827\n",
            "Epoch [4540/5000], Training loss: 1.1219, Validation loss: 1.1849\n",
            "Epoch [4550/5000], Training loss: 1.1192, Validation loss: 1.1826\n",
            "Epoch [4560/5000], Training loss: 1.1180, Validation loss: 1.1821\n",
            "Epoch [4570/5000], Training loss: 1.1187, Validation loss: 1.1825\n",
            "Epoch [4580/5000], Training loss: 1.1184, Validation loss: 1.1835\n",
            "Epoch [4590/5000], Training loss: 1.1166, Validation loss: 1.1853\n",
            "Epoch [4600/5000], Training loss: 1.1199, Validation loss: 1.1852\n",
            "Epoch [4610/5000], Training loss: 1.1178, Validation loss: 1.1863\n",
            "Epoch [4620/5000], Training loss: 1.1205, Validation loss: 1.1830\n",
            "Epoch [4630/5000], Training loss: 1.1196, Validation loss: 1.1838\n",
            "Epoch [4640/5000], Training loss: 1.1194, Validation loss: 1.1839\n",
            "Epoch [4650/5000], Training loss: 1.1202, Validation loss: 1.1842\n",
            "Epoch [4660/5000], Training loss: 1.1198, Validation loss: 1.1850\n",
            "Epoch [4670/5000], Training loss: 1.1178, Validation loss: 1.1847\n",
            "Epoch [4680/5000], Training loss: 1.1159, Validation loss: 1.1848\n",
            "Epoch [4690/5000], Training loss: 1.1175, Validation loss: 1.1828\n",
            "Epoch [4700/5000], Training loss: 1.1205, Validation loss: 1.1838\n",
            "Epoch [4710/5000], Training loss: 1.1200, Validation loss: 1.1833\n",
            "Epoch [4720/5000], Training loss: 1.1199, Validation loss: 1.1826\n",
            "Epoch [4730/5000], Training loss: 1.1180, Validation loss: 1.1831\n",
            "Epoch [4740/5000], Training loss: 1.1218, Validation loss: 1.1833\n",
            "Epoch [4750/5000], Training loss: 1.1176, Validation loss: 1.1839\n",
            "Epoch [4760/5000], Training loss: 1.1167, Validation loss: 1.1840\n",
            "Epoch [4770/5000], Training loss: 1.1171, Validation loss: 1.1856\n",
            "Epoch [4780/5000], Training loss: 1.1185, Validation loss: 1.1846\n",
            "Epoch [4790/5000], Training loss: 1.1163, Validation loss: 1.1853\n",
            "Epoch [4800/5000], Training loss: 1.1178, Validation loss: 1.1841\n",
            "Epoch [4810/5000], Training loss: 1.1180, Validation loss: 1.1860\n",
            "Epoch [4820/5000], Training loss: 1.1163, Validation loss: 1.1849\n",
            "Epoch [4830/5000], Training loss: 1.1176, Validation loss: 1.1862\n",
            "Epoch [4840/5000], Training loss: 1.1186, Validation loss: 1.1846\n",
            "Epoch [4850/5000], Training loss: 1.1197, Validation loss: 1.1825\n",
            "Epoch [4860/5000], Training loss: 1.1191, Validation loss: 1.1864\n",
            "Epoch [4870/5000], Training loss: 1.1170, Validation loss: 1.1864\n",
            "Epoch [4880/5000], Training loss: 1.1183, Validation loss: 1.1853\n",
            "Epoch [4890/5000], Training loss: 1.1166, Validation loss: 1.1856\n",
            "Epoch [4900/5000], Training loss: 1.1192, Validation loss: 1.1850\n",
            "Epoch [4910/5000], Training loss: 1.1162, Validation loss: 1.1869\n",
            "Epoch [4920/5000], Training loss: 1.1164, Validation loss: 1.1844\n",
            "Epoch [4930/5000], Training loss: 1.1161, Validation loss: 1.1859\n",
            "Epoch [4940/5000], Training loss: 1.1180, Validation loss: 1.1846\n",
            "Epoch [4950/5000], Training loss: 1.1176, Validation loss: 1.1848\n",
            "Epoch [4960/5000], Training loss: 1.1155, Validation loss: 1.1860\n",
            "Epoch [4970/5000], Training loss: 1.1167, Validation loss: 1.1869\n",
            "Epoch [4980/5000], Training loss: 1.1159, Validation loss: 1.1864\n",
            "Epoch [4990/5000], Training loss: 1.1164, Validation loss: 1.1866\n",
            "Epoch [5000/5000], Training loss: 1.1190, Validation loss: 1.1869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(trans_losses, label= \"Transformer Val\")\n",
        "plt.plot(trans_train_loss, label= \"Transformer Train\")\n",
        "plt.plot(rnn_train_loss, label = \"LSTM Train\")\n",
        "plt.plot(rnn_losses, label = \"LSTM Val\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vgOVYqBQHUE1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "314f58a0-1f26-4540-ad27-51e1f7a5cda7"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}