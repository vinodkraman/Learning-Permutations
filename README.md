# LearningPermutations
We view Transformers and RNNs as sequence-to-sequence maps and compare their ability to permute their input sequence of tokens. Despite their superiority in language modeling, we find that Transformers perform *worse* than RNNs when attempting to learn an unknown permutation transformation from labeled training data. 
